{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 3\n",
    "\n",
    "### Regression and Classification with the Ames Housing Data\n",
    "\n",
    "---\n",
    "\n",
    "You have just joined a new \"full stack\" real estate company in Ames, Iowa. The strategy of the firm is two-fold:\n",
    "- Own the entire process from the purchase of the land all the way to sale of the house, and anything in between.\n",
    "- Use statistical analysis to optimize investment and maximize return.\n",
    "\n",
    "The company is still small, and though investment is substantial the short-term goals of the company are more oriented towards purchasing existing houses and flipping them as opposed to constructing entirely new houses. That being said, the company has access to a large construction workforce operating at rock-bottom prices.\n",
    "\n",
    "This project uses the [Ames housing data recently made available on kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from joblib import Memory\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, LinearRegression, RidgeCV, SGDRegressor, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, RFECV, chi2, f_regression, mutual_info_regression,f_classif\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle \n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Estimating the value of homes from fixed characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "Your superiors have outlined this year's strategy for the company:\n",
    "1. Develop an algorithm to reliably estimate the value of residential houses based on *fixed* characteristics.\n",
    "2. Identify characteristics of houses that the company can cost-effectively change/renovate with their construction team.\n",
    "3. Evaluate the mean dollar value of different renovations.\n",
    "\n",
    "Then we can use that to buy houses that are likely to sell for more than the cost of the purchase plus renovations.\n",
    "\n",
    "Your first job is to tackle #1. You have a dataset of housing sale data with a huge amount of features identifying different aspects of the house. The full description of the data features can be found in a separate file:\n",
    "\n",
    "    housing.csv\n",
    "    data_description.txt\n",
    "    \n",
    "You need to build a reliable estimator for the price of the house given characteristics of the house that cannot be renovated. Some examples include:\n",
    "- The neighborhood\n",
    "- Square feet\n",
    "- Bedrooms, bathrooms\n",
    "- Basement and garage space\n",
    "\n",
    "and many more. \n",
    "\n",
    "Some examples of things that **ARE renovate-able:**\n",
    "- Roof and exterior features\n",
    "- \"Quality\" metrics, such as kitchen quality\n",
    "- \"Condition\" metrics, such as condition of garage\n",
    "- Heating and electrical components\n",
    "\n",
    "and generally anything you deem can be modified without having to undergo major construction on the house.\n",
    "\n",
    "---\n",
    "\n",
    "**Your goals:**\n",
    "1. Perform any cleaning, feature engineering, and EDA you deem necessary.\n",
    "- Be sure to remove any houses that are not residential from the dataset.\n",
    "- Identify **fixed** features that can predict price.\n",
    "- Train a model on pre-2010 data and evaluate its performance on the 2010 houses.\n",
    "- Characterize your model. How well does it perform? What are the best estimates of price?\n",
    "\n",
    "> **Note:** The EDA and feature engineering component to this project is not trivial! Be sure to always think critically and creatively. Justify your actions! Use the data description file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Take NA as 'NA' instead of transfering to nan, since NA do have specific meaning in the data description.\n",
    "house = pd.read_csv('./housing.csv', keep_default_na=False, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1460 non-null object\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            1460 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1460 non-null object\n",
      "MasVnrArea       1460 non-null object\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1460 non-null object\n",
      "BsmtCond         1460 non-null object\n",
      "BsmtExposure     1460 non-null object\n",
      "BsmtFinType1     1460 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1460 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1460 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      1460 non-null object\n",
      "GarageType       1460 non-null object\n",
      "GarageYrBlt      1460 non-null object\n",
      "GarageFinish     1460 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1460 non-null object\n",
      "GarageCond       1460 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           1460 non-null object\n",
      "Fence            1460 non-null object\n",
      "MiscFeature      1460 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: int64(35), object(46)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# getting house info\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check house dimension\n",
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the column MSZoning, the unique values are ['RL' 'RM' 'C (all)' 'FV' 'RH']\n",
      "For the column LotFrontage, the unique values are ['65' '80' '68' '60' '84' '85' '75' 'NA' '51' '50' '70' '91' '72' '66'\n",
      " '101' '57' '44' '110' '98' '47' '108' '112' '74' '115' '61' '48' '33' '52'\n",
      " '100' '24' '89' '63' '76' '81' '95' '69' '21' '32' '78' '121' '122' '40'\n",
      " '105' '73' '77' '64' '94' '34' '90' '55' '88' '82' '71' '120' '107' '92'\n",
      " '134' '62' '86' '141' '97' '54' '41' '79' '174' '99' '67' '83' '43' '103'\n",
      " '93' '30' '129' '140' '35' '37' '118' '87' '116' '150' '111' '49' '96'\n",
      " '59' '36' '56' '102' '58' '38' '109' '130' '53' '137' '45' '106' '104'\n",
      " '42' '39' '144' '114' '128' '149' '313' '168' '182' '138' '160' '152'\n",
      " '124' '153' '46']\n",
      "For the column Street, the unique values are ['Pave' 'Grvl']\n",
      "For the column Alley, the unique values are ['NA' 'Grvl' 'Pave']\n",
      "For the column LotShape, the unique values are ['Reg' 'IR1' 'IR2' 'IR3']\n",
      "For the column LandContour, the unique values are ['Lvl' 'Bnk' 'Low' 'HLS']\n",
      "For the column Utilities, the unique values are ['AllPub' 'NoSeWa']\n",
      "For the column LotConfig, the unique values are ['Inside' 'FR2' 'Corner' 'CulDSac' 'FR3']\n",
      "For the column LandSlope, the unique values are ['Gtl' 'Mod' 'Sev']\n",
      "For the column Neighborhood, the unique values are ['CollgCr' 'Veenker' 'Crawfor' 'NoRidge' 'Mitchel' 'Somerst' 'NWAmes'\n",
      " 'OldTown' 'BrkSide' 'Sawyer' 'NridgHt' 'NAmes' 'SawyerW' 'IDOTRR'\n",
      " 'MeadowV' 'Edwards' 'Timber' 'Gilbert' 'StoneBr' 'ClearCr' 'NPkVill'\n",
      " 'Blmngtn' 'BrDale' 'SWISU' 'Blueste']\n",
      "For the column Condition1, the unique values are ['Norm' 'Feedr' 'PosN' 'Artery' 'RRAe' 'RRNn' 'RRAn' 'PosA' 'RRNe']\n",
      "For the column Condition2, the unique values are ['Norm' 'Artery' 'RRNn' 'Feedr' 'PosN' 'PosA' 'RRAn' 'RRAe']\n",
      "For the column BldgType, the unique values are ['1Fam' '2fmCon' 'Duplex' 'TwnhsE' 'Twnhs']\n",
      "For the column HouseStyle, the unique values are ['2Story' '1Story' '1.5Fin' '1.5Unf' 'SFoyer' 'SLvl' '2.5Unf' '2.5Fin']\n",
      "For the column RoofStyle, the unique values are ['Gable' 'Hip' 'Gambrel' 'Mansard' 'Flat' 'Shed']\n",
      "For the column RoofMatl, the unique values are ['CompShg' 'WdShngl' 'Metal' 'WdShake' 'Membran' 'Tar&Grv' 'Roll' 'ClyTile']\n",
      "For the column Exterior1st, the unique values are ['VinylSd' 'MetalSd' 'Wd Sdng' 'HdBoard' 'BrkFace' 'WdShing' 'CemntBd'\n",
      " 'Plywood' 'AsbShng' 'Stucco' 'BrkComm' 'AsphShn' 'Stone' 'ImStucc'\n",
      " 'CBlock']\n",
      "For the column Exterior2nd, the unique values are ['VinylSd' 'MetalSd' 'Wd Shng' 'HdBoard' 'Plywood' 'Wd Sdng' 'CmentBd'\n",
      " 'BrkFace' 'Stucco' 'AsbShng' 'Brk Cmn' 'ImStucc' 'AsphShn' 'Stone' 'Other'\n",
      " 'CBlock']\n",
      "For the column MasVnrType, the unique values are ['BrkFace' 'None' 'Stone' 'BrkCmn' 'NA']\n",
      "For the column MasVnrArea, the unique values are ['196' '0' '162' '350' '186' '240' '286' '306' '212' '180' '380' '281'\n",
      " '640' '200' '246' '132' '650' '101' '412' '272' '456' '1031' '178' '573'\n",
      " '344' '287' '167' '1115' '40' '104' '576' '443' '468' '66' '22' '284' '76'\n",
      " '203' '68' '183' '48' '28' '336' '600' '768' '480' '220' '184' '1129'\n",
      " '116' '135' '266' '85' '309' '136' '288' '70' '320' '50' '120' '436' '252'\n",
      " '84' '664' '226' '300' '653' '112' '491' '268' '748' '98' '275' '138'\n",
      " '205' '262' '128' '260' '153' '64' '312' '16' '922' '142' '290' '127'\n",
      " '506' '297' 'NA' '604' '254' '36' '102' '472' '481' '108' '302' '172'\n",
      " '399' '270' '46' '210' '174' '348' '315' '299' '340' '166' '72' '31' '34'\n",
      " '238' '1600' '365' '56' '150' '278' '256' '225' '370' '388' '175' '296'\n",
      " '146' '113' '176' '616' '30' '106' '870' '362' '530' '500' '510' '247'\n",
      " '305' '255' '125' '100' '432' '126' '473' '74' '145' '232' '376' '42'\n",
      " '161' '110' '18' '224' '248' '80' '304' '215' '772' '435' '378' '562'\n",
      " '168' '89' '285' '360' '94' '333' '921' '762' '594' '219' '188' '479'\n",
      " '584' '182' '250' '292' '245' '207' '82' '97' '335' '208' '420' '170'\n",
      " '459' '280' '99' '192' '204' '233' '156' '452' '513' '261' '164' '259'\n",
      " '209' '263' '216' '351' '660' '381' '54' '528' '258' '464' '57' '147'\n",
      " '1170' '293' '630' '466' '109' '41' '160' '289' '651' '169' '95' '442'\n",
      " '202' '338' '894' '328' '673' '603' '1' '375' '90' '38' '157' '11' '140'\n",
      " '130' '148' '860' '424' '1047' '243' '816' '387' '223' '158' '137' '115'\n",
      " '189' '274' '117' '60' '122' '92' '415' '760' '27' '75' '361' '105' '342'\n",
      " '298' '541' '236' '144' '423' '44' '151' '975' '450' '230' '571' '24' '53'\n",
      " '206' '14' '324' '295' '396' '67' '154' '425' '45' '1378' '337' '149'\n",
      " '143' '51' '171' '234' '63' '766' '32' '81' '163' '554' '218' '632' '114'\n",
      " '567' '359' '451' '621' '788' '86' '796' '391' '228' '88' '165' '428'\n",
      " '410' '564' '368' '318' '579' '65' '705' '408' '244' '123' '366' '731'\n",
      " '448' '294' '310' '237' '426' '96' '438' '194' '119']\n",
      "For the column ExterQual, the unique values are ['Gd' 'TA' 'Ex' 'Fa']\n",
      "For the column ExterCond, the unique values are ['TA' 'Gd' 'Fa' 'Po' 'Ex']\n",
      "For the column Foundation, the unique values are ['PConc' 'CBlock' 'BrkTil' 'Wood' 'Slab' 'Stone']\n",
      "For the column BsmtQual, the unique values are ['Gd' 'TA' 'Ex' 'NA' 'Fa']\n",
      "For the column BsmtCond, the unique values are ['TA' 'Gd' 'NA' 'Fa' 'Po']\n",
      "For the column BsmtExposure, the unique values are ['No' 'Gd' 'Mn' 'Av' 'NA']\n",
      "For the column BsmtFinType1, the unique values are ['GLQ' 'ALQ' 'Unf' 'Rec' 'BLQ' 'NA' 'LwQ']\n",
      "For the column BsmtFinType2, the unique values are ['Unf' 'BLQ' 'NA' 'ALQ' 'Rec' 'LwQ' 'GLQ']\n",
      "For the column Heating, the unique values are ['GasA' 'GasW' 'Grav' 'Wall' 'OthW' 'Floor']\n",
      "For the column HeatingQC, the unique values are ['Ex' 'Gd' 'TA' 'Fa' 'Po']\n",
      "For the column CentralAir, the unique values are ['Y' 'N']\n",
      "For the column Electrical, the unique values are ['SBrkr' 'FuseF' 'FuseA' 'FuseP' 'Mix' 'NA']\n",
      "For the column KitchenQual, the unique values are ['Gd' 'TA' 'Ex' 'Fa']\n",
      "For the column Functional, the unique values are ['Typ' 'Min1' 'Maj1' 'Min2' 'Mod' 'Maj2' 'Sev']\n",
      "For the column FireplaceQu, the unique values are ['NA' 'TA' 'Gd' 'Fa' 'Ex' 'Po']\n",
      "For the column GarageType, the unique values are ['Attchd' 'Detchd' 'BuiltIn' 'CarPort' 'NA' 'Basment' '2Types']\n",
      "For the column GarageYrBlt, the unique values are ['2003' '1976' '2001' '1998' '2000' '1993' '2004' '1973' '1931' '1939'\n",
      " '1965' '2005' '1962' '2006' '1960' '1991' '1970' '1967' '1958' '1930'\n",
      " '2002' '1968' '2007' '2008' '1957' '1920' '1966' '1959' '1995' '1954'\n",
      " '1953' 'NA' '1983' '1977' '1997' '1985' '1963' '1981' '1964' '1999' '1935'\n",
      " '1990' '1945' '1987' '1989' '1915' '1956' '1948' '1974' '2009' '1950'\n",
      " '1961' '1921' '1900' '1979' '1951' '1969' '1936' '1975' '1971' '1923'\n",
      " '1984' '1926' '1955' '1986' '1988' '1916' '1932' '1972' '1918' '1980'\n",
      " '1924' '1996' '1940' '1949' '1994' '1910' '1978' '1982' '1992' '1925'\n",
      " '1941' '2010' '1927' '1947' '1937' '1942' '1938' '1952' '1928' '1922'\n",
      " '1934' '1906' '1914' '1946' '1908' '1929' '1933']\n",
      "For the column GarageFinish, the unique values are ['RFn' 'Unf' 'Fin' 'NA']\n",
      "For the column GarageQual, the unique values are ['TA' 'Fa' 'Gd' 'NA' 'Ex' 'Po']\n",
      "For the column GarageCond, the unique values are ['TA' 'Fa' 'NA' 'Gd' 'Po' 'Ex']\n",
      "For the column PavedDrive, the unique values are ['Y' 'N' 'P']\n",
      "For the column PoolQC, the unique values are ['NA' 'Ex' 'Fa' 'Gd']\n",
      "For the column Fence, the unique values are ['NA' 'MnPrv' 'GdWo' 'GdPrv' 'MnWw']\n",
      "For the column MiscFeature, the unique values are ['NA' 'Shed' 'Gar2' 'Othr' 'TenC']\n",
      "For the column SaleType, the unique values are ['WD' 'New' 'COD' 'ConLD' 'ConLI' 'CWD' 'ConLw' 'Con' 'Oth']\n",
      "For the column SaleCondition, the unique values are ['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']\n"
     ]
    }
   ],
   "source": [
    "# check the content of object column\n",
    "# find LotFrontage, MasVnrArea, GarageYrBlt are numerical values that having missing data\n",
    "for col in house.columns:\n",
    "    if house[col].dtypes == object:\n",
    "        print(f\"For the column {col}, the unique values are {house[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the column Id, the unique values are [   1    2    3 ..., 1458 1459 1460]\n",
      "For the column MSSubClass, the unique values are [ 60  20  70  50 190  45  90 120  30  85  80 160  75 180  40]\n",
      "For the column LotArea, the unique values are [ 8450  9600 11250 ..., 17217 13175  9717]\n",
      "For the column OverallQual, the unique values are [ 7  6  8  5  9  4 10  3  1  2]\n",
      "For the column OverallCond, the unique values are [5 8 6 7 4 2 3 9 1]\n",
      "For the column YearBuilt, the unique values are [2003 1976 2001 1915 2000 1993 2004 1973 1931 1939 1965 2005 1962 2006 1960\n",
      " 1929 1970 1967 1958 1930 2002 1968 2007 1951 1957 1927 1920 1966 1959 1994\n",
      " 1954 1953 1955 1983 1975 1997 1934 1963 1981 1964 1999 1972 1921 1945 1982\n",
      " 1998 1956 1948 1910 1995 1991 2009 1950 1961 1977 1985 1979 1885 1919 1990\n",
      " 1969 1935 1988 1971 1952 1936 1923 1924 1984 1926 1940 1941 1987 1986 2008\n",
      " 1908 1892 1916 1932 1918 1912 1947 1925 1900 1980 1989 1992 1949 1880 1928\n",
      " 1978 1922 1996 2010 1946 1913 1937 1942 1938 1974 1893 1914 1906 1890 1898\n",
      " 1904 1882 1875 1911 1917 1872 1905]\n",
      "For the column YearRemodAdd, the unique values are [2003 1976 2002 1970 2000 1995 2005 1973 1950 1965 2006 1962 2007 1960 2001\n",
      " 1967 2004 2008 1997 1959 1990 1955 1983 1980 1966 1963 1987 1964 1972 1996\n",
      " 1998 1989 1953 1956 1968 1981 1992 2009 1982 1961 1993 1999 1985 1979 1977\n",
      " 1969 1958 1991 1971 1952 1975 2010 1984 1986 1994 1988 1954 1957 1951 1978\n",
      " 1974]\n",
      "For the column BsmtFinSF1, the unique values are [ 706  978  486  216  655  732 1369  859    0  851  906  998  737  733  578\n",
      "  646  504  840  188  234 1218 1277 1018 1153 1213  731  643  967  747  280\n",
      "  179  456 1351   24  763  182  104 1810  384  490  649  632  941  739  912\n",
      " 1013  603 1880  565  320  462  228  336  448 1201   33  588  600  713 1046\n",
      "  648  310 1162  520  108  569 1200  224  705  444  250  984   35  774  419\n",
      "  170 1470  938  570  300  120  116  512  567  445  695  405 1005  668  821\n",
      "  432 1300  507  679 1332  209  680  716 1400  416  429  222   57  660 1016\n",
      "  370  351  379 1288  360  639  495  288 1398  477  831 1904  436  352  611\n",
      " 1086  297  626  560  390  566 1126 1036 1088  641  617  662  312 1065  787\n",
      "  468   36  822  378  946  341   16  550  524   56  321  842  689  625  358\n",
      "  402   94 1078  329  929  697 1573  270  922  503 1334  361  672  506  714\n",
      "  403  751  226  620  546  392  421  905  904  430  614  450  210  292  795\n",
      " 1285  819  420  841  281  894 1464  700  262 1274  518 1236  425  692  987\n",
      "  970   28  256 1619   40  846 1124  720  828 1249  810  213  585  129  498\n",
      " 1270  573 1410 1082  236  388  334  874  956  773  399  162  712  609  371\n",
      "  540   72  623  428  350  298 1445  218  985  631 1280  241  690  266  777\n",
      "  812  786 1116  789 1056   50 1128  775 1309 1246  986  616 1518  664  387\n",
      "  471  385  365 1767  133  642  247  331  742 1606  916  185  544  553  326\n",
      "  778  386  426  368  459 1350 1196  630  994  168 1261 1567  299  897  607\n",
      "  836  515  374 1231  111  356  400  698 1247  257  380   27  141  991  650\n",
      "  521 1436 2260  719  377 1330  348 1219  783  969  673 1358 1260  144  584\n",
      "  554 1002  619  180  559  308  866  895  637  604 1302 1071  290  728    2\n",
      " 1441  943  231  414  349  442  328  594  816 1460 1324 1338  685 1422 1283\n",
      "   81  454  903  605  990  206  150  457   48  871   41  674  624  480 1154\n",
      "  738  493 1121  282  500  131 1696  806 1361  920 1721  187 1138  988  193\n",
      "  551  767 1186  892  311  827  543 1003 1059  239  945   20 1455  965  980\n",
      "  863  533 1084 1173  523 1148  191 1234  375  808  724  152 1180  252  832\n",
      "  575  919  439  381  438  549  612 1163  437  394 1416  422  762  975 1097\n",
      "  251  686  656  568  539  862  197  516  663  608 1636  784  249 1040  483\n",
      "  196  572  338  330  156 1390  513  460  659  364  564  306  505  932  750\n",
      "   64  633 1170  899  902 1238  528 1024 1064  285 2188  465  322  860  599\n",
      "  354   63  223  301  443  489  284  294  814  165  552  833  464  936  772\n",
      " 1440  748  982  398  562  484  417  699  696  896  556 1106  651  867  854\n",
      " 1646 1074  536 1172  915  595 1237  273  684  324 1165  138 1513  317 1012\n",
      " 1022  509  900 1085 1104  240  383  644  397  740  837  220  586  535  410\n",
      "   75  824  592 1039  510  423  661  248  704  412 1032  219  708  415 1004\n",
      "  353  702  369  622  212  645  852 1150 1258  275  176  296  538 1157  492\n",
      " 1198 1387  522  658 1216 1480 2096 1159  440 1456  883  547  788  485  340\n",
      " 1220  427  344  756 1540  666  803 1000  885 1386  319  534  125 1314  602\n",
      "  192  593  804 1053  532 1158 1014  194  167  776 5644  694 1572  746 1406\n",
      "  925  482  189  765   80 1443  259  735  734 1447  548  315 1282  408  309\n",
      "  203  865  204  790 1320  769 1070  264  759 1373  976  781   25 1110  404\n",
      "  580  678  958 1336 1079   49  830]\n",
      "For the column BsmtFinSF2, the unique values are [   0   32  668  486   93  491  506  712  362   41  169  869  150  670   28\n",
      " 1080  181  768  215  374  208  441  184  279  306  180  580  690  692  228\n",
      "  125 1063  620  175  820 1474  264  479  147  232  380  544  294  258  121\n",
      "  391  531  344  539  713  210  311 1120  165  532   96  495  174 1127  139\n",
      "  202  645  123  551  219  606  612  480  182  132  336  468  287   35  499\n",
      "  723  119   40  117  239   80  472   64 1057  127  630  128  377  764  345\n",
      " 1085  435  823  500  290  324  634  411  841 1061  466  396  354  149  193\n",
      "  273  465  400  682  557  230  106  791  240  547  469  177  108  600  492\n",
      "  211  168 1031  438  375  144   81  906  608  276  661   68  173  972  105\n",
      "  420  546  334  352  872  110  627  163 1029]\n",
      "For the column BsmtUnfSF, the unique values are [ 150  284  434  540  490   64  317  216  952  140  134  177  175 1494  520\n",
      "  832  426    0  468  525 1158  637 1777  200  204 1566  180  486  207  649\n",
      " 1228 1234  380  408 1117 1097   84  326  445  383  167  465 1296   83 1632\n",
      "  736  192  612  816   32  935  321  860 1410  148  217  530 1346  576  318\n",
      " 1143 1035  440  747  701  343  280  404  840  724  295 1768  448   36 1530\n",
      " 1065  384 1288  684 1013  402  635  163  168  176  370  350  381  410  741\n",
      " 1226 1053  641  516  793 1139  550  905  104  310  252 1125  203  728  732\n",
      "  510  899 1362   30  958  556  413  479  297  658  262  891 1304  519 1907\n",
      "  336  107  432  403  811  396  970  506  884  400  896  253  409   93 1200\n",
      "  572  774  769 1335  340  882  779  112  470  294 1686  360  441  354  700\n",
      "  725  320  554  312  968  504 1107  577  660   99  871  474  289  600  755\n",
      "  625 1121  276  186 1424 1140  375   92  305 1176   78  274  311  710  686\n",
      "  457 1232 1498 1010  160 2336  630  638  162   70 1357 1194  773  483  235\n",
      "  125 1390  594 1694  488  357  626  916 1020 1367  798  452  392  975  361\n",
      "  270  602 1482  680  606   88  342  212 1095   96  628 1560  744 2121  768\n",
      "  386 1468 1145  244  698 1079  570  476  131  184  143 1092  324 1541 1470\n",
      "  536  319  599  622  179  292  286   80  712  291  153 1088 1249  166  906\n",
      "  604  100  818  844  596  210 1603  115  103  673  726  995  967  721 1656\n",
      "  972  460  208  191  438 1869  371  624  552  322  598  268  130  484  785\n",
      "  733  953  847  333 1580  411  982  808 1293  939  784  595  229  114  522\n",
      "  735  405  117  961 1286  672 1141  806  165 1064 1063  245 1276  892 1008\n",
      "  499 1316  463  242  444  281   35  356  988  580  651  619  544  387  901\n",
      "  926  135  648   75  788 1307 1078 1258  273 1436  557  930  780  813  878\n",
      "  122  248  588  524  288  389  424 1375 1626  406  298 2153  417  739  225\n",
      "  611  237  290  264  238  363  190 1969  697  414  316  466  420  254  960\n",
      "  397 1191  548   50  178 1368  169  748  689 1264  467  605 1257  551  678\n",
      "  707  880  378  223  578  969  379  765  149  912  620 1709  132  993  197\n",
      " 1374   90  195  706 1163  367 1122 1515   55 1497  450  846   23  390  861\n",
      "  285 1050  331 2042 1237  113  742  924  512  119  314  308  293  537  126\n",
      "  427  309  914  173 1774  823  485 1116  978  636  564  108 1184  796  366\n",
      "  300  542  645  664  756  247  776  849 1392   38 1406  111  545  121 2046\n",
      "  161  261  567 1195  874 1342  151  989 1073  927  219  224  526 1164  761\n",
      "  461  876  859  171  718  138  941  464  250   72  508 1584  415   82  948\n",
      "  893  864 1349   76  487  652 1240  801  279 1030  348  234 1198  740   89\n",
      "  586  323 1836  480  456 1935  338 1594  102  374 1413  491 1129  255 1496\n",
      "  650 1926  154  999 1734  124 1417   15  834 1649  936  778 1489  442 1434\n",
      "  352  458 1221 1099  416 1800  227  907  528  189 1273  563  372  702 1090\n",
      "  435  198 1372  174 1638  894  299  105  676 1120  431  218  110  795 1098\n",
      " 1043  481  666  142  447  783 1670  277  412  794  239  662 1072  717  546\n",
      "  430  422  188  266 1181 1753  964 1450 1905 1480  772 1032  220  187   29\n",
      "  495  640  193  196  720  918 1428   77 1266 1128  692  770  750 1442 1007\n",
      "  501  691 1550 1680 1330 1710  746  814  515  571  359  355  301  668  920\n",
      " 1055 1420 1752  304 1302  833  133  549  705  722  799  462  429  810  155\n",
      "  170  230 1459 1082  758 1290 1074  251  172  868  797  365  418  730  533\n",
      "  671 1012 1528 1005 1373  500  762  752  399 1042   40   26  932  278  459\n",
      "  568 1502  543  574  977  449  983  731  120  538  831  994  341  879  815\n",
      " 1212  866 1630  328  141  364 1380   81  303  940  764 1048  334 1689  690\n",
      "  792  585  473  246 1045 1405  201   14  841 1104  241  925 2002   74  661\n",
      "  708 1152  256  804  812 1085  344  425 1616  976  496  349  971 1393 1622\n",
      " 1352 1795 1017 1588  428  803  693  858 1284 1203 1652   39  539 1217  257\n",
      "  715  616  240  315 1351 1026 1571  156   61   95  482 1094   60  862  221\n",
      "  791  398  777  503  734  709 1252  656 1319 1422  560 1573  589  877  136]\n",
      "For the column TotalBsmtSF, the unique values are [ 856 1262  920  756 1145  796 1686 1107  952  991 1040 1175  912 1494 1253\n",
      "  832 1004    0 1114 1029 1158  637 1777 1060 1566  900 1704 1484  520  649\n",
      " 1228 1234 1398 1561 1117 1097 1297 1057 1088 1350  840  938 1150 1752 1434\n",
      " 1656  736  955  794  816 1842  384 1425  970  860 1410  780  530 1370  576\n",
      " 1143 1947 1453  747 1304 2223  845 1086  462  672 1768  440  896 1237 1563\n",
      " 1065 1288  684  612 1013  990 1235  876 1214  824  680 1588  960  458  950\n",
      " 1610  741 1226 1053  641  789  793 1844  994 1264 1809 1028  729 1092 1125\n",
      " 1673  728  732 1080 1199 1362 1078  660 1008  924  992 1063 1267 1461 1907\n",
      "  928  864 1734  910 1490 1728  715  884  969 1710  825 1602 1200  572  774\n",
      " 1392 1232 1572 1541  882 1149  644 1617 1582  720 1064 1606 1202 1151 1052\n",
      " 2216  968  504 1188 1593  853  725 1431  855 1726 1360  755 1713 1121 1196\n",
      "  617  848 1424 1140 1100 1157 1212  689 1070 1436  686  798 1248 1498 1010\n",
      "  713 2392  630 1203  483 1373 1194 1462  894 1414  996 1694  735  540  626\n",
      "  948 1845 1020 1367 1444 1573 1302 1314  975 1604  963 1482  506  926 1422\n",
      "  802  740 1095 1385 1152 1240 1560 2121 1160  807 1468 1575  625  858  698\n",
      " 1079  768  795 1416 1003  702 1165 1470 2000  700  319  861 1896  697  972\n",
      " 2136  716 1347 1372 1249 1136 1502 1162  710 1719 1383  844  596 1056 3206\n",
      " 1358  943 1499 1922 1536 1208 1215  967  721 1684  536  958 1478  764 1848\n",
      " 1869  616  624  940 1142 1062  888  883 1394 1099 1268  953  744  608  847\n",
      "  683  870 1580 1856  982 1026 1293  939  784 1256  658 1041 1682  804  788\n",
      " 1144  961 1260 1310 1141  806 1281 1034 1276 1340 1344  988  651 1518  907\n",
      "  901  765  799  648 3094 1440 1258  915 1517  930  813 1533  872 1242 1364\n",
      "  588  709  560 1375 1277 1626 1488  808  547 1976 2153 1705 1833 1792 1216\n",
      "  999 1113 1073  954  264 1269  190 3200  866 1501  777 1218 1368 1084 2006\n",
      " 1244 3138 1379 1257 1452  528 2035  611  707  880 1051 1581 1838 1650  723\n",
      "  654 1204 1069 1709  998  993 1374 1389 1163 1122 1496  846  372 1164 1050\n",
      " 2042 1868 1437  742  770 1722 1814 1430 1058  908  600  965 1032 1299 1120\n",
      "  936  783 1822 1522  980 1116  978 1156  636 1554 1386  811 1520 1952 1766\n",
      "  981 1094 2109  525  776 1486 1629 1138 2077 1406 1021 1408  738 1477 2046\n",
      "  923 1291 1195 1190  874  551 1419 2444 1210  927 1112 1391 1800  360 1473\n",
      " 1643 1324  270  859  718 1176 1311  971 1742  941 1698 1584 1595  868 1153\n",
      "  893 1349 1337 1720 1479 1030 1318 1252  983 1860  836 1935 1614  761 1413\n",
      "  956  712  650  773 1926  731 1417 1024  849 1442 1649 1568  778 1489 2078\n",
      " 1454 1516 1067 1559 1127 1390 1273  918 1763 1090 1054 1039 1148 1002 1638\n",
      "  105  676 1184 1109  892 2217 1505 1059  951 2330 1670 1623 1017 1105 1001\n",
      "  546  480 1134 1104 1272 1316 1126 1181 1753  964 1466  925 1905 1500  585\n",
      " 1632  819 1616 1161  828  945  979  561  696 1330  817 1098 1428  673 1241\n",
      "  944 1225 1266 1128  485 1930 1396  916  822  750 1700 1007 1187  691 1574\n",
      " 1680 1346  985 1657  602 1022 1082  810 1504 1220 1132 1565 1338 1654 1620\n",
      " 1055  800 1306 1475 2524 1992 1193  973  854  662 1103 1154  942 1048  727\n",
      "  690 1096 1459 1251 1247 1074 1271  290  655 1463 1836  803  833  408  533\n",
      " 1012 1552 1005 1530  974 1567 1006 1042 1298  704  932 1219 1296 1198  959\n",
      " 1261 1598 1683  818 1600 2396 1624  831 1224  663  879  815 1630 2158  931\n",
      " 1660  559 1300 1702 1075 1361 1106 1476 1689 2076  792 2110 1405 1192  746\n",
      " 1986  841 2002 1332  935 1019  661 1309 1328 1085 6110 1246  771  976 1652\n",
      " 1278 1902 1274 1393 1622 1352  420 1795  544 1510  911  693 1284 1732 2033\n",
      "  570 1980  814  873  757 1108 2633 1571  984 1205  714 1746 1525  482 1356\n",
      "  862  839 1286 1485 1594  622  791  708 1223  913  656 1319 1932  539 1221\n",
      " 1542]\n",
      "For the column 1stFlrSF, the unique values are [ 856 1262  920  961 1145  796 1694 1107 1022 1077 1040 1182  912 1494 1253\n",
      "  854 1004 1296 1114 1339 1158 1108 1795 1060 1600  900 1704  520  649 1228\n",
      " 1234 1700 1561 1132 1097 1297 1057 1152 1324 1328  884  938 1150 1752 1518\n",
      " 1656  736  955  794  816 1842 1360 1425  983  860 1426  780  581 1370  902\n",
      " 1143 2207 1479  747 1304 2223  845  885 1086  840  526  952 1072 1768  682\n",
      " 1337 1563 1065  804 1301  684  612 1013  990 1235  964 1260  905  680 1588\n",
      "  960  835 1225 1610  977 1535 1226 1053 1047  789  997 1844 1216  774 1282\n",
      " 2259 1436  729 1092 1125 1699  728  988  772 1080 1199 1586  958  660 1327\n",
      " 1721 1682 1214 1959  928  864 1734  910 1501 1728  970  875  896  969 1710\n",
      " 1252 1200  572  991 1392 1232 1572 1541  882 1149  808 1867 1707 1064 1362\n",
      " 1651 2158 1164 2234  968  769  901 1340  936 1217 1224 1593 1549  725 1431\n",
      "  855 1726  929 1713 1121 1279  865  848  720 1442 1696 1100 1180 1212  932\n",
      "  689 1236  810 1137 1248 1498 1010  811 2392  630  483 1555 1194 1490  894\n",
      " 1414 1014  798 1566  866  889  626 1222 1872  908 1375 1444 1306 1625 1302\n",
      " 1314 1005 1604  963 1382 1482  926  764 1422  802 1052  778 1113 1095 1363\n",
      " 1632 1560 2121 1156 1175 1468 1575  625 1085  858  698 1079 1148 1644 1003\n",
      "  975 1041 1336 1210 1675 2000 1122 1035  861 1944  697  972  793 2036  832\n",
      "  716 1153 1088 1372 1472 1249 1136 1553 1163 1898  803 1719 1383 1445  596\n",
      " 1056 1629 1358  943 1619 1922 1536 1621 1215  993  841 1684  536 1478 1848\n",
      " 1869 1453  616 1192 1167 1142 1352  495  790  672 1394 1268 1287  953 1120\n",
      "  752 1319  847  904  914 1580 1856 1007 1026  939  784 1269  658 1742  788\n",
      "  735 1144  876 1112 1288 1310 1165  806 1620 1166 1071 1050 1276 1028  756\n",
      " 1344 1602 1470 1196  707  907 1208 1412  765  827  734  694 2402 1440 1128\n",
      " 1258  933 1689 1888  956  679  813 1533  888  786 1242  624 1663  833  979\n",
      "  575  849 1277 1634 1502 1161 1976 1652 1493 2069 1718 1131 1850 1792  916\n",
      "  999 1073 1484 1766  886 3228 1133  899 1801 1218 1368 2020 1378 1244 3138\n",
      " 1266 1476  605 2515 1509  751  334  820  880 1159 1601 1838 1680  767  664\n",
      " 1377  915  768  825 1069 1717 1126 1006 1048  897 1557 1389  996 1134 1496\n",
      "  846  576  877 1320  703 1429 2042 1521  989 2028  838 1473  779  770  924\n",
      " 1826 1402 1647 1058  927  600 1186 1940 1029 1032 1299 1054  807 1828 1548\n",
      "  980 1012 1116 1520 1350 1089 1554 1411  800 1567  981 1094 1051  822  755\n",
      "  909 2113  525  851 1486 1686 1181 2097 1454 1465 1679 1437  738 1839  792\n",
      " 2046  923 1291 1668 1195 1190  874  551 1419 2444 1238 1067 1391 1800 1264\n",
      "  372 1824  859 1576 1178 1325  971 1698 1776 1616 1146  948 1349 1464 1720\n",
      " 1038  742  757 1506 1836 1690 1220 1117 1973 1204 1614 1430 1110 1342  966\n",
      "  976 1062 1127 1285  773 1966 1428 1075 1309 1044  686 1661 1008  944 1489\n",
      " 2084 1434 1160  941 1516 1559 1099 1701 1307 1456  918 1779  702 1512 1039\n",
      " 1002 1646 1547 1036  676 1184 1462 1155 1090 1187  954  892 1709 1712  872\n",
      " 2217 1505 1068  951 2364 1670 1063 1636 1020 1105 1015 1001  546  480 1229\n",
      " 1272 1316 1617 1098 1788 1466  925 1905 1500 1207 1188 1381  965 1168  561\n",
      "  696 1542  824  783  673  869 1241 1118 1407  750  691 1574 1504  985 1657\n",
      " 1664 1082 2898 1687 1654 1055 1803 1532 2524 1733 1992 1771  930 1526 1091\n",
      " 1523 1364 1130 1096 1338 1103 1154  799  893  829 1240 1459 1251 1247 1390\n",
      "  438  950  887 1021 1552  812 1530  974  986 1042 1298 1811 1265 1640 1432\n",
      "  959 1831 1261 1170 2129  818 1124 2411  949 1624  831 1622  842  663  879\n",
      "  815 1630 1074 2196 1283 1660 1318 1211 2136 1138 1702 1507 1361 1024 1141\n",
      " 1173 2076 1140 1034 2110 1405  760 1987 1104  713 2018 1968 1332  935 1357\n",
      "  661 1724 1573 1582 1659 4692 1246  753 1203 1294 1902 1274 1787 1061  708\n",
      " 1584 1334  693 1284 1172 2156 2053  992 1078 1980 1281  814 2633 1571  984\n",
      "  754 2117  998 1416 1746 1525 1221  741 1569 1223  962 1537 1932 1423  913\n",
      " 1578 2073 1256]\n",
      "For the column 2ndFlrSF, the unique values are [ 854    0  866  756 1053  566  983  752 1142 1218  668 1320  631  716  676\n",
      "  860 1519  530  808  977 1330  833  765  462  213  548  960  670 1116  876\n",
      "  612 1031  881  790  755  592  939  520  639  656 1414  884  729 1523  728\n",
      "  351  688  941 1032  848  836  475  739 1151  448  896  524 1194  956 1070\n",
      " 1096  467  547  551  880  703  901  720  316 1518  704 1178  754  601 1360\n",
      "  929  445  564  882  920  518  817 1257  741  672 1306  504 1304 1100  730\n",
      "  689  591  888 1020  828  700  842 1286  864  829 1092  709  844 1106  596\n",
      "  807  625  649  698  840  780  568  795  648  975  702 1242 1818 1121  371\n",
      "  804  325  809 1200  871 1274 1347 1332 1177 1080  695  167  915  576  605\n",
      "  862  495  403  838  517 1427  784  711  468 1081  886  793  665  858  874\n",
      "  526  590  406 1157  299  936  438 1098  766 1101 1028 1017 1254  378 1160\n",
      "  682  110  600  678  834  384  512  930  868  224 1103  560  811  878  574\n",
      "  910  620  687  546  902 1000  846 1067  914  660 1538 1015 1237  611  707\n",
      "  527 1288  832  806 1182 1040  439  717  511 1129 1370  636  533  745  584\n",
      "  812  684  595  988  800  677  573 1066  778  661 1440  872  788  843  713\n",
      "  567  651  762  482  738  586  679  644  900  887 1872 1281  472 1312  319\n",
      "  978 1093  473  664 1540 1276  441  348 1060  714  744 1203  783 1097  734\n",
      "  767 1589  742  686 1128 1111 1174  787 1072 1088 1063  545  966  623  432\n",
      "  581  540  769 1051  761  779  514  455 1426  785  521  252  813 1120 1037\n",
      " 1169 1001 1215  928 1140 1243  571 1196 1038  561  979  701  332  368  883\n",
      " 1336 1141  634  912  798  985  826  831  750  456  602  855  336  408  980\n",
      "  998 1168 1208  797  850  898 1054  895  954  772 1230  727  454  370  628\n",
      "  304  582 1122 1134  885  640  580 1112  653  220  240 1362  534  539  650\n",
      "  918  933  712 1796  971 1175  743  523 1216 2065  272  685  776  630  984\n",
      "  875  913  464 1039 1259  940  892  725  924  764  925 1479  192  589  992\n",
      "  903  430  748  587  994  950 1323  732 1357  557 1296  390 1185  873 1611\n",
      "  457  796  908  550  989  932  358 1392  349  691 1349  768  208  622  857\n",
      "  556 1044  708  626  904  510 1104  830  981  870  694 1152]\n",
      "For the column LowQualFinSF, the unique values are [  0 360 513 234 528 572 144 392 371 390 420 473 156 515  80  53 232 481\n",
      " 120 514 397 479 205 384]\n",
      "For the column GrLivArea, the unique values are [1710 1262 1786 1717 2198 1362 1694 2090 1774 1077 1040 2324  912 1494 1253\n",
      "  854 1004 1296 1114 1339 2376 1108 1795 1060 1600  900 1704  520 1317 1228\n",
      " 1234 1700 1561 2452 1097 1297 1057 1152 1324 1328  884  938 1150 1752 2149\n",
      " 1656 1452  955 1470 1176  816 1842 1360 1425 1739 1720 2945  780 1158 1111\n",
      " 1370 2034 2473 2207 1479  747 2287 2223  845 1718 1086 1605  988  952 1285\n",
      " 1768 1230 2142 1337 1563 1065 1474 2417 1560 1224 1526  990 1235  964 2291\n",
      " 1588  960  835 1225 1610 1732 1535 1226 1818 1992 1047  789 1517 1844 1855\n",
      " 1430 2696 2259 2320 1458 1092 1125 3222 1456 1123 1080 1199 1586  754  958\n",
      "  840 1348 1053 2157 2054 1327 1721 1682 1214 1959 1852 1764  864 1734 1385\n",
      " 1501 1728 1709  875 2035 1344  969 1993 1252 1200 1096 1968 1947 2462 1232\n",
      " 2668 1541  882 1616 1355 1867 2161 1707 1382 1767 1651 2158 2060 1920 2234\n",
      "  968 1525 1802 1340 2082 3608 1217 1593 2727 1431 1726 3112 2229 1713 1121\n",
      " 1279 1310  848 1284 1442 1696 1100 2062 1212 1392 1236 1436 1954 1248 1498\n",
      " 2267 1552 2392 1302 2520  987 1555 1194 2794  894 1960 1414 1744 1487 1566\n",
      "  866 1440 2110 1872 1928 1375 1668 2144 1306 1625 1640 1314 1604 1792 2574\n",
      " 1316  764 1422 1511 2192  778 1113 1939 1363 2270 1632 1548 2121 2022 1982\n",
      " 1468 1575 1250  858 1396 1919 1716 2263 1644 1003 1558 1950 1743 1336 3493\n",
      " 2000 2243 1406  861 1944  972 1118 2036 1641 1432 2353 2646 1472 2596 2468\n",
      " 2730 1163 2978  803 1719 1383 2134 1192 1056 1629 1358 1638 1922 1536 1621\n",
      " 1215 1908  841 1684 1112 1577 1478 1626 2728 1869 1453  720 1595 1167 1142\n",
      " 1352 1924 1505 1574 1394 1268 1287 1664  752 1319  904  914 2466 1856 1800\n",
      " 1691 1301 1797  784 1953 1269 1184 2332 1367 1961  788 1034 1144 1812 1550\n",
      " 1288  672 1572 1620 1639 1680 2172 2078 1276 1028 2097 1400 2624 1134 1602\n",
      " 2630 1196 1389  907 1208 1412 1198 1365  630 1661  694 2402 1573 1258 1689\n",
      " 1888 1886 1376 1183  813 1533 1756 1590 1242 1663 1666 1203 1935 1135 1660\n",
      " 1277 1634 1502 1969 1072 1976 1652  970 1493 2643 1131 1850 1826 1216  999\n",
      " 1073 1484 2414 1304 1578  886 3228 1820  899 1218 1801 1322 1911 1378 1041\n",
      " 1368 2020 2119 2344 1796 2080 1294 1244 4676 2398 1266  928 2713  605 2515\n",
      " 1509  827  334 1347 1724 1159 1601 1838 2285  767 1496 2183 1635  768  825\n",
      " 2094 1069 1126 2046 1048 1446 1557  996 1674 2295 1647 2504 2132  943 1692\n",
      " 1109 1477 1320 1429 2042 2775 2028  838  860 1473  935 1582 2296  924 1402\n",
      " 1556 1904 1915 1986 2008 3194 1029 2153 1032 1120 1054  832 1828 2262 2614\n",
      "  980 1512 1790 1116 1520 1350 1750 1554 1411 3395  800 1387  796 1567 1518\n",
      " 1929 2704 1766  981 1094 1839 1665 1510 1469 2113 1486 2448 1181 1936 2380\n",
      " 1679 1437 1180 1476 1369 1136 1441  792  923 1291 1761 1102 1419 4316 2519\n",
      " 1539 1137  616 1148 1391 1164 2576 1824  729 1178 2554 2418  971 1742 1698\n",
      " 1776 1146 2031  948 1349 1464 2715 2256 2640 1529 1140 2098 1026 1471 1386\n",
      " 2531 1547 2365 1506 1714 1836 3279 1220 1117 1973 1204 1614 1603 1110 1342\n",
      " 2084  901 2087 1145 1062 2013 1895 1564  773 3140 1688 2822 1128 1428 1576\n",
      " 2138 1309 1044 1008 1052  936 1733 1489 1434 2126 1223 1829 1516 1067 1559\n",
      " 1099 1482 1165 1416 1701 1775 2358 1646 1445 1779 1481 2654 1426 1039 1372\n",
      " 1002 1949  910 2610 2224 1155 1090 2230  892 1712 1393 2217 1683 1068  951\n",
      " 2240 2364 1670  902 1063 1636 2057 2274 1015 2002  480 1229 2127 2200 1617\n",
      " 1686 2374 1978 1788 2236 1466  925 1905 1500 2069 1971 1962 2403 1381  965\n",
      " 1958 2872 1894 1308 1098 1095  918 2019  869 1241 2612 2290 1940 2030 1851\n",
      " 1050  944  691 1504  985 1657 1522 1271 1022 1082 1132 2898 1264 3082 1654\n",
      "  954 1803 2329 2524 2868 1771  930 1977 1989 1523 1364 2184 1991 1338 2337\n",
      " 1103 1154 2260 1571 1611 2521  893 1240 1740 1459 1251 1247 1088  438  950\n",
      " 2622 2021 1690 1658 1964  833 1012  698 1005 1530 1981  974 2210  986 1020\n",
      " 1868 2828 1006 1298  932 1811 1265 1580 1876 1671 2108 3627 1261 3086 2345\n",
      " 1343 1124 2514 4476 1130 1221 1699 1624 1804 1622 1863 1630 1074 2196 1283\n",
      " 1845 1902 1211 1846 2136 1490 1138 1933 1702 1507 2620 1190 1188 1784 1948\n",
      " 1141 1173 2076 1553 2058 1405  874 2167 1987 1166 1675 1889 2018 3447 1524\n",
      " 1357 1395 2447 1659 1970 2372 5642 1246 1983 2526 1708 1122 1274 2810 2599\n",
      " 2112 1787 1923  708  774 2792 1334  693 1861  872 2169 1913 2156 2634 3238\n",
      " 1865 1078 1980 2601 1738 1475 1374 2633  790 2117 1762 2784 1746 1584 1912\n",
      " 2482 1687 1513 1608 2093 1840 1848 1569 2450 2201  804 1537 1932 1725 2555\n",
      " 2007  913 1346 2073 2340 1256]\n",
      "For the column BsmtFullBath, the unique values are [1 0 2 3]\n",
      "For the column BsmtHalfBath, the unique values are [0 1 2]\n",
      "For the column FullBath, the unique values are [2 1 3 0]\n",
      "For the column HalfBath, the unique values are [1 0 2]\n",
      "For the column BedroomAbvGr, the unique values are [3 4 1 2 0 5 6 8]\n",
      "For the column KitchenAbvGr, the unique values are [1 2 3 0]\n",
      "For the column TotRmsAbvGrd, the unique values are [ 8  6  7  9  5 11  4 10 12  3  2 14]\n",
      "For the column Fireplaces, the unique values are [0 1 2 3]\n",
      "For the column GarageCars, the unique values are [2 3 1 0 4]\n",
      "For the column GarageArea, the unique values are [ 548  460  608  642  836  480  636  484  468  205  384  736  352  840  576\n",
      "  516  294  853  280  534  572  270  890  772  319  240  250  271  447  556\n",
      "  691  672  498  246    0  440  308  504  300  670  826  386  388  528  894\n",
      "  565  641  288  645  852  558  220  667  360  427  490  379  297  283  509\n",
      "  405  758  461  400  462  420  432  506  684  472  366  476  410  740  648\n",
      "  273  546  325  792  450  180  430  594  390  540  264  530  435  453  750\n",
      "  487  624  471  318  766  660  470  720  577  380  434  866  495  564  312\n",
      "  625  680  678  726  532  216  303  789  511  616  521  451 1166  252  497\n",
      "  682  666  786  795  856  473  398  500  349  454  644  299  210  431  438\n",
      "  675  968  721  336  810  494  457  818  463  604  389  538  520  309  429\n",
      "  673  884  868  492  413  924 1053  439  671  338  573  732  505  575  626\n",
      "  898  529  685  281  539  418  588  282  375  683  843  552  870  888  746\n",
      "  708  513 1025  656  872  292  441  189  880  676  301  474  706  617  445\n",
      "  200  592  566  514  296  244  610  834  639  501  846  560  596  600  373\n",
      "  947  350  396  864  304  784  696  569  628  550  493  578  198  422  228\n",
      "  526  525  908  499  508  694  874  164  402  515  286  603  900  583  889\n",
      "  858  502  392  403  527  765  367  426  615  871  570  406  590  612  650\n",
      " 1390  275  452  842  816  621  544  486  230  261  531  393  774  749  364\n",
      "  627  260  256  478  442  562  512  839  330  711 1134  416  779  702  567\n",
      "  832  326  551  606  739  408  475  704  983  768  632  541  320  800  831\n",
      "  554  878  752  614  481  496  423  841  895  412  865  630  605  602  618\n",
      "  444  397  455  409  820 1020  598  857  595  433  776 1220  458  613  456\n",
      "  436  812  686  611  425  343  479  619  902  574  523  414  738  354  483\n",
      "  327  756  690  284  833  601  533  522  788  555  689  796  808  510  255\n",
      "  424  305  368  824  328  160  437  665  290  912  905  542  716  586  467\n",
      "  582 1248 1043  254  712  719  862  928  782  466  714 1052  225  234  324\n",
      "  306  830  807  358  186  693  482  813  995  757 1356  459  701  322  315\n",
      "  668  404  543  954  850  477  276  518 1014  753 1418  213  844  860  748\n",
      "  248  287  825  647  342  770  663  377  804  936  722  208  662  754  622\n",
      "  620  370 1069  372  923  192]\n",
      "For the column WoodDeckSF, the unique values are [  0 298 192  40 255 235  90 147 140 160  48 240 171 100 406 222 288  49\n",
      " 203 113 392 145 196 168 112 106 857 115 120  12 576 301 144 300  74 127\n",
      " 232 158 352 182 180 166 224  80 367  53 188 105  24  98 276 200 409 239\n",
      " 400 476 178 574 237 210 441 116 280 104  87 132 238 149 355  60 139 108\n",
      " 351 209 216 248 143 365 370  58 197 263 123 138 333 250 292  95 262  81\n",
      " 289 124 172 110 208 468 256 302 190 340 233 184 201 142 122 155 670 135\n",
      " 495 536 306  64 364 353  66 159 146 296 125  44 215 264  88  89  96 414\n",
      " 519 206 141 260 324 156 220  38 261 126  85 466 270  78 169 320 268  72\n",
      " 349  42  35 326 382 161 179 103 253 148 335 176 390 328 312 185 269 195\n",
      "  57 236 517 304 198 426  28 316 322 307 257 219 416 344 380  68 114 327\n",
      " 165 187 181  92 228 245 503 315 241 303 133 403  36  52 265 207 150 290\n",
      " 486 278  70 418 234  26 342  97 272 121 243 511 154 164 173 384 202  56\n",
      " 321  86 194 421 305 117 550 509 153 394 371  63 252 136 186 170 474 214\n",
      " 199 728 436  55 431 448 361 362 162 229 439 379 356  84 635 325  33 212\n",
      " 314 242 294  30 128  45 177 227 218 309 404 500 668 402 283 183 175 586\n",
      " 295  32 366 736]\n",
      "For the column OpenPorchSF, the unique values are [ 61   0  42  35  84  30  57 204   4  21  33 213 112 102 154 159 110  90\n",
      "  56  32  50 258  54  65  38  47  64  52 138 104  82  43 146  75  72  70\n",
      "  49  11  36 151  29  94 101 199  99 234 162  63  68  46  45 122 184 120\n",
      "  20  24 130 205 108  80  66  48  25  96 111 106  40 114   8 136 132  62\n",
      " 228  60 238 260  27  74  16 198  26  83  34  55  22  98 172 119 208 105\n",
      " 140 168  28  39 148  12  51 150 117 250  10  81  44 144 175 195 128  76\n",
      "  17  59 214 121  53 231 134 192 123  78 187  85 133 176 113 137 125 523\n",
      " 100 285  88 406 155  73 182 502 274 158 142 243 235 312 124 267 265  87\n",
      " 288  23 152 341 116 160 174 247 291  18 170 156 166 129 418 240  77 364\n",
      " 188 207  67  69 131 191  41 118 252 189 282 135  95 224 169 319  58  93\n",
      " 244 185 200  92 180 263 304 229 103 211 287 292 241 547  91  86 262 210\n",
      " 141  15 126 236]\n",
      "For the column EnclosedPorch, the unique values are [  0 272 228 205 176  87 172 102  37 144  64 114 202 128 156  44  77 192\n",
      " 140 180 183  39 184  40 552  30 126  96  60 150 120 112 252  52 224 234\n",
      " 244 268 137  24 108 294 177 218 242  91 160 130 169 105  34 248 236  32\n",
      "  80 115 291 116 158 210  36 200  84 148 136 240  54 100 189 293 164 216\n",
      " 239  67  90  56 129  98 143  70 386 154 185 134 196 264 275 230 254  68\n",
      " 194 318  48  94 138 226 174  19 170 220 214 280 190 330 208 145 259  81\n",
      "  42 123 162 286 168  20 301 198 221 212  50  99]\n",
      "For the column 3SsnPorch, the unique values are [  0 320 407 130 180 168 140 508 238 245 196 144 182 162  23 216  96 153\n",
      " 290 304]\n",
      "For the column ScreenPorch, the unique values are [  0 176 198 291 252  99 184 168 130 142 192 410 224 266 170 154 153 144\n",
      " 128 259 160 271 234 374 185 182  90 396 140 276 180 161 145 200 122  95\n",
      " 120  60 126 189 260 147 385 287 156 100 216 210 197 204 225 152 175 312\n",
      " 222 265 322 190 233  63  53 143 273 288 263  80 163 116 480 178 440 155\n",
      " 220 119 165  40]\n",
      "For the column PoolArea, the unique values are [  0 512 648 576 555 480 519 738]\n",
      "For the column MiscVal, the unique values are [    0   700   350   500   400   480   450 15500  1200   800  2000   600\n",
      "  3500  1300    54   620   560  1400  8300  1150  2500]\n",
      "For the column MoSold, the unique values are [ 2  5  9 12 10  8 11  4  1  7  3  6]\n",
      "For the column YrSold, the unique values are [2008 2007 2006 2009 2010]\n",
      "For the column SalePrice, the unique values are [208500 181500 223500 140000 250000 143000 307000 200000 129900 118000\n",
      " 129500 345000 144000 279500 157000 132000 149000  90000 159000 139000\n",
      " 325300 139400 230000 154000 256300 134800 306000 207500  68500  40000\n",
      " 149350 179900 165500 277500 309000 145000 153000 109000  82000 160000\n",
      " 170000 130250 141000 319900 239686 249700 113000 127000 177000 114500\n",
      " 110000 385000 130000 180500 172500 196500 438780 124900 158000 101000\n",
      " 202500 219500 317000 180000 226000  80000 225000 244000 185000 144900\n",
      " 107400  91000 135750 136500 193500 153500 245000 126500 168500 260000\n",
      " 174000 164500  85000 123600 109900  98600 163500 133900 204750 214000\n",
      "  94750  83000 128950 205000 178000 118964 198900 169500 100000 115000\n",
      " 190000 136900 383970 217000 259500 176000 155000 320000 163990 136000\n",
      " 153900 181000  84500 128000  87000 150000 150750 220000 171000 231500\n",
      " 166000 204000 125000 105000 222500 122000 372402 235000  79000 109500\n",
      " 269500 254900 162500 412500 103200 152000 127500 325624 183500 228000\n",
      " 128500 215000 239000 163000 184000 243000 211000 501837 200100 120000\n",
      " 475000 173000 135000 153337 286000 315000 192000 148500 311872 104000\n",
      " 274900 171500 112000 143900 277000  98000 186000 252678 156000 161750\n",
      " 134450 210000 107000 311500 167240 204900  97000 386250 290000 106000\n",
      " 192500 148000 403000  94500 128200 216500  89500 185500 194500 318000\n",
      " 262500 110500 241500 137000  76500 276000 151000  73000 175500 179500\n",
      " 120500 266000 124500 201000 415298 228500 244600 179200 164700  88000\n",
      " 153575 233230 135900 131000 167000 142500 175000 158500 267000 149900\n",
      " 295000 305900  82500 360000 165600 119900 375000 188500 270000 187500\n",
      " 342643 354000 301000 126175 242000 324000 145250 214500  78000 119000\n",
      " 284000 207000 228950 377426 202900  87500 140200 151500 157500 437154\n",
      " 318061  95000 105900 177500 134000 280000 198500 147000 165000 162000\n",
      " 172400 134432 123000  61000 340000 394432 179000 187750 213500  76000\n",
      " 240000  81000 191000 426000 106500 129000  67000 241000 245500 164990\n",
      " 108000 258000 168000 339750  60000 222000 181134 149500 126000 142000\n",
      " 206300 275000 109008 195400  85400  79900 122500 212000 116000  90350\n",
      " 555000 162900 199900 119500 188000 256000 161000 263435  62383 188700\n",
      " 124000 178740 146500 187000 440000 251000 132500 208900 380000 297000\n",
      "  89471 326000 374000 164000  86000 133000 172785  91300  34900 430000\n",
      " 226700 289000 208300 164900 202665  96500 402861 265000 234000 106250\n",
      " 184750 315750 446261 200624 107500  39300 111250 272000 248000 213250\n",
      " 179665 229000 263000 112500 255500 121500 268000 325000 316600 135960\n",
      " 142600 224500 118500 146000 131500 181900 253293 369900  79500 185900\n",
      " 451950 138000 319000 114504 194201 217500 221000 359100 313000 261500\n",
      "  75500 137500 183200 105500 314813 305000 165150 139900 209500  93000\n",
      " 264561 274000 370878 143250  98300 205950 350000 145500  97500 197900\n",
      " 402000 423000 230500 173500 103600 257500 372500 159434 285000 227875\n",
      " 148800 392000 194700 755000 335000 108480 141500  89000 123500 138500\n",
      " 196000 312500 361919 213000  55000 302000 254000 179540  52000 102776\n",
      " 189000 130500 159500 341000 103000 236500 131400  93500 239900 299800\n",
      " 236000 265979 260400 275500 158900 179400 215200 337000 264132 216837\n",
      " 538000 134900 102000 395000 221500 175900 187100 161500 233000 107900\n",
      " 160200 146800 269790 143500 485000 582933 227680 135500 159950 144500\n",
      "  55993 157900 224900 271000 224000 183000 139500 232600 147400 237000\n",
      " 139950 174900 133500 189950 250580 248900 169000 200500  66500 303477\n",
      " 132250 328900 122900 154500 118858 142953 611657 125500 255000 154300\n",
      " 173733  75000  35311 238000 176500 145900 169990 193000 117500 184900\n",
      " 253000 239799 244400 150900 197500 172000 116500 214900 178900  37900\n",
      "  99500 182000 167500  85500 178400 336000 159895 255900 117000 395192\n",
      " 195000 197000 348000 173900 337500 121600 206000 232000 136905 119200\n",
      " 227000 203000 213490 194000 287000 293077 310000 119750  84000 315500\n",
      " 262280 278000 139600 556581  84900 176485 200141 185850 328000 167900\n",
      " 151400  91500 138800 155900  83500 252000  92900 176432 274725 134500\n",
      " 184100 133700 118400 212900 163900 259000 239500  94000 424870 174500\n",
      " 116900 201800 218000 235128 108959 233170 245350 625000 171900 154900\n",
      " 392500 745000 186700 104900 262000 219210 116050 271900 229456  80500\n",
      " 137900 367294 101800 138887 265900 248328 465000 186500 169900 171750\n",
      " 294000 165400 301500  99900 128900 183900 378500 381000 185750  68400\n",
      " 150500 281000 333168 206900 295493 111000 156500  72500  52500 155835\n",
      " 108500 283463 410000 156932 144152 216000 274300 466500  58500 237500\n",
      " 377500 246578 281213 137450 193879 282922 257000 223000 274970 182900\n",
      " 192140 143750  64500 394617 149700 149300 121000 179600  92000 287090\n",
      " 266500 142125 147500]\n"
     ]
    }
   ],
   "source": [
    "# check numerial unique value\n",
    "for col in house.columns:\n",
    "    if house[col].dtypes != object:\n",
    "        print(f\"For the column {col}, the unique values are {house[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RL', 'RM', 'C (all)', 'FV', 'RH'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check MSZoning, there are non residential house\n",
    "house.MSZoning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RL    1151\n",
       "RM     218\n",
       "FV      65\n",
       "RH      16\n",
       "Name: MSZoning, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove non-residential house\n",
    "residential = [True if ele in ['RL', 'RM', 'FV', 'RH'] else False for ele in house.MSZoning]\n",
    "house_residential = house[residential].copy()\n",
    "house_residential.MSZoning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LotFrontage NA value to 0, since there is no zero value exist, consider as input error, 0 feet is record as NA\n",
    "# convert to int\n",
    "house_residential.LotFrontage = [0 if ele=='NA' else int(ele) for ele in house_residential.LotFrontage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65,  80,  68,  60,  84,  85,  75,   0,  51,  50,  70,  91,  72,\n",
       "        66, 101,  57,  44, 110,  98,  47, 108, 112,  74, 115,  61,  48,\n",
       "        33,  52, 100,  24,  89,  63,  76,  81,  95,  69,  21,  32,  78,\n",
       "       121, 122,  40,  73,  77,  64,  94, 105,  34,  90,  55,  88,  82,\n",
       "        71, 120, 107,  92, 134,  62,  86, 141,  97,  54,  41,  79, 174,\n",
       "        99,  67,  83,  43, 103,  93,  30, 129, 140,  35,  37, 118,  87,\n",
       "       116, 150, 111,  49,  96,  59,  36,  56, 102,  58,  38, 109, 130,\n",
       "        53, 137,  45, 106, 104,  42,  39, 144, 114, 128, 149, 313, 168,\n",
       "       182, 138, 160, 152, 124, 153,  46], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recheck the value of LotFrontage\n",
    "house_residential['LotFrontage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24063c990f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 374
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look of the house price distribution\n",
    "# there are few very large values\n",
    "house_residential.SalePrice.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No    947\n",
       "Av    219\n",
       "Gd    134\n",
       "Mn    112\n",
       "NA     38\n",
       "Name: BsmtExposure, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking basment exposure, No is no exposure, NA is no basement\n",
    "house_residential.BsmtExposure.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BsmtQual BsmtCond BsmtExposure BsmtFinType1  BsmtFinSF1 BsmtFinType2  \\\n",
       "17         NA       NA           NA           NA           0           NA   \n",
       "39         NA       NA           NA           NA           0           NA   \n",
       "90         NA       NA           NA           NA           0           NA   \n",
       "102        NA       NA           NA           NA           0           NA   \n",
       "156        NA       NA           NA           NA           0           NA   \n",
       "182        NA       NA           NA           NA           0           NA   \n",
       "259        NA       NA           NA           NA           0           NA   \n",
       "342        NA       NA           NA           NA           0           NA   \n",
       "362        NA       NA           NA           NA           0           NA   \n",
       "371        NA       NA           NA           NA           0           NA   \n",
       "392        NA       NA           NA           NA           0           NA   \n",
       "520        NA       NA           NA           NA           0           NA   \n",
       "532        NA       NA           NA           NA           0           NA   \n",
       "533        NA       NA           NA           NA           0           NA   \n",
       "553        NA       NA           NA           NA           0           NA   \n",
       "646        NA       NA           NA           NA           0           NA   \n",
       "705        NA       NA           NA           NA           0           NA   \n",
       "736        NA       NA           NA           NA           0           NA   \n",
       "749        NA       NA           NA           NA           0           NA   \n",
       "778        NA       NA           NA           NA           0           NA   \n",
       "868        NA       NA           NA           NA           0           NA   \n",
       "894        NA       NA           NA           NA           0           NA   \n",
       "897        NA       NA           NA           NA           0           NA   \n",
       "948        Gd       TA           NA          Unf           0          Unf   \n",
       "984        NA       NA           NA           NA           0           NA   \n",
       "1000       NA       NA           NA           NA           0           NA   \n",
       "1011       NA       NA           NA           NA           0           NA   \n",
       "1035       NA       NA           NA           NA           0           NA   \n",
       "1045       NA       NA           NA           NA           0           NA   \n",
       "1048       NA       NA           NA           NA           0           NA   \n",
       "1049       NA       NA           NA           NA           0           NA   \n",
       "1090       NA       NA           NA           NA           0           NA   \n",
       "1179       NA       NA           NA           NA           0           NA   \n",
       "1216       NA       NA           NA           NA           0           NA   \n",
       "1218       NA       NA           NA           NA           0           NA   \n",
       "1232       NA       NA           NA           NA           0           NA   \n",
       "1321       NA       NA           NA           NA           0           NA   \n",
       "1412       NA       NA           NA           NA           0           NA   \n",
       "\n",
       "      BsmtFinSF2  TotalBsmtSF  BsmtFullBath  BsmtHalfBath  \n",
       "17             0            0             0             0  \n",
       "39             0            0             0             0  \n",
       "90             0            0             0             0  \n",
       "102            0            0             0             0  \n",
       "156            0            0             0             0  \n",
       "182            0            0             0             0  \n",
       "259            0            0             0             0  \n",
       "342            0            0             0             0  \n",
       "362            0            0             0             0  \n",
       "371            0            0             0             0  \n",
       "392            0            0             0             0  \n",
       "520            0            0             0             0  \n",
       "532            0            0             0             0  \n",
       "533            0            0             0             0  \n",
       "553            0            0             0             0  \n",
       "646            0            0             0             0  \n",
       "705            0            0             0             0  \n",
       "736            0            0             0             0  \n",
       "749            0            0             0             0  \n",
       "778            0            0             0             0  \n",
       "868            0            0             0             0  \n",
       "894            0            0             0             0  \n",
       "897            0            0             0             0  \n",
       "948            0          936             0             0  \n",
       "984            0            0             0             0  \n",
       "1000           0            0             0             0  \n",
       "1011           0            0             0             0  \n",
       "1035           0            0             0             0  \n",
       "1045           0            0             0             0  \n",
       "1048           0            0             0             0  \n",
       "1049           0            0             0             0  \n",
       "1090           0            0             0             0  \n",
       "1179           0            0             0             0  \n",
       "1216           0            0             0             0  \n",
       "1218           0            0             0             0  \n",
       "1232           0            0             0             0  \n",
       "1321           0            0             0             0  \n",
       "1412           0            0             0             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the no basement house\n",
    "# item 948 basement exist, BsmtExposure change to No\n",
    "house_residential.loc[house_residential.BsmtExposure=='NA', ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1','BsmtFinType2', 'BsmtFinSF2','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting 948, 'BsmtExposure' = 'No'\n",
    "house_residential.loc[948, 'BsmtExposure'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>1124</td>\n",
       "      <td>NA</td>\n",
       "      <td>479</td>\n",
       "      <td>3206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BsmtQual BsmtCond BsmtExposure BsmtFinType1  BsmtFinSF1 BsmtFinType2  \\\n",
       "17         NA       NA           NA           NA           0           NA   \n",
       "39         NA       NA           NA           NA           0           NA   \n",
       "90         NA       NA           NA           NA           0           NA   \n",
       "102        NA       NA           NA           NA           0           NA   \n",
       "156        NA       NA           NA           NA           0           NA   \n",
       "182        NA       NA           NA           NA           0           NA   \n",
       "259        NA       NA           NA           NA           0           NA   \n",
       "332        Gd       TA           No          GLQ        1124           NA   \n",
       "342        NA       NA           NA           NA           0           NA   \n",
       "362        NA       NA           NA           NA           0           NA   \n",
       "371        NA       NA           NA           NA           0           NA   \n",
       "392        NA       NA           NA           NA           0           NA   \n",
       "520        NA       NA           NA           NA           0           NA   \n",
       "532        NA       NA           NA           NA           0           NA   \n",
       "533        NA       NA           NA           NA           0           NA   \n",
       "553        NA       NA           NA           NA           0           NA   \n",
       "646        NA       NA           NA           NA           0           NA   \n",
       "705        NA       NA           NA           NA           0           NA   \n",
       "736        NA       NA           NA           NA           0           NA   \n",
       "749        NA       NA           NA           NA           0           NA   \n",
       "778        NA       NA           NA           NA           0           NA   \n",
       "868        NA       NA           NA           NA           0           NA   \n",
       "894        NA       NA           NA           NA           0           NA   \n",
       "897        NA       NA           NA           NA           0           NA   \n",
       "984        NA       NA           NA           NA           0           NA   \n",
       "1000       NA       NA           NA           NA           0           NA   \n",
       "1011       NA       NA           NA           NA           0           NA   \n",
       "1035       NA       NA           NA           NA           0           NA   \n",
       "1045       NA       NA           NA           NA           0           NA   \n",
       "1048       NA       NA           NA           NA           0           NA   \n",
       "1049       NA       NA           NA           NA           0           NA   \n",
       "1090       NA       NA           NA           NA           0           NA   \n",
       "1179       NA       NA           NA           NA           0           NA   \n",
       "1216       NA       NA           NA           NA           0           NA   \n",
       "1218       NA       NA           NA           NA           0           NA   \n",
       "1232       NA       NA           NA           NA           0           NA   \n",
       "1321       NA       NA           NA           NA           0           NA   \n",
       "1412       NA       NA           NA           NA           0           NA   \n",
       "\n",
       "      BsmtFinSF2  TotalBsmtSF  BsmtFullBath  BsmtHalfBath  \n",
       "17             0            0             0             0  \n",
       "39             0            0             0             0  \n",
       "90             0            0             0             0  \n",
       "102            0            0             0             0  \n",
       "156            0            0             0             0  \n",
       "182            0            0             0             0  \n",
       "259            0            0             0             0  \n",
       "332          479         3206             1             0  \n",
       "342            0            0             0             0  \n",
       "362            0            0             0             0  \n",
       "371            0            0             0             0  \n",
       "392            0            0             0             0  \n",
       "520            0            0             0             0  \n",
       "532            0            0             0             0  \n",
       "533            0            0             0             0  \n",
       "553            0            0             0             0  \n",
       "646            0            0             0             0  \n",
       "705            0            0             0             0  \n",
       "736            0            0             0             0  \n",
       "749            0            0             0             0  \n",
       "778            0            0             0             0  \n",
       "868            0            0             0             0  \n",
       "894            0            0             0             0  \n",
       "897            0            0             0             0  \n",
       "984            0            0             0             0  \n",
       "1000           0            0             0             0  \n",
       "1011           0            0             0             0  \n",
       "1035           0            0             0             0  \n",
       "1045           0            0             0             0  \n",
       "1048           0            0             0             0  \n",
       "1049           0            0             0             0  \n",
       "1090           0            0             0             0  \n",
       "1179           0            0             0             0  \n",
       "1216           0            0             0             0  \n",
       "1218           0            0             0             0  \n",
       "1232           0            0             0             0  \n",
       "1321           0            0             0             0  \n",
       "1412           0            0             0             0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking other basement related columns\n",
    "# item 332 basement exist but missing BsmtFinType2 (NA means no basement, which is not the incorrect info of this house)\n",
    "house_residential.loc[house_residential.BsmtFinType2=='NA', ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting 332, 'BsmtFinType2' = 'othr'\n",
    "house_residential.loc[332, 'BsmtFinType2'] = 'othr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MasVnrType MasVnrArea\n",
       "234          NA         NA\n",
       "529          NA         NA\n",
       "650          NA         NA\n",
       "936          NA         NA\n",
       "973          NA         NA\n",
       "977          NA         NA\n",
       "1243         NA         NA\n",
       "1278         NA         NA"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some house missing masonry veneer type, set area to 0\n",
    "house_residential.loc[house_residential.MasVnrArea=='NA',['MasVnrType','MasVnrArea' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_residential.MasVnrArea = [0 if ele=='NA' else int(ele) for ele in house_residential.MasVnrArea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change None to NA, the None also repersent not exist. To keep the consistency with other columns\n",
    "house_residential.loc[house_residential.MasVnrType=='None','MasVnrType']='NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the missing electrical with the most common type SBrkr, since in the dictionary do not indicate what is NA repersent for\n",
    "# Electrical, take ask incorrect input\n",
    "house_residential.loc[house_residential.Electrical=='NA','Electrical']= house_residential.Electrical.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GarageType GarageFinish GarageQual GarageCond GarageYrBlt\n",
       "39           NA           NA         NA         NA          NA\n",
       "48           NA           NA         NA         NA          NA\n",
       "78           NA           NA         NA         NA          NA\n",
       "89           NA           NA         NA         NA          NA\n",
       "99           NA           NA         NA         NA          NA\n",
       "108          NA           NA         NA         NA          NA\n",
       "125          NA           NA         NA         NA          NA\n",
       "127          NA           NA         NA         NA          NA\n",
       "140          NA           NA         NA         NA          NA\n",
       "148          NA           NA         NA         NA          NA\n",
       "155          NA           NA         NA         NA          NA\n",
       "163          NA           NA         NA         NA          NA\n",
       "165          NA           NA         NA         NA          NA\n",
       "198          NA           NA         NA         NA          NA\n",
       "210          NA           NA         NA         NA          NA\n",
       "241          NA           NA         NA         NA          NA\n",
       "250          NA           NA         NA         NA          NA\n",
       "287          NA           NA         NA         NA          NA\n",
       "291          NA           NA         NA         NA          NA\n",
       "307          NA           NA         NA         NA          NA\n",
       "375          NA           NA         NA         NA          NA\n",
       "386          NA           NA         NA         NA          NA\n",
       "393          NA           NA         NA         NA          NA\n",
       "431          NA           NA         NA         NA          NA\n",
       "434          NA           NA         NA         NA          NA\n",
       "441          NA           NA         NA         NA          NA\n",
       "464          NA           NA         NA         NA          NA\n",
       "520          NA           NA         NA         NA          NA\n",
       "528          NA           NA         NA         NA          NA\n",
       "533          NA           NA         NA         NA          NA\n",
       "...         ...          ...        ...        ...         ...\n",
       "954          NA           NA         NA         NA          NA\n",
       "960          NA           NA         NA         NA          NA\n",
       "968          NA           NA         NA         NA          NA\n",
       "970          NA           NA         NA         NA          NA\n",
       "976          NA           NA         NA         NA          NA\n",
       "1009         NA           NA         NA         NA          NA\n",
       "1011         NA           NA         NA         NA          NA\n",
       "1030         NA           NA         NA         NA          NA\n",
       "1038         NA           NA         NA         NA          NA\n",
       "1096         NA           NA         NA         NA          NA\n",
       "1123         NA           NA         NA         NA          NA\n",
       "1131         NA           NA         NA         NA          NA\n",
       "1137         NA           NA         NA         NA          NA\n",
       "1143         NA           NA         NA         NA          NA\n",
       "1173         NA           NA         NA         NA          NA\n",
       "1179         NA           NA         NA         NA          NA\n",
       "1218         NA           NA         NA         NA          NA\n",
       "1219         NA           NA         NA         NA          NA\n",
       "1234         NA           NA         NA         NA          NA\n",
       "1257         NA           NA         NA         NA          NA\n",
       "1283         NA           NA         NA         NA          NA\n",
       "1323         NA           NA         NA         NA          NA\n",
       "1325         NA           NA         NA         NA          NA\n",
       "1326         NA           NA         NA         NA          NA\n",
       "1337         NA           NA         NA         NA          NA\n",
       "1349         NA           NA         NA         NA          NA\n",
       "1407         NA           NA         NA         NA          NA\n",
       "1449         NA           NA         NA         NA          NA\n",
       "1450         NA           NA         NA         NA          NA\n",
       "1453         NA           NA         NA         NA          NA\n",
       "\n",
       "[79 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Garage info\n",
    "house_residential.loc[house_residential.GarageYrBlt=='NA',['GarageType','GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set garageYrBlt to numerical, NA to 0, varified with GarageCond, this column later will be used in garage age calculation,\n",
    "# put no garage build year as 0 for now\n",
    "house_residential.GarageYrBlt = [0 if ele=='NA' else int(ele) for ele in house_residential.GarageYrBlt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the column MSZoning, the unique values are ['RL' 'RM' 'FV' 'RH']\n",
      "For the column Street, the unique values are ['Pave' 'Grvl']\n",
      "For the column Alley, the unique values are ['NA' 'Grvl' 'Pave']\n",
      "For the column LotShape, the unique values are ['Reg' 'IR1' 'IR2' 'IR3']\n",
      "For the column LandContour, the unique values are ['Lvl' 'Bnk' 'Low' 'HLS']\n",
      "For the column Utilities, the unique values are ['AllPub' 'NoSeWa']\n",
      "For the column LotConfig, the unique values are ['Inside' 'FR2' 'Corner' 'CulDSac' 'FR3']\n",
      "For the column LandSlope, the unique values are ['Gtl' 'Mod' 'Sev']\n",
      "For the column Neighborhood, the unique values are ['CollgCr' 'Veenker' 'Crawfor' 'NoRidge' 'Mitchel' 'Somerst' 'NWAmes'\n",
      " 'OldTown' 'BrkSide' 'Sawyer' 'NridgHt' 'NAmes' 'SawyerW' 'IDOTRR'\n",
      " 'MeadowV' 'Edwards' 'Timber' 'Gilbert' 'StoneBr' 'ClearCr' 'NPkVill'\n",
      " 'Blmngtn' 'BrDale' 'SWISU' 'Blueste']\n",
      "For the column Condition1, the unique values are ['Norm' 'Feedr' 'PosN' 'Artery' 'RRAe' 'RRNn' 'RRAn' 'PosA' 'RRNe']\n",
      "For the column Condition2, the unique values are ['Norm' 'Artery' 'RRNn' 'Feedr' 'PosN' 'PosA' 'RRAn' 'RRAe']\n",
      "For the column BldgType, the unique values are ['1Fam' '2fmCon' 'Duplex' 'TwnhsE' 'Twnhs']\n",
      "For the column HouseStyle, the unique values are ['2Story' '1Story' '1.5Fin' '1.5Unf' 'SFoyer' 'SLvl' '2.5Unf' '2.5Fin']\n",
      "For the column RoofStyle, the unique values are ['Gable' 'Hip' 'Gambrel' 'Mansard' 'Flat' 'Shed']\n",
      "For the column RoofMatl, the unique values are ['CompShg' 'WdShngl' 'Metal' 'WdShake' 'Membran' 'Tar&Grv' 'Roll' 'ClyTile']\n",
      "For the column Exterior1st, the unique values are ['VinylSd' 'MetalSd' 'Wd Sdng' 'HdBoard' 'BrkFace' 'WdShing' 'CemntBd'\n",
      " 'Plywood' 'AsbShng' 'Stucco' 'BrkComm' 'AsphShn' 'Stone' 'ImStucc'\n",
      " 'CBlock']\n",
      "For the column Exterior2nd, the unique values are ['VinylSd' 'MetalSd' 'Wd Shng' 'HdBoard' 'Plywood' 'Wd Sdng' 'CmentBd'\n",
      " 'BrkFace' 'Stucco' 'AsbShng' 'Brk Cmn' 'ImStucc' 'AsphShn' 'Stone' 'Other'\n",
      " 'CBlock']\n",
      "For the column MasVnrType, the unique values are ['BrkFace' 'NA' 'Stone' 'BrkCmn']\n",
      "For the column ExterQual, the unique values are ['Gd' 'TA' 'Ex' 'Fa']\n",
      "For the column ExterCond, the unique values are ['TA' 'Gd' 'Fa' 'Po' 'Ex']\n",
      "For the column Foundation, the unique values are ['PConc' 'CBlock' 'BrkTil' 'Wood' 'Slab' 'Stone']\n",
      "For the column BsmtQual, the unique values are ['Gd' 'TA' 'Ex' 'NA' 'Fa']\n",
      "For the column BsmtCond, the unique values are ['TA' 'Gd' 'NA' 'Fa' 'Po']\n",
      "For the column BsmtExposure, the unique values are ['No' 'Gd' 'Mn' 'Av' 'NA']\n",
      "For the column BsmtFinType1, the unique values are ['GLQ' 'ALQ' 'Unf' 'Rec' 'BLQ' 'NA' 'LwQ']\n",
      "For the column BsmtFinType2, the unique values are ['Unf' 'BLQ' 'NA' 'ALQ' 'Rec' 'LwQ' 'GLQ' 'othr']\n",
      "For the column Heating, the unique values are ['GasA' 'GasW' 'Grav' 'Wall' 'OthW' 'Floor']\n",
      "For the column HeatingQC, the unique values are ['Ex' 'Gd' 'TA' 'Fa' 'Po']\n",
      "For the column CentralAir, the unique values are ['Y' 'N']\n",
      "For the column Electrical, the unique values are ['SBrkr' 'FuseF' 'FuseA' 'FuseP' 'Mix']\n",
      "For the column KitchenQual, the unique values are ['Gd' 'TA' 'Ex' 'Fa']\n",
      "For the column Functional, the unique values are ['Typ' 'Min1' 'Maj1' 'Min2' 'Mod' 'Maj2' 'Sev']\n",
      "For the column FireplaceQu, the unique values are ['NA' 'TA' 'Gd' 'Fa' 'Ex' 'Po']\n",
      "For the column GarageType, the unique values are ['Attchd' 'Detchd' 'BuiltIn' 'CarPort' 'NA' 'Basment' '2Types']\n",
      "For the column GarageFinish, the unique values are ['RFn' 'Unf' 'Fin' 'NA']\n",
      "For the column GarageQual, the unique values are ['TA' 'Fa' 'Gd' 'NA' 'Ex' 'Po']\n",
      "For the column GarageCond, the unique values are ['TA' 'NA' 'Gd' 'Fa' 'Po' 'Ex']\n",
      "For the column PavedDrive, the unique values are ['Y' 'N' 'P']\n",
      "For the column PoolQC, the unique values are ['NA' 'Ex' 'Fa' 'Gd']\n",
      "For the column Fence, the unique values are ['NA' 'MnPrv' 'GdWo' 'GdPrv' 'MnWw']\n",
      "For the column MiscFeature, the unique values are ['NA' 'Shed' 'Gar2' 'Othr' 'TenC']\n",
      "For the column SaleType, the unique values are ['WD' 'New' 'COD' 'ConLI' 'CWD' 'ConLw' 'Con' 'ConLD' 'Oth']\n",
      "For the column SaleCondition, the unique values are ['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']\n"
     ]
    }
   ],
   "source": [
    "# recheck the object data\n",
    "for col in house_residential.columns:\n",
    "    if house_residential[col].dtypes == object:\n",
    "        print(f\"For the column {col}, the unique values are {house_residential[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.00000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>731.364138</td>\n",
       "      <td>56.903448</td>\n",
       "      <td>57.54000</td>\n",
       "      <td>10523.831724</td>\n",
       "      <td>6.114483</td>\n",
       "      <td>5.582069</td>\n",
       "      <td>1971.593103</td>\n",
       "      <td>1985.049655</td>\n",
       "      <td>103.828276</td>\n",
       "      <td>445.162759</td>\n",
       "      <td>...</td>\n",
       "      <td>94.826897</td>\n",
       "      <td>46.537931</td>\n",
       "      <td>21.536552</td>\n",
       "      <td>3.433103</td>\n",
       "      <td>15.096552</td>\n",
       "      <td>2.777931</td>\n",
       "      <td>43.365517</td>\n",
       "      <td>6.312414</td>\n",
       "      <td>2007.812414</td>\n",
       "      <td>181654.942069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.554345</td>\n",
       "      <td>42.265033</td>\n",
       "      <td>34.71992</td>\n",
       "      <td>10012.185355</td>\n",
       "      <td>1.372860</td>\n",
       "      <td>1.107330</td>\n",
       "      <td>30.022696</td>\n",
       "      <td>20.552319</td>\n",
       "      <td>181.150114</td>\n",
       "      <td>456.353871</td>\n",
       "      <td>...</td>\n",
       "      <td>125.560863</td>\n",
       "      <td>65.222761</td>\n",
       "      <td>60.838826</td>\n",
       "      <td>29.416948</td>\n",
       "      <td>55.893399</td>\n",
       "      <td>40.315051</td>\n",
       "      <td>497.636495</td>\n",
       "      <td>2.698244</td>\n",
       "      <td>1.326321</td>\n",
       "      <td>79176.485241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>37900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>366.250000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>41.25000</td>\n",
       "      <td>7544.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>130000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>731.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163945.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1096.750000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>79.00000</td>\n",
       "      <td>11613.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>713.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.00000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1450.000000  1450.000000   1450.00000    1450.000000  1450.000000   \n",
       "mean    731.364138    56.903448     57.54000   10523.831724     6.114483   \n",
       "std     421.554345    42.265033     34.71992   10012.185355     1.372860   \n",
       "min       1.000000    20.000000      0.00000    1300.000000     1.000000   \n",
       "25%     366.250000    20.000000     41.25000    7544.500000     5.000000   \n",
       "50%     731.500000    50.000000     63.00000    9496.000000     6.000000   \n",
       "75%    1096.750000    70.000000     79.00000   11613.500000     7.000000   \n",
       "max    1460.000000   190.000000    313.00000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \\\n",
       "count  1450.000000  1450.000000   1450.000000  1450.000000  1450.000000   \n",
       "mean      5.582069  1971.593103   1985.049655   103.828276   445.162759   \n",
       "std       1.107330    30.022696     20.552319   181.150114   456.353871   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   384.000000   \n",
       "75%       6.000000  2001.000000   2004.000000   166.000000   713.750000   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   \n",
       "\n",
       "           ...         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count      ...        1450.000000  1450.000000    1450.000000  1450.000000   \n",
       "mean       ...          94.826897    46.537931      21.536552     3.433103   \n",
       "std        ...         125.560863    65.222761      60.838826    29.416948   \n",
       "min        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "25%        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "50%        ...           0.000000    25.000000       0.000000     0.000000   \n",
       "75%        ...         168.000000    68.000000       0.000000     0.000000   \n",
       "max        ...         857.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \\\n",
       "count  1450.000000  1450.000000   1450.000000  1450.000000  1450.000000   \n",
       "mean     15.096552     2.777931     43.365517     6.312414  2007.812414   \n",
       "std      55.893399    40.315051    497.636495     2.698244     1.326321   \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000   \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000   \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000   \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000   \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1450.000000  \n",
       "mean   181654.942069  \n",
       "std     79176.485241  \n",
       "min     37900.000000  \n",
       "25%    130000.000000  \n",
       "50%    163945.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recheck the numerial data\n",
    "house_residential.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engieering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the Porch area\n",
    "house_residential['PorchSF'] = house_residential[['OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the age of building when it was sold\n",
    "house_residential.insert(loc = int(np.argwhere(house_residential.columns=='YearBuilt')[0,0] +1), column='sold_built_age', value=(house_residential.YrSold - house_residential.YearBuilt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age of the renovation of the building when it was sold\n",
    "house_residential.insert(loc = int(np.argwhere(house_residential.columns=='YearRemodAdd')[0,0] +1), column='sold_Remod_age', value=(house_residential.YrSold - house_residential.YearRemodAdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sold_Remod_age</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>YearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>-1</td>\n",
       "      <td>2008</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sold_Remod_age  YearRemodAdd  YrSold  YearBuilt\n",
       "523              -1          2008    2007       2007"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is one building that renovated after sold, so change the value to 0, means newly renovated when sold\n",
    "house_residential.loc[house_residential.sold_Remod_age==-1, ['sold_Remod_age', 'YearRemodAdd', 'YrSold', 'YearBuilt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_residential.loc[house_residential.sold_Remod_age==-1, 'sold_Remod_age'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Garage age\n",
    "gAge = (house_residential.YrSold - house_residential.GarageYrBlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list whether there is garage, by checking the age of the garage\n",
    "H_gar = [1 if age<200 else 0 for age in gAge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed the house without garage to garage age = 1000, will be compensate by the having garage column\n",
    "gAge = [age if age<200 else 1000 for age in gAge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add garage age\n",
    "house_residential.insert(loc = int(np.argwhere(house_residential.columns=='GarageYrBlt')[0,0] +1), column='sold_Garage_age', value=gAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add where have garage\n",
    "house_residential.insert(loc = int(np.argwhere(house_residential.columns=='GarageYrBlt')[0,0] +1), column='have_garage', value=H_gar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sold_Garage_age</th>\n",
       "      <th>sold_Remod_age</th>\n",
       "      <th>sold_built_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>82.075862</td>\n",
       "      <td>22.763448</td>\n",
       "      <td>36.219310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>221.717391</td>\n",
       "      <td>20.540721</td>\n",
       "      <td>30.064048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sold_Garage_age  sold_Remod_age  sold_built_age\n",
       "count      1450.000000     1450.000000     1450.000000\n",
       "mean         82.075862       22.763448       36.219310\n",
       "std         221.717391       20.540721       30.064048\n",
       "min           0.000000        0.000000        0.000000\n",
       "25%           7.000000        4.000000        7.000000\n",
       "50%          30.000000       14.000000       34.000000\n",
       "75%          50.000000       41.000000       54.000000\n",
       "max        1000.000000       60.000000      136.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check Garage info\n",
    "house_residential[['sold_Garage_age', 'sold_Remod_age', 'sold_built_age']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
       "       'sold_built_age', 'YearRemodAdd', 'sold_Remod_age', 'RoofStyle',\n",
       "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n",
       "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
       "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
       "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n",
       "       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
       "       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
       "       'have_garage', 'sold_Garage_age', 'GarageFinish', 'GarageCars',\n",
       "       'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
       "       'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold',\n",
       "       'SaleType', 'SaleCondition', 'SalePrice', 'PorchSF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all column of the data\n",
    "house_residential.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed ch selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_ch = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n",
    "            'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', \n",
    "            'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "            'sold_built_age', 'sold_Remod_age',  'MasVnrType', 'MasVnrArea', 'Foundation', 'BsmtExposure',\n",
    "            'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', \n",
    "            'BsmtHalfBath','FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n",
    "            'TotRmsAbvGrd', 'Fireplaces', 'have_garage', 'GarageType', 'sold_Garage_age', 'GarageCars',\n",
    "            'GarageArea', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n",
    "            '3SsnPorch', 'ScreenPorch','PorchSF','PoolArea',\n",
    "            'MiscFeature', 'MiscVal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OverallQual',\n",
       " 'OverallCond',\n",
       " 'YearBuilt',\n",
       " 'YearRemodAdd',\n",
       " 'RoofStyle',\n",
       " 'RoofMatl',\n",
       " 'Exterior1st',\n",
       " 'Exterior2nd',\n",
       " 'ExterQual',\n",
       " 'ExterCond',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinSF1',\n",
       " 'BsmtFinType2',\n",
       " 'BsmtFinSF2',\n",
       " 'BsmtUnfSF',\n",
       " 'Heating',\n",
       " 'HeatingQC',\n",
       " 'CentralAir',\n",
       " 'Electrical',\n",
       " 'LowQualFinSF',\n",
       " 'KitchenQual',\n",
       " 'Functional',\n",
       " 'FireplaceQu',\n",
       " 'GarageYrBlt',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'PoolQC',\n",
       " 'Fence',\n",
       " 'MoSold',\n",
       " 'YrSold',\n",
       " 'SaleType',\n",
       " 'SaleCondition']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the rest of the column\n",
    "rest_ch = [col for col in house_residential.columns if col not in fixed_ch]\n",
    "rest_ch = rest_ch[1:-1]\n",
    "rest_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review time factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2406408c860>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 276,
       "width": 391
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sold year price info\n",
    "house_residential.groupby(['YrSold']).agg({'SalePrice': ['mean','std'] }).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting functions to check the significant difference by p-value, if nothing print, means no significant different\n",
    "\n",
    "\n",
    "def p_year_test(house_residential, year1, year2):\n",
    "\n",
    "    t_stat, p_value = stats.ttest_ind(house_residential.loc[ house_residential.YrSold==year1, 'SalePrice'],house_residential.loc[ house_residential.YrSold==year2, 'SalePrice'])\n",
    "    if p_value<0.05:\n",
    "        print(f'Between {year1} and {year2} the p_value is {p_value}\\n')\n",
    "\n",
    "        \n",
    "def p_year_build_test(house_residential, year1, year2):\n",
    "\n",
    "    t_stat, p_value = stats.ttest_ind(house_residential.loc[ house_residential.YearBuilt==year1, 'SalePrice'],house_residential.loc[ house_residential.YearBuilt==year2, 'SalePrice'])\n",
    "    if p_value<0.05:\n",
    "        print(f'Between {year1} and {year2} the p_value is {p_value}\\n')        \n",
    "        \n",
    "        \n",
    "        \n",
    "def p_month_test(house_residential, m1, m2):\n",
    "\n",
    "    t_stat, p_value = stats.ttest_ind(house_residential.loc[ house_residential.MoSold==m1, 'SalePrice'],house_residential.loc[ house_residential.MoSold==m2, 'SalePrice'])\n",
    "    if p_value<0.05:\n",
    "        print(f'Between {m1} and {m2} the p_value is {p_value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is significant difference between the price of each sold year\n",
    "# no significant difference, it is possible to use pass year data to predict 2010 price\n",
    "for year in range(2006,2011):\n",
    "    for compare in range(year+1,2011):\n",
    "        p_year_test(house_residential, year, compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 1950 and 1960 the p_value is 0.006999309961260744\n",
      "\n",
      "Between 1950 and 1964 the p_value is 0.0019898023011491015\n",
      "\n",
      "Between 1950 and 1966 the p_value is 0.016036172417833224\n",
      "\n",
      "Between 1950 and 1968 the p_value is 0.0027678743161151237\n",
      "\n",
      "Between 1950 and 1969 the p_value is 0.008395118412309936\n",
      "\n",
      "Between 1950 and 1974 the p_value is 0.001205281675727996\n",
      "\n",
      "Between 1950 and 1976 the p_value is 0.003829503311702932\n",
      "\n",
      "Between 1950 and 1977 the p_value is 0.0004173916377547709\n",
      "\n",
      "Between 1950 and 1978 the p_value is 0.006518684319980653\n",
      "\n",
      "Between 1950 and 1980 the p_value is 0.0167371288557425\n",
      "\n",
      "Between 1950 and 1981 the p_value is 3.292807010855059e-05\n",
      "\n",
      "Between 1950 and 1983 the p_value is 0.045815783936969424\n",
      "\n",
      "Between 1950 and 1984 the p_value is 0.012397132311155848\n",
      "\n",
      "Between 1950 and 1985 the p_value is 0.004018085628005743\n",
      "\n",
      "Between 1950 and 1986 the p_value is 0.00019438595336002066\n",
      "\n",
      "Between 1950 and 1987 the p_value is 0.0034311087920962887\n",
      "\n",
      "Between 1950 and 1988 the p_value is 2.0134882342195863e-06\n",
      "\n",
      "Between 1950 and 1989 the p_value is 1.197710758112895e-06\n",
      "\n",
      "Between 1950 and 1990 the p_value is 4.707949237423926e-06\n",
      "\n",
      "Between 1950 and 1991 the p_value is 0.010153102953060185\n",
      "\n",
      "Between 1950 and 1992 the p_value is 5.3735533390502854e-05\n",
      "\n",
      "Between 1950 and 1993 the p_value is 1.4922198787095504e-06\n",
      "\n",
      "Between 1950 and 1994 the p_value is 0.0008700488733535049\n",
      "\n",
      "Between 1950 and 1995 the p_value is 3.476024185461004e-05\n",
      "\n",
      "Between 1950 and 1996 the p_value is 0.0004321585040757231\n",
      "\n",
      "Between 1950 and 1997 the p_value is 2.4327365045322127e-07\n",
      "\n",
      "Between 1950 and 1998 the p_value is 6.132693949711507e-08\n",
      "\n",
      "Between 1950 and 1999 the p_value is 4.782792361548478e-07\n",
      "\n",
      "Between 1950 and 2000 the p_value is 7.002182325676754e-09\n",
      "\n",
      "Between 1950 and 2001 the p_value is 9.719531421824858e-09\n",
      "\n",
      "Between 1950 and 2002 the p_value is 7.649307561544718e-12\n",
      "\n",
      "Between 1950 and 2003 the p_value is 2.4082108951749753e-06\n",
      "\n",
      "Between 1950 and 2004 the p_value is 6.364209441766551e-07\n",
      "\n",
      "Between 1950 and 2005 the p_value is 6.056880151415738e-07\n",
      "\n",
      "Between 1950 and 2006 the p_value is 2.441037925721534e-09\n",
      "\n",
      "Between 1950 and 2007 the p_value is 1.4091352034865944e-10\n",
      "\n",
      "Between 1950 and 2008 the p_value is 2.5942974257478177e-10\n",
      "\n",
      "Between 1950 and 2009 the p_value is 4.156893317578245e-06\n",
      "\n",
      "Between 1951 and 1964 the p_value is 0.02387625332616829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3146: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\numpy\\core\\_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 1951 and 1974 the p_value is 0.045138358952885295\n",
      "\n",
      "Between 1951 and 1981 the p_value is 0.011578764545595088\n",
      "\n",
      "Between 1951 and 1986 the p_value is 0.00956647840852032\n",
      "\n",
      "Between 1951 and 1987 the p_value is 0.011375305393709005\n",
      "\n",
      "Between 1951 and 1988 the p_value is 0.0006004844667255602\n",
      "\n",
      "Between 1951 and 1989 the p_value is 0.0005000779305980009\n",
      "\n",
      "Between 1951 and 1990 the p_value is 0.0048019277951849345\n",
      "\n",
      "Between 1951 and 1992 the p_value is 0.020073791300466288\n",
      "\n",
      "Between 1951 and 1993 the p_value is 0.0036659923802447275\n",
      "\n",
      "Between 1951 and 1995 the p_value is 0.021500101998641986\n",
      "\n",
      "Between 1951 and 1997 the p_value is 0.0005779017396698458\n",
      "\n",
      "Between 1951 and 1998 the p_value is 0.0011254720634977214\n",
      "\n",
      "Between 1951 and 1999 the p_value is 0.002933607893139462\n",
      "\n",
      "Between 1951 and 2000 the p_value is 0.00018782145707883733\n",
      "\n",
      "Between 1951 and 2001 the p_value is 0.00039414342646572413\n",
      "\n",
      "Between 1951 and 2002 the p_value is 1.5307754485067103e-06\n",
      "\n",
      "Between 1951 and 2003 the p_value is 0.010247040365508091\n",
      "\n",
      "Between 1951 and 2004 the p_value is 0.007075346683423809\n",
      "\n",
      "Between 1951 and 2005 the p_value is 0.007185909181485878\n",
      "\n",
      "Between 1951 and 2006 the p_value is 0.0008845348676780443\n",
      "\n",
      "Between 1951 and 2007 the p_value is 0.00019281489380461138\n",
      "\n",
      "Between 1951 and 2008 the p_value is 0.00012859491217742945\n",
      "\n",
      "Between 1951 and 2009 the p_value is 0.00889762423633754\n",
      "\n",
      "Between 1952 and 1964 the p_value is 0.013466820031048692\n",
      "\n",
      "Between 1952 and 1974 the p_value is 0.048957809597974446\n",
      "\n",
      "Between 1952 and 1977 the p_value is 0.035413362842357646\n",
      "\n",
      "Between 1952 and 1981 the p_value is 0.029340575578824298\n",
      "\n",
      "Between 1952 and 1986 the p_value is 0.023541723318021963\n",
      "\n",
      "Between 1952 and 1987 the p_value is 0.03870382692428334\n",
      "\n",
      "Between 1952 and 1988 the p_value is 0.0016684743658135124\n",
      "\n",
      "Between 1952 and 1989 the p_value is 0.005156190345187543\n",
      "\n",
      "Between 1952 and 1990 the p_value is 0.009670863644701159\n",
      "\n",
      "Between 1952 and 1992 the p_value is 0.03129589747126209\n",
      "\n",
      "Between 1952 and 1993 the p_value is 0.006905168127968954\n",
      "\n",
      "Between 1952 and 1995 the p_value is 0.039084460566539775\n",
      "\n",
      "Between 1952 and 1997 the p_value is 0.0015069343504795622\n",
      "\n",
      "Between 1952 and 1998 the p_value is 0.002373534061734119\n",
      "\n",
      "Between 1952 and 1999 the p_value is 0.004899020221956399\n",
      "\n",
      "Between 1952 and 2000 the p_value is 0.0004427442030547711\n",
      "\n",
      "Between 1952 and 2001 the p_value is 0.001225519445634935\n",
      "\n",
      "Between 1952 and 2002 the p_value is 9.403551806485982e-06\n",
      "\n",
      "Between 1952 and 2003 the p_value is 0.016995263843566342\n",
      "\n",
      "Between 1952 and 2004 the p_value is 0.010205262950295129\n",
      "\n",
      "Between 1952 and 2005 the p_value is 0.012638360191230009\n",
      "\n",
      "Between 1952 and 2006 the p_value is 0.002502425718031746\n",
      "\n",
      "Between 1952 and 2007 the p_value is 0.0007069153832594807\n",
      "\n",
      "Between 1952 and 2008 the p_value is 0.0007454385074537195\n",
      "\n",
      "Between 1952 and 2009 the p_value is 0.019229447482370158\n",
      "\n",
      "Between 1953 and 1981 the p_value is 0.003794905329000163\n",
      "\n",
      "Between 1953 and 1986 the p_value is 0.014541872277199807\n",
      "\n",
      "Between 1953 and 1988 the p_value is 0.0016253944692114126\n",
      "\n",
      "Between 1953 and 1989 the p_value is 0.0003894755071917877\n",
      "\n",
      "Between 1953 and 1990 the p_value is 0.0019599508902033926\n",
      "\n",
      "Between 1953 and 1992 the p_value is 0.007856134543980063\n",
      "\n",
      "Between 1953 and 1993 the p_value is 0.0012140982086392387\n",
      "\n",
      "Between 1953 and 1994 the p_value is 0.024619398421010776\n",
      "\n",
      "Between 1953 and 1995 the p_value is 0.003813016388318326\n",
      "\n",
      "Between 1953 and 1996 the p_value is 0.015546986895486328\n",
      "\n",
      "Between 1953 and 1997 the p_value is 0.00042085983342382707\n",
      "\n",
      "Between 1953 and 1998 the p_value is 0.00022913480821402677\n",
      "\n",
      "Between 1953 and 1999 the p_value is 0.000939288777190695\n",
      "\n",
      "Between 1953 and 2000 the p_value is 8.692471413878571e-05\n",
      "\n",
      "Between 1953 and 2001 the p_value is 4.076916290391279e-05\n",
      "\n",
      "Between 1953 and 2002 the p_value is 6.518969509212425e-07\n",
      "\n",
      "Between 1953 and 2003 the p_value is 0.0017645537668821492\n",
      "\n",
      "Between 1953 and 2004 the p_value is 0.0015480431907419455\n",
      "\n",
      "Between 1953 and 2005 the p_value is 0.0009431488378756\n",
      "\n",
      "Between 1953 and 2006 the p_value is 3.2692065482526815e-05\n",
      "\n",
      "Between 1953 and 2007 the p_value is 4.644709010597539e-06\n",
      "\n",
      "Between 1953 and 2008 the p_value is 1.3259421687877647e-06\n",
      "\n",
      "Between 1953 and 2009 the p_value is 0.0010611078935550522\n",
      "\n",
      "Between 1954 and 1968 the p_value is 0.04150185555855083\n",
      "\n",
      "Between 1954 and 1974 the p_value is 0.014664024965349125\n",
      "\n",
      "Between 1954 and 1976 the p_value is 0.04774284307038544\n",
      "\n",
      "Between 1954 and 1977 the p_value is 0.01413489581418603\n",
      "\n",
      "Between 1954 and 1981 the p_value is 0.00017895159950448173\n",
      "\n",
      "Between 1954 and 1985 the p_value is 0.029385663498038467\n",
      "\n",
      "Between 1954 and 1986 the p_value is 0.0026734838791657523\n",
      "\n",
      "Between 1954 and 1987 the p_value is 0.03386914842320101\n",
      "\n",
      "Between 1954 and 1988 the p_value is 0.00011443105656253081\n",
      "\n",
      "Between 1954 and 1989 the p_value is 2.2225128897436607e-05\n",
      "\n",
      "Between 1954 and 1990 the p_value is 4.847462488353751e-05\n",
      "\n",
      "Between 1954 and 1992 the p_value is 0.00026849586384268634\n",
      "\n",
      "Between 1954 and 1993 the p_value is 1.5800381764411463e-05\n",
      "\n",
      "Between 1954 and 1994 the p_value is 0.0011243279621595077\n",
      "\n",
      "Between 1954 and 1995 the p_value is 4.558490729590409e-05\n",
      "\n",
      "Between 1954 and 1996 the p_value is 0.0005263942938905202\n",
      "\n",
      "Between 1954 and 1997 the p_value is 8.62320845120041e-06\n",
      "\n",
      "Between 1954 and 1998 the p_value is 7.86039466568854e-07\n",
      "\n",
      "Between 1954 and 1999 the p_value is 6.944915143532839e-06\n",
      "\n",
      "Between 1954 and 2000 the p_value is 4.244978800476879e-07\n",
      "\n",
      "Between 1954 and 2001 the p_value is 6.919571419645345e-08\n",
      "\n",
      "Between 1954 and 2002 the p_value is 1.060224708383822e-09\n",
      "\n",
      "Between 1954 and 2003 the p_value is 7.1121349175138275e-06\n",
      "\n",
      "Between 1954 and 2004 the p_value is 4.580387697117301e-06\n",
      "\n",
      "Between 1954 and 2005 the p_value is 1.7240463549763225e-06\n",
      "\n",
      "Between 1954 and 2006 the p_value is 4.9409097384926335e-09\n",
      "\n",
      "Between 1954 and 2007 the p_value is 3.6542428291435567e-10\n",
      "\n",
      "Between 1954 and 2008 the p_value is 1.0390518508791257e-10\n",
      "\n",
      "Between 1954 and 2009 the p_value is 5.5558427566894635e-06\n",
      "\n",
      "Between 1955 and 1960 the p_value is 0.022435417846838487\n",
      "\n",
      "Between 1955 and 1964 the p_value is 0.0038891182356020986\n",
      "\n",
      "Between 1955 and 1968 the p_value is 0.012418285564524517\n",
      "\n",
      "Between 1955 and 1969 the p_value is 0.021032189250905286\n",
      "\n",
      "Between 1955 and 1974 the p_value is 0.002981405806704187\n",
      "\n",
      "Between 1955 and 1976 the p_value is 0.021516838492868607\n",
      "\n",
      "Between 1955 and 1977 the p_value is 0.003247220995155274\n",
      "\n",
      "Between 1955 and 1978 the p_value is 0.019069288289954984\n",
      "\n",
      "Between 1955 and 1980 the p_value is 0.035570318083513935\n",
      "\n",
      "Between 1955 and 1981 the p_value is 9.813752854106932e-05\n",
      "\n",
      "Between 1955 and 1984 the p_value is 0.020433320206516826\n",
      "\n",
      "Between 1955 and 1985 the p_value is 0.006009231324760231\n",
      "\n",
      "Between 1955 and 1986 the p_value is 0.00024413805867841271\n",
      "\n",
      "Between 1955 and 1987 the p_value is 0.001989842452958366\n",
      "\n",
      "Between 1955 and 1988 the p_value is 3.528261129648802e-06\n",
      "\n",
      "Between 1955 and 1989 the p_value is 9.1713672802991e-07\n",
      "\n",
      "Between 1955 and 1990 the p_value is 2.5174381605917242e-05\n",
      "\n",
      "Between 1955 and 1991 the p_value is 0.01657769178220915\n",
      "\n",
      "Between 1955 and 1992 the p_value is 0.00030363902682504075\n",
      "\n",
      "Between 1955 and 1993 the p_value is 1.2563394485506286e-05\n",
      "\n",
      "Between 1955 and 1994 the p_value is 0.003925030641115068\n",
      "\n",
      "Between 1955 and 1995 the p_value is 0.0002670933285747103\n",
      "\n",
      "Between 1955 and 1996 the p_value is 0.0021129037921247713\n",
      "\n",
      "Between 1955 and 1997 the p_value is 1.0663708510236488e-06\n",
      "\n",
      "Between 1955 and 1998 the p_value is 1.02939745508712e-06\n",
      "\n",
      "Between 1955 and 1999 the p_value is 6.5356038749798825e-06\n",
      "\n",
      "Between 1955 and 2000 the p_value is 8.182945266136335e-08\n",
      "\n",
      "Between 1955 and 2001 the p_value is 1.6129956633381366e-07\n",
      "\n",
      "Between 1955 and 2002 the p_value is 6.525589198219567e-11\n",
      "\n",
      "Between 1955 and 2003 the p_value is 4.3826742121881644e-05\n",
      "\n",
      "Between 1955 and 2004 the p_value is 1.8154153502133502e-05\n",
      "\n",
      "Between 1955 and 2005 the p_value is 1.6690184875897595e-05\n",
      "\n",
      "Between 1955 and 2006 the p_value is 1.687574279553223e-07\n",
      "\n",
      "Between 1955 and 2007 the p_value is 1.1255620210319336e-08\n",
      "\n",
      "Between 1955 and 2008 the p_value is 1.0516102319923796e-08\n",
      "\n",
      "Between 1955 and 2009 the p_value is 4.350932746485279e-05\n",
      "\n",
      "Between 1956 and 1960 the p_value is 0.04961927528692155\n",
      "\n",
      "Between 1956 and 1964 the p_value is 0.015431865664240804\n",
      "\n",
      "Between 1956 and 1968 the p_value is 0.028250030288635892\n",
      "\n",
      "Between 1956 and 1974 the p_value is 0.008810213429785597\n",
      "\n",
      "Between 1956 and 1976 the p_value is 0.042598213578197396\n",
      "\n",
      "Between 1956 and 1977 the p_value is 0.009415106481204579\n",
      "\n",
      "Between 1956 and 1978 the p_value is 0.045352538046365\n",
      "\n",
      "Between 1956 and 1981 the p_value is 0.0004241859885356356\n",
      "\n",
      "Between 1956 and 1985 the p_value is 0.016417120655977207\n",
      "\n",
      "Between 1956 and 1986 the p_value is 0.001222164231218115\n",
      "\n",
      "Between 1956 and 1987 the p_value is 0.008769151732629628\n",
      "\n",
      "Between 1956 and 1988 the p_value is 3.51668991535476e-05\n",
      "\n",
      "Between 1956 and 1989 the p_value is 1.3060245325703322e-05\n",
      "\n",
      "Between 1956 and 1990 the p_value is 0.0001239351340150228\n",
      "\n",
      "Between 1956 and 1991 the p_value is 0.036371888474995916\n",
      "\n",
      "Between 1956 and 1992 the p_value is 0.0009656372018907388\n",
      "\n",
      "Between 1956 and 1993 the p_value is 6.354772611234212e-05\n",
      "\n",
      "Between 1956 and 1994 the p_value is 0.007655130312895205\n",
      "\n",
      "Between 1956 and 1995 the p_value is 0.0007244326918514123\n",
      "\n",
      "Between 1956 and 1996 the p_value is 0.004459204241725848\n",
      "\n",
      "Between 1956 and 1997 the p_value is 9.541206296387454e-06\n",
      "\n",
      "Between 1956 and 1998 the p_value is 6.779416267813878e-06\n",
      "\n",
      "Between 1956 and 1999 the p_value is 3.550357950151226e-05\n",
      "\n",
      "Between 1956 and 2000 the p_value is 9.262497457879356e-07\n",
      "\n",
      "Between 1956 and 2001 the p_value is 1.2377263204001985e-06\n",
      "\n",
      "Between 1956 and 2002 the p_value is 1.890752859203156e-09\n",
      "\n",
      "Between 1956 and 2003 the p_value is 0.00015603642047605797\n",
      "\n",
      "Between 1956 and 2004 the p_value is 7.821282905332812e-05\n",
      "\n",
      "Between 1956 and 2005 the p_value is 6.685650708637709e-05\n",
      "\n",
      "Between 1956 and 2006 the p_value is 1.1162903183961758e-06\n",
      "\n",
      "Between 1956 and 2007 the p_value is 9.934851064812075e-08\n",
      "\n",
      "Between 1956 and 2008 the p_value is 7.614747672256247e-08\n",
      "\n",
      "Between 1956 and 2009 the p_value is 0.00014655184523108383\n",
      "\n",
      "Between 1957 and 1981 the p_value is 0.0005607563311773278\n",
      "\n",
      "Between 1957 and 1986 the p_value is 0.00607581716737287\n",
      "\n",
      "Between 1957 and 1988 the p_value is 0.0005080036398495144\n",
      "\n",
      "Between 1957 and 1989 the p_value is 3.438091064628676e-05\n",
      "\n",
      "Between 1957 and 1990 the p_value is 0.0003139047275002923\n",
      "\n",
      "Between 1957 and 1992 the p_value is 0.0016907298811140912\n",
      "\n",
      "Between 1957 and 1993 the p_value is 0.00016291328890042666\n",
      "\n",
      "Between 1957 and 1994 the p_value is 0.005643191937992876\n",
      "\n",
      "Between 1957 and 1995 the p_value is 0.00038921990536579097\n",
      "\n",
      "Between 1957 and 1996 the p_value is 0.0028104643363676953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Between 1957 and 1997 the p_value is 6.276677621448197e-05\n",
      "\n",
      "Between 1957 and 1998 the p_value is 1.700210854371334e-05\n",
      "\n",
      "Between 1957 and 1999 the p_value is 0.0001234494674580208\n",
      "\n",
      "Between 1957 and 2000 the p_value is 8.37395220969459e-06\n",
      "\n",
      "Between 1957 and 2001 the p_value is 1.3306015778107468e-06\n",
      "\n",
      "Between 1957 and 2002 the p_value is 1.9432814832195223e-08\n",
      "\n",
      "Between 1957 and 2003 the p_value is 0.00016462384488935756\n",
      "\n",
      "Between 1957 and 2004 the p_value is 0.00017367642415072615\n",
      "\n",
      "Between 1957 and 2005 the p_value is 6.328900619880618e-05\n",
      "\n",
      "Between 1957 and 2006 the p_value is 4.743137529215555e-07\n",
      "\n",
      "Between 1957 and 2007 the p_value is 3.8944759351780514e-08\n",
      "\n",
      "Between 1957 and 2008 the p_value is 4.424445904028684e-09\n",
      "\n",
      "Between 1957 and 2009 the p_value is 6.403217479944986e-05\n",
      "\n",
      "Between 1958 and 1974 the p_value is 0.013617638643586264\n",
      "\n",
      "Between 1958 and 1977 the p_value is 0.021450322318904837\n",
      "\n",
      "Between 1958 and 1981 the p_value is 9.59062879678341e-05\n",
      "\n",
      "Between 1958 and 1985 the p_value is 0.023842379786447543\n",
      "\n",
      "Between 1958 and 1986 the p_value is 0.001475535759952345\n",
      "\n",
      "Between 1958 and 1987 the p_value is 0.022384268180001336\n",
      "\n",
      "Between 1958 and 1988 the p_value is 5.2986250146901725e-05\n",
      "\n",
      "Between 1958 and 1989 the p_value is 4.929474122027515e-06\n",
      "\n",
      "Between 1958 and 1990 the p_value is 3.260283946319936e-05\n",
      "\n",
      "Between 1958 and 1991 the p_value is 0.04946816896292658\n",
      "\n",
      "Between 1958 and 1992 the p_value is 0.0002462870087506521\n",
      "\n",
      "Between 1958 and 1993 the p_value is 1.256499565424266e-05\n",
      "\n",
      "Between 1958 and 1994 the p_value is 0.0013642601402158552\n",
      "\n",
      "Between 1958 and 1995 the p_value is 5.242392946408722e-05\n",
      "\n",
      "Between 1958 and 1996 the p_value is 0.0006070240593551375\n",
      "\n",
      "Between 1958 and 1997 the p_value is 4.248229080312301e-06\n",
      "\n",
      "Between 1958 and 1998 the p_value is 6.938411146265929e-07\n",
      "\n",
      "Between 1958 and 1999 the p_value is 6.802794905665646e-06\n",
      "\n",
      "Between 1958 and 2000 the p_value is 2.575515261532923e-07\n",
      "\n",
      "Between 1958 and 2001 the p_value is 4.7770739221153405e-08\n",
      "\n",
      "Between 1958 and 2002 the p_value is 2.949979204560647e-10\n",
      "\n",
      "Between 1958 and 2003 the p_value is 1.0974565549193468e-05\n",
      "\n",
      "Between 1958 and 2004 the p_value is 7.891645828880425e-06\n",
      "\n",
      "Between 1958 and 2005 the p_value is 3.036597469081615e-06\n",
      "\n",
      "Between 1958 and 2006 the p_value is 8.762071425707232e-09\n",
      "\n",
      "Between 1958 and 2007 the p_value is 5.145949456624474e-10\n",
      "\n",
      "Between 1958 and 2008 the p_value is 1.0315501386646044e-10\n",
      "\n",
      "Between 1958 and 2009 the p_value is 6.0202219743153655e-06\n",
      "\n",
      "Between 1959 and 1981 the p_value is 0.0018229150966348835\n",
      "\n",
      "Between 1959 and 1986 the p_value is 0.027975171388518322\n",
      "\n",
      "Between 1959 and 1988 the p_value is 0.005158052704165417\n",
      "\n",
      "Between 1959 and 1989 the p_value is 0.0004261530331626915\n",
      "\n",
      "Between 1959 and 1990 the p_value is 0.0011513298541580925\n",
      "\n",
      "Between 1959 and 1992 the p_value is 0.0032884207014623225\n",
      "\n",
      "Between 1959 and 1993 the p_value is 0.0005092557417918222\n",
      "\n",
      "Between 1959 and 1994 the p_value is 0.0036040054337872158\n",
      "\n",
      "Between 1959 and 1995 the p_value is 0.00021554666674697856\n",
      "\n",
      "Between 1959 and 1996 the p_value is 0.0017151726803417999\n",
      "\n",
      "Between 1959 and 1997 the p_value is 0.0005506072851452538\n",
      "\n",
      "Between 1959 and 1998 the p_value is 4.915934070055105e-05\n",
      "\n",
      "Between 1959 and 1999 the p_value is 0.00036655188986267916\n",
      "\n",
      "Between 1959 and 2000 the p_value is 7.87159052187109e-05\n",
      "\n",
      "Between 1959 and 2001 the p_value is 3.1196670155224485e-06\n",
      "\n",
      "Between 1959 and 2002 the p_value is 5.899379358465322e-07\n",
      "\n",
      "Between 1959 and 2003 the p_value is 0.00011066959035652755\n",
      "\n",
      "Between 1959 and 2004 the p_value is 0.00018881266995527648\n",
      "\n",
      "Between 1959 and 2005 the p_value is 3.242584005032029e-05\n",
      "\n",
      "Between 1959 and 2006 the p_value is 1.2367420379082556e-07\n",
      "\n",
      "Between 1959 and 2007 the p_value is 1.4174241807766509e-08\n",
      "\n",
      "Between 1959 and 2008 the p_value is 4.3779072635201954e-10\n",
      "\n",
      "Between 1959 and 2009 the p_value is 3.362683634054341e-05\n",
      "\n",
      "Between 1960 and 1971 the p_value is 0.040480858245538105\n",
      "\n",
      "Between 1960 and 1981 the p_value is 0.0026838213572001374\n",
      "\n",
      "Between 1960 and 1986 the p_value is 0.02371913891498286\n",
      "\n",
      "Between 1960 and 1988 the p_value is 0.004982094366262554\n",
      "\n",
      "Between 1960 and 1989 the p_value is 0.00013920489866950137\n",
      "\n",
      "Between 1960 and 1990 the p_value is 0.0028937378992915045\n",
      "\n",
      "Between 1960 and 1992 the p_value is 0.01137565359286267\n",
      "\n",
      "Between 1960 and 1993 the p_value is 0.0021152657948895017\n",
      "\n",
      "Between 1960 and 1994 the p_value is 0.021939406773957972\n",
      "\n",
      "Between 1960 and 1995 the p_value is 0.0025768714514190086\n",
      "\n",
      "Between 1960 and 1996 the p_value is 0.01189364714221358\n",
      "\n",
      "Between 1960 and 1997 the p_value is 0.000916223396730758\n",
      "\n",
      "Between 1960 and 1998 the p_value is 0.0004316946097390718\n",
      "\n",
      "Between 1960 and 1999 the p_value is 0.002394003552264182\n",
      "\n",
      "Between 1960 and 2000 the p_value is 0.00030530495817077043\n",
      "\n",
      "Between 1960 and 2001 the p_value is 3.3217819714576684e-05\n",
      "\n",
      "Between 1960 and 2002 the p_value is 1.2119220206866865e-06\n",
      "\n",
      "Between 1960 and 2003 the p_value is 0.0024088043850818445\n",
      "\n",
      "Between 1960 and 2004 the p_value is 0.003911470994350894\n",
      "\n",
      "Between 1960 and 2005 the p_value is 0.0012652865738299384\n",
      "\n",
      "Between 1960 and 2006 the p_value is 2.359030042434088e-05\n",
      "\n",
      "Between 1960 and 2007 the p_value is 2.754496993436126e-06\n",
      "\n",
      "Between 1960 and 2008 the p_value is 1.2527646333638267e-07\n",
      "\n",
      "Between 1960 and 2009 the p_value is 0.0005859787228158109\n",
      "\n",
      "Between 1961 and 1964 the p_value is 0.00903774076730165\n",
      "\n",
      "Between 1961 and 1968 the p_value is 0.03783470016724723\n",
      "\n",
      "Between 1961 and 1974 the p_value is 0.006738876607660879\n",
      "\n",
      "Between 1961 and 1977 the p_value is 0.01437105995635088\n",
      "\n",
      "Between 1961 and 1981 the p_value is 0.00022521000332041056\n",
      "\n",
      "Between 1961 and 1984 the p_value is 0.036844075657558976\n",
      "\n",
      "Between 1961 and 1985 the p_value is 0.00939967705394115\n",
      "\n",
      "Between 1961 and 1986 the p_value is 0.0003406371495351288\n",
      "\n",
      "Between 1961 and 1987 the p_value is 0.0012143053314952691\n",
      "\n",
      "Between 1961 and 1988 the p_value is 6.825486284225301e-06\n",
      "\n",
      "Between 1961 and 1989 the p_value is 7.734893269833682e-07\n",
      "\n",
      "Between 1961 and 1990 the p_value is 8.623590403440024e-05\n",
      "\n",
      "Between 1961 and 1991 the p_value is 0.02682475133262845\n",
      "\n",
      "Between 1961 and 1992 the p_value is 0.0009628553822845822\n",
      "\n",
      "Between 1961 and 1993 the p_value is 5.5596085098200846e-05\n",
      "\n",
      "Between 1961 and 1994 the p_value is 0.009193048249290934\n",
      "\n",
      "Between 1961 and 1995 the p_value is 0.000863681101719308\n",
      "\n",
      "Between 1961 and 1996 the p_value is 0.005203896885768247\n",
      "\n",
      "Between 1961 and 1997 the p_value is 3.710447720462335e-06\n",
      "\n",
      "Between 1961 and 1998 the p_value is 6.915302447751958e-06\n",
      "\n",
      "Between 1961 and 1999 the p_value is 3.883438395037378e-05\n",
      "\n",
      "Between 1961 and 2000 the p_value is 5.514719040368659e-07\n",
      "\n",
      "Between 1961 and 2001 the p_value is 1.0359600537297817e-06\n",
      "\n",
      "Between 1961 and 2002 the p_value is 3.909769284309653e-10\n",
      "\n",
      "Between 1961 and 2003 the p_value is 0.0002381026876496187\n",
      "\n",
      "Between 1961 and 2004 the p_value is 0.00013493419434576511\n",
      "\n",
      "Between 1961 and 2005 the p_value is 0.00011160848956866628\n",
      "\n",
      "Between 1961 and 2006 the p_value is 1.937791930687581e-06\n",
      "\n",
      "Between 1961 and 2007 the p_value is 1.5405384821888164e-07\n",
      "\n",
      "Between 1961 and 2008 the p_value is 8.576044998838912e-08\n",
      "\n",
      "Between 1961 and 2009 the p_value is 0.0001707377987002987\n",
      "\n",
      "Between 1962 and 1964 the p_value is 0.03702116547152499\n",
      "\n",
      "Between 1962 and 1974 the p_value is 0.007569544282751415\n",
      "\n",
      "Between 1962 and 1977 the p_value is 0.02813896896327888\n",
      "\n",
      "Between 1962 and 1981 the p_value is 5.2100069424961344e-05\n",
      "\n",
      "Between 1962 and 1985 the p_value is 0.009094081525354871\n",
      "\n",
      "Between 1962 and 1986 the p_value is 0.00019079936125323868\n",
      "\n",
      "Between 1962 and 1987 the p_value is 0.0016336883584969937\n",
      "\n",
      "Between 1962 and 1988 the p_value is 3.7009038307837237e-06\n",
      "\n",
      "Between 1962 and 1989 the p_value is 8.283707740577062e-08\n",
      "\n",
      "Between 1962 and 1990 the p_value is 2.7048556926102907e-05\n",
      "\n",
      "Between 1962 and 1991 the p_value is 0.027838789976051354\n",
      "\n",
      "Between 1962 and 1992 the p_value is 0.000401545995822123\n",
      "\n",
      "Between 1962 and 1993 the p_value is 1.7477777804940415e-05\n",
      "\n",
      "Between 1962 and 1994 the p_value is 0.004143944631396137\n",
      "\n",
      "Between 1962 and 1995 the p_value is 0.00023377041080209184\n",
      "\n",
      "Between 1962 and 1996 the p_value is 0.001976681196735369\n",
      "\n",
      "Between 1962 and 1997 the p_value is 1.1086021139558688e-06\n",
      "\n",
      "Between 1962 and 1998 the p_value is 1.658059249486506e-06\n",
      "\n",
      "Between 1962 and 1999 the p_value is 1.4029436606562545e-05\n",
      "\n",
      "Between 1962 and 2000 the p_value is 1.5568699987806628e-07\n",
      "\n",
      "Between 1962 and 2001 the p_value is 1.2579507376384293e-07\n",
      "\n",
      "Between 1962 and 2002 the p_value is 4.049491252133566e-11\n",
      "\n",
      "Between 1962 and 2003 the p_value is 7.359344467950532e-05\n",
      "\n",
      "Between 1962 and 2004 the p_value is 5.2298300208188285e-05\n",
      "\n",
      "Between 1962 and 2005 the p_value is 2.967473197989429e-05\n",
      "\n",
      "Between 1962 and 2006 the p_value is 2.059303160905121e-07\n",
      "\n",
      "Between 1962 and 2007 the p_value is 1.1166231758743469e-08\n",
      "\n",
      "Between 1962 and 2008 the p_value is 2.7080101566226744e-09\n",
      "\n",
      "Between 1962 and 2009 the p_value is 3.287314855227996e-05\n",
      "\n",
      "Between 1963 and 1964 the p_value is 0.01821471933341875\n",
      "\n",
      "Between 1963 and 1974 the p_value is 0.00731155273265271\n",
      "\n",
      "Between 1963 and 1977 the p_value is 0.02244816405416357\n",
      "\n",
      "Between 1963 and 1981 the p_value is 0.00011658983379949903\n",
      "\n",
      "Between 1963 and 1985 the p_value is 0.008949619520642133\n",
      "\n",
      "Between 1963 and 1986 the p_value is 0.00022963160910859136\n",
      "\n",
      "Between 1963 and 1987 the p_value is 0.0010142625859925663\n",
      "\n",
      "Between 1963 and 1988 the p_value is 4.5205243872018435e-06\n",
      "\n",
      "Between 1963 and 1989 the p_value is 1.9496646872307563e-07\n",
      "\n",
      "Between 1963 and 1990 the p_value is 5.429675278843708e-05\n",
      "\n",
      "Between 1963 and 1991 the p_value is 0.027244919656316435\n",
      "\n",
      "Between 1963 and 1992 the p_value is 0.0007085469157906768\n",
      "\n",
      "Between 1963 and 1993 the p_value is 3.6555126819073755e-05\n",
      "\n",
      "Between 1963 and 1994 the p_value is 0.006989142767890268\n",
      "\n",
      "Between 1963 and 1995 the p_value is 0.0005378712532601493\n",
      "\n",
      "Between 1963 and 1996 the p_value is 0.003678626016942294\n",
      "\n",
      "Between 1963 and 1997 the p_value is 2.152356051215151e-06\n",
      "\n",
      "Between 1963 and 1998 the p_value is 4.219801016933468e-06\n",
      "\n",
      "Between 1963 and 1999 the p_value is 2.8277028621938692e-05\n",
      "\n",
      "Between 1963 and 2000 the p_value is 3.434289870965519e-07\n",
      "\n",
      "Between 1963 and 2001 the p_value is 4.606294998739957e-07\n",
      "\n",
      "Between 1963 and 2002 the p_value is 1.3984587175556113e-10\n",
      "\n",
      "Between 1963 and 2003 the p_value is 0.0001654716213560882\n",
      "\n",
      "Between 1963 and 2004 the p_value is 0.00010665400174445609\n",
      "\n",
      "Between 1963 and 2005 the p_value is 7.413719446837246e-05\n",
      "\n",
      "Between 1963 and 2006 the p_value is 8.979173265370325e-07\n",
      "\n",
      "Between 1963 and 2007 the p_value is 6.058045019692634e-08\n",
      "\n",
      "Between 1963 and 2008 the p_value is 2.2290555843130403e-08\n",
      "\n",
      "Between 1963 and 2009 the p_value is 9.277013523895351e-05\n",
      "\n",
      "Between 1964 and 1971 the p_value is 0.028822509463441247\n",
      "\n",
      "Between 1964 and 1981 the p_value is 0.0013067159460019586\n",
      "\n",
      "Between 1964 and 1982 the p_value is 0.004305331762029174\n",
      "\n",
      "Between 1964 and 1986 the p_value is 0.006107729405402519\n",
      "\n",
      "Between 1964 and 1987 the p_value is 0.029569811978713438\n",
      "\n",
      "Between 1964 and 1988 the p_value is 0.0008136768419545707\n",
      "\n",
      "Between 1964 and 1989 the p_value is 5.074096741151052e-06\n",
      "\n",
      "Between 1964 and 1990 the p_value is 0.0017212047650410642\n",
      "\n",
      "Between 1964 and 1992 the p_value is 0.010304125525964316\n",
      "\n",
      "Between 1964 and 1993 the p_value is 0.0015737338769800816\n",
      "\n",
      "Between 1964 and 1994 the p_value is 0.028913204128204674\n",
      "\n",
      "Between 1964 and 1995 the p_value is 0.0037365255883595955\n",
      "\n",
      "Between 1964 and 1996 the p_value is 0.016020766859564245\n",
      "\n",
      "Between 1964 and 1997 the p_value is 0.00026542472804870606\n",
      "\n",
      "Between 1964 and 1998 the p_value is 0.00037331609898098406\n",
      "\n",
      "Between 1964 and 1999 the p_value is 0.002082320433151505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Between 1964 and 2000 the p_value is 0.00012794785285341007\n",
      "\n",
      "Between 1964 and 2001 the p_value is 2.8040835030246293e-05\n",
      "\n",
      "Between 1964 and 2002 the p_value is 1.4323610372632213e-07\n",
      "\n",
      "Between 1964 and 2003 the p_value is 0.003622034899514504\n",
      "\n",
      "Between 1964 and 2004 the p_value is 0.005375152985851034\n",
      "\n",
      "Between 1964 and 2005 the p_value is 0.002124439659853535\n",
      "\n",
      "Between 1964 and 2006 the p_value is 5.383469040198645e-05\n",
      "\n",
      "Between 1964 and 2007 the p_value is 5.898656876834775e-06\n",
      "\n",
      "Between 1964 and 2008 the p_value is 3.5120295910849254e-07\n",
      "\n",
      "Between 1964 and 2009 the p_value is 0.0008862599113022624\n",
      "\n",
      "Between 1965 and 1981 the p_value is 0.004660047787466683\n",
      "\n",
      "Between 1965 and 1986 the p_value is 0.049210035601724426\n",
      "\n",
      "Between 1965 and 1988 the p_value is 0.011708229439782003\n",
      "\n",
      "Between 1965 and 1989 the p_value is 0.0016902917325483746\n",
      "\n",
      "Between 1965 and 1990 the p_value is 0.0028558848458219133\n",
      "\n",
      "Between 1965 and 1992 the p_value is 0.006373648465053742\n",
      "\n",
      "Between 1965 and 1993 the p_value is 0.0012966089674988305\n",
      "\n",
      "Between 1965 and 1994 the p_value is 0.005418320788695102\n",
      "\n",
      "Between 1965 and 1995 the p_value is 0.00043850759189794096\n",
      "\n",
      "Between 1965 and 1996 the p_value is 0.002852003133111119\n",
      "\n",
      "Between 1965 and 1997 the p_value is 0.0016501634530914086\n",
      "\n",
      "Between 1965 and 1998 the p_value is 0.00015318934831379954\n",
      "\n",
      "Between 1965 and 1999 the p_value is 0.0008943876538003588\n",
      "\n",
      "Between 1965 and 2000 the p_value is 0.0002772185653646983\n",
      "\n",
      "Between 1965 and 2001 the p_value is 1.3963211729661037e-05\n",
      "\n",
      "Between 1965 and 2002 the p_value is 4.693166699186359e-06\n",
      "\n",
      "Between 1965 and 2003 the p_value is 0.0002166886742192828\n",
      "\n",
      "Between 1965 and 2004 the p_value is 0.0003631708052749971\n",
      "\n",
      "Between 1965 and 2005 the p_value is 6.507968539862591e-05\n",
      "\n",
      "Between 1965 and 2006 the p_value is 3.8244917093508593e-07\n",
      "\n",
      "Between 1965 and 2007 the p_value is 6.246519660871773e-08\n",
      "\n",
      "Between 1965 and 2008 the p_value is 2.529148905770535e-09\n",
      "\n",
      "Between 1965 and 2009 the p_value is 8.452896720379953e-05\n",
      "\n",
      "Between 1966 and 1981 the p_value is 0.003183893729257483\n",
      "\n",
      "Between 1966 and 1986 the p_value is 0.030334086280962933\n",
      "\n",
      "Between 1966 and 1988 the p_value is 0.006640028131562287\n",
      "\n",
      "Between 1966 and 1989 the p_value is 0.00032189784084008823\n",
      "\n",
      "Between 1966 and 1990 the p_value is 0.002933475945131583\n",
      "\n",
      "Between 1966 and 1992 the p_value is 0.010048574285400258\n",
      "\n",
      "Between 1966 and 1993 the p_value is 0.0019073226211545908\n",
      "\n",
      "Between 1966 and 1994 the p_value is 0.017193358209340846\n",
      "\n",
      "Between 1966 and 1995 the p_value is 0.0018832314211086885\n",
      "\n",
      "Between 1966 and 1996 the p_value is 0.009260783779927116\n",
      "\n",
      "Between 1966 and 1997 the p_value is 0.0011042977465735794\n",
      "\n",
      "Between 1966 and 1998 the p_value is 0.0003459270801036295\n",
      "\n",
      "Between 1966 and 1999 the p_value is 0.0019233651717148743\n",
      "\n",
      "Between 1966 and 2000 the p_value is 0.00030293321403439765\n",
      "\n",
      "Between 1966 and 2001 the p_value is 2.8249943756832117e-05\n",
      "\n",
      "Between 1966 and 2002 the p_value is 1.841598042356008e-06\n",
      "\n",
      "Between 1966 and 2003 the p_value is 0.0015532524467819226\n",
      "\n",
      "Between 1966 and 2004 the p_value is 0.002480942287359867\n",
      "\n",
      "Between 1966 and 2005 the p_value is 0.0007488414371362964\n",
      "\n",
      "Between 1966 and 2006 the p_value is 1.1483834353640072e-05\n",
      "\n",
      "Between 1966 and 2007 the p_value is 1.382684108695684e-06\n",
      "\n",
      "Between 1966 and 2008 the p_value is 6.57224371271406e-08\n",
      "\n",
      "Between 1966 and 2009 the p_value is 0.00041501220593794853\n",
      "\n",
      "Between 1967 and 1974 the p_value is 0.025553424977614245\n",
      "\n",
      "Between 1967 and 1977 the p_value is 0.037837112692802655\n",
      "\n",
      "Between 1967 and 1981 the p_value is 0.0007027949066837113\n",
      "\n",
      "Between 1967 and 1985 the p_value is 0.039953608394650446\n",
      "\n",
      "Between 1967 and 1986 the p_value is 0.003988565149445386\n",
      "\n",
      "Between 1967 and 1987 the p_value is 0.03181283960471261\n",
      "\n",
      "Between 1967 and 1988 the p_value is 0.00022759565125767334\n",
      "\n",
      "Between 1967 and 1989 the p_value is 4.5913784253311434e-05\n",
      "\n",
      "Between 1967 and 1990 the p_value is 0.00027470252283750234\n",
      "\n",
      "Between 1967 and 1992 the p_value is 0.0015770290746019084\n",
      "\n",
      "Between 1967 and 1993 the p_value is 0.0001388422182085825\n",
      "\n",
      "Between 1967 and 1994 the p_value is 0.007553038290547192\n",
      "\n",
      "Between 1967 and 1995 the p_value is 0.0006740605930010616\n",
      "\n",
      "Between 1967 and 1996 the p_value is 0.004208684362165637\n",
      "\n",
      "Between 1967 and 1997 the p_value is 3.9695421382092026e-05\n",
      "\n",
      "Between 1967 and 1998 the p_value is 1.530234657084603e-05\n",
      "\n",
      "Between 1967 and 1999 the p_value is 8.762497359626004e-05\n",
      "\n",
      "Between 1967 and 2000 the p_value is 4.492083117769364e-06\n",
      "\n",
      "Between 1967 and 2001 the p_value is 2.066852948298399e-06\n",
      "\n",
      "Between 1967 and 2002 the p_value is 1.3930556978573e-08\n",
      "\n",
      "Between 1967 and 2003 the p_value is 0.00019744545946169631\n",
      "\n",
      "Between 1967 and 2004 the p_value is 0.00013994626788541993\n",
      "\n",
      "Between 1967 and 2005 the p_value is 8.159534190058294e-05\n",
      "\n",
      "Between 1967 and 2006 the p_value is 1.0780286767564476e-06\n",
      "\n",
      "Between 1967 and 2007 the p_value is 1.0130944267609871e-07\n",
      "\n",
      "Between 1967 and 2008 the p_value is 3.5424401750977054e-08\n",
      "\n",
      "Between 1967 and 2009 the p_value is 0.00013175362623955695\n",
      "\n",
      "Between 1968 and 1970 the p_value is 0.025667356802915302\n",
      "\n",
      "Between 1968 and 1971 the p_value is 0.015508788254449457\n",
      "\n",
      "Between 1968 and 1981 the p_value is 0.003148320311506571\n",
      "\n",
      "Between 1968 and 1986 the p_value is 0.04538340659824391\n",
      "\n",
      "Between 1968 and 1988 the p_value is 0.014006427418924928\n",
      "\n",
      "Between 1968 and 1989 the p_value is 0.0002986534024375036\n",
      "\n",
      "Between 1968 and 1990 the p_value is 0.004069408151376547\n",
      "\n",
      "Between 1968 and 1992 the p_value is 0.01275483279446025\n",
      "\n",
      "Between 1968 and 1993 the p_value is 0.002769908704743214\n",
      "\n",
      "Between 1968 and 1994 the p_value is 0.014881057114070855\n",
      "\n",
      "Between 1968 and 1995 the p_value is 0.0013490909685553091\n",
      "\n",
      "Between 1968 and 1996 the p_value is 0.007292719125316329\n",
      "\n",
      "Between 1968 and 1997 the p_value is 0.0020288442542877364\n",
      "\n",
      "Between 1968 and 1998 the p_value is 0.0005083620123732814\n",
      "\n",
      "Between 1968 and 1999 the p_value is 0.003279138135761172\n",
      "\n",
      "Between 1968 and 2000 the p_value is 0.0006786123143487853\n",
      "\n",
      "Between 1968 and 2001 the p_value is 2.6408957243943398e-05\n",
      "\n",
      "Between 1968 and 2002 the p_value is 3.633930161787441e-06\n",
      "\n",
      "Between 1968 and 2003 the p_value is 0.001659320772814696\n",
      "\n",
      "Between 1968 and 2004 the p_value is 0.0038044186706304123\n",
      "\n",
      "Between 1968 and 2005 the p_value is 0.0007654016382905823\n",
      "\n",
      "Between 1968 and 2006 the p_value is 7.454330767508177e-06\n",
      "\n",
      "Between 1968 and 2007 the p_value is 8.209987778940602e-07\n",
      "\n",
      "Between 1968 and 2008 the p_value is 1.1268894261637288e-08\n",
      "\n",
      "Between 1968 and 2009 the p_value is 0.00026079242529316866\n",
      "\n",
      "Between 1969 and 1981 the p_value is 0.003414500728843937\n",
      "\n",
      "Between 1969 and 1982 the p_value is 0.0494983172204985\n",
      "\n",
      "Between 1969 and 1986 the p_value is 0.018981649716126005\n",
      "\n",
      "Between 1969 and 1988 the p_value is 0.0035617816282731505\n",
      "\n",
      "Between 1969 and 1989 the p_value is 0.00010525896831186117\n",
      "\n",
      "Between 1969 and 1990 the p_value is 0.003625914632472543\n",
      "\n",
      "Between 1969 and 1992 the p_value is 0.015194069399593587\n",
      "\n",
      "Between 1969 and 1993 the p_value is 0.0029293763394425106\n",
      "\n",
      "Between 1969 and 1994 the p_value is 0.034031492605539816\n",
      "\n",
      "Between 1969 and 1995 the p_value is 0.005104171991628556\n",
      "\n",
      "Between 1969 and 1996 the p_value is 0.019892640598153415\n",
      "\n",
      "Between 1969 and 1997 the p_value is 0.000916353825504394\n",
      "\n",
      "Between 1969 and 1998 the p_value is 0.0007189380544409777\n",
      "\n",
      "Between 1969 and 1999 the p_value is 0.0034074104156800985\n",
      "\n",
      "Between 1969 and 2000 the p_value is 0.0003661585622365489\n",
      "\n",
      "Between 1969 and 2001 the p_value is 7.129872230061305e-05\n",
      "\n",
      "Between 1969 and 2002 the p_value is 1.3624884676149852e-06\n",
      "\n",
      "Between 1969 and 2003 the p_value is 0.004632087054876088\n",
      "\n",
      "Between 1969 and 2004 the p_value is 0.006641636860832415\n",
      "\n",
      "Between 1969 and 2005 the p_value is 0.0027209806881778216\n",
      "\n",
      "Between 1969 and 2006 the p_value is 8.536583769812311e-05\n",
      "\n",
      "Between 1969 and 2007 the p_value is 1.1437766046591306e-05\n",
      "\n",
      "Between 1969 and 2008 the p_value is 8.343768936894736e-07\n",
      "\n",
      "Between 1969 and 2009 the p_value is 0.0013562747891868404\n",
      "\n",
      "Between 1970 and 1974 the p_value is 0.013527378231616092\n",
      "\n",
      "Between 1970 and 1976 the p_value is 0.024980054371139\n",
      "\n",
      "Between 1970 and 1977 the p_value is 0.007288676168624822\n",
      "\n",
      "Between 1970 and 1981 the p_value is 0.0002872322046895455\n",
      "\n",
      "Between 1970 and 1985 the p_value is 0.03168670388197694\n",
      "\n",
      "Between 1970 and 1986 the p_value is 0.003884993960093712\n",
      "\n",
      "Between 1970 and 1987 the p_value is 0.042559003166439305\n",
      "\n",
      "Between 1970 and 1988 the p_value is 0.00017902740231942298\n",
      "\n",
      "Between 1970 and 1989 the p_value is 6.956455663325502e-05\n",
      "\n",
      "Between 1970 and 1990 the p_value is 6.015804809403486e-05\n",
      "\n",
      "Between 1970 and 1992 the p_value is 0.0002553801115084466\n",
      "\n",
      "Between 1970 and 1993 the p_value is 1.647012161182833e-05\n",
      "\n",
      "Between 1970 and 1994 the p_value is 0.000832717599692851\n",
      "\n",
      "Between 1970 and 1995 the p_value is 3.5338187591619694e-05\n",
      "\n",
      "Between 1970 and 1996 the p_value is 0.00041475667970909805\n",
      "\n",
      "Between 1970 and 1997 the p_value is 1.3258102572775771e-05\n",
      "\n",
      "Between 1970 and 1998 the p_value is 7.141657261352729e-07\n",
      "\n",
      "Between 1970 and 1999 the p_value is 5.676211040673872e-06\n",
      "\n",
      "Between 1970 and 2000 the p_value is 5.170934917109778e-07\n",
      "\n",
      "Between 1970 and 2001 the p_value is 8.339991900365926e-08\n",
      "\n",
      "Between 1970 and 2002 the p_value is 2.5593457759083776e-09\n",
      "\n",
      "Between 1970 and 2003 the p_value is 3.6384549078788483e-06\n",
      "\n",
      "Between 1970 and 2004 the p_value is 1.964356251059567e-06\n",
      "\n",
      "Between 1970 and 2005 the p_value is 7.356668971226552e-07\n",
      "\n",
      "Between 1970 and 2006 the p_value is 2.0570436644809076e-09\n",
      "\n",
      "Between 1970 and 2007 the p_value is 1.9921363073711565e-10\n",
      "\n",
      "Between 1970 and 2008 the p_value is 9.407423645940498e-11\n",
      "\n",
      "Between 1970 and 2009 the p_value is 4.558842946898924e-06\n",
      "\n",
      "Between 1971 and 1974 the p_value is 0.008873059390295257\n",
      "\n",
      "Between 1971 and 1976 the p_value is 0.015324687638304973\n",
      "\n",
      "Between 1971 and 1977 the p_value is 0.003764737122888468\n",
      "\n",
      "Between 1971 and 1978 the p_value is 0.043322825726717404\n",
      "\n",
      "Between 1971 and 1981 the p_value is 0.0002364941147790704\n",
      "\n",
      "Between 1971 and 1985 the p_value is 0.023116108334796923\n",
      "\n",
      "Between 1971 and 1986 the p_value is 0.0026257165659514634\n",
      "\n",
      "Between 1971 and 1987 the p_value is 0.030795530983900657\n",
      "\n",
      "Between 1971 and 1988 the p_value is 9.502783173101232e-05\n",
      "\n",
      "Between 1971 and 1989 the p_value is 4.9059764413068724e-05\n",
      "\n",
      "Between 1971 and 1990 the p_value is 4.3718860102231716e-05\n",
      "\n",
      "Between 1971 and 1991 the p_value is 0.04163429220815318\n",
      "\n",
      "Between 1971 and 1992 the p_value is 0.00021160390658637405\n",
      "\n",
      "Between 1971 and 1993 the p_value is 1.21144483621643e-05\n",
      "\n",
      "Between 1971 and 1994 the p_value is 0.0009649191359116961\n",
      "\n",
      "Between 1971 and 1995 the p_value is 4.4206816254708705e-05\n",
      "\n",
      "Between 1971 and 1996 the p_value is 0.0004957749588348352\n",
      "\n",
      "Between 1971 and 1997 the p_value is 7.693986860601029e-06\n",
      "\n",
      "Between 1971 and 1998 the p_value is 5.342802026571892e-07\n",
      "\n",
      "Between 1971 and 1999 the p_value is 3.960528599130104e-06\n",
      "\n",
      "Between 1971 and 2000 the p_value is 2.7452040597228e-07\n",
      "\n",
      "Between 1971 and 2001 the p_value is 7.495457397047316e-08\n",
      "\n",
      "Between 1971 and 2002 the p_value is 1.2499740612077088e-09\n",
      "\n",
      "Between 1971 and 2003 the p_value is 3.9796414055049216e-06\n",
      "\n",
      "Between 1971 and 2004 the p_value is 1.7325498236693095e-06\n",
      "\n",
      "Between 1971 and 2005 the p_value is 8.661843126096104e-07\n",
      "\n",
      "Between 1971 and 2006 the p_value is 3.1054723142528943e-09\n",
      "\n",
      "Between 1971 and 2007 the p_value is 2.843727607862728e-10\n",
      "\n",
      "Between 1971 and 2008 the p_value is 2.2386033584563582e-10\n",
      "\n",
      "Between 1971 and 2009 the p_value is 5.950498415544996e-06\n",
      "\n",
      "Between 1972 and 1974 the p_value is 0.03889219823720167\n",
      "\n",
      "Between 1972 and 1977 the p_value is 0.035395191812135914\n",
      "\n",
      "Between 1972 and 1981 the p_value is 0.0009548890419716363\n",
      "\n",
      "Between 1972 and 1986 the p_value is 0.011357554484152377\n",
      "\n",
      "Between 1972 and 1988 the p_value is 0.0010204728654556877\n",
      "\n",
      "Between 1972 and 1989 the p_value is 0.0002812902439449458\n",
      "\n",
      "Between 1972 and 1990 the p_value is 0.0003011784282152866\n",
      "\n",
      "Between 1972 and 1992 the p_value is 0.0009797845799374197\n",
      "\n",
      "Between 1972 and 1993 the p_value is 0.00010070219186951447\n",
      "\n",
      "Between 1972 and 1994 the p_value is 0.0019277747393131955\n",
      "\n",
      "Between 1972 and 1995 the p_value is 0.00011112062857769028\n",
      "\n",
      "Between 1972 and 1996 the p_value is 0.0009989801360129691\n",
      "\n",
      "Between 1972 and 1997 the p_value is 9.760035679624495e-05\n",
      "\n",
      "Between 1972 and 1998 the p_value is 6.572660557642964e-06\n",
      "\n",
      "Between 1972 and 1999 the p_value is 4.542459583237193e-05\n",
      "\n",
      "Between 1972 and 2000 the p_value is 6.752534766265905e-06\n",
      "\n",
      "Between 1972 and 2001 the p_value is 7.306947235151666e-07\n",
      "\n",
      "Between 1972 and 2002 the p_value is 5.5704752004636707e-08\n",
      "\n",
      "Between 1972 and 2003 the p_value is 2.079500417995513e-05\n",
      "\n",
      "Between 1972 and 2004 the p_value is 1.7092220381357865e-05\n",
      "\n",
      "Between 1972 and 2005 the p_value is 5.059729940621303e-06\n",
      "\n",
      "Between 1972 and 2006 the p_value is 2.1079658851762635e-08\n",
      "\n",
      "Between 1972 and 2007 the p_value is 2.5106247183381376e-09\n",
      "\n",
      "Between 1972 and 2008 the p_value is 5.449035055759861e-10\n",
      "\n",
      "Between 1972 and 2009 the p_value is 1.7330286103756038e-05\n",
      "\n",
      "Between 1973 and 1981 the p_value is 0.006232976664360294\n",
      "\n",
      "Between 1973 and 1986 the p_value is 0.02201424071815721\n",
      "\n",
      "Between 1973 and 1988 the p_value is 0.002619148630818671\n",
      "\n",
      "Between 1973 and 1989 the p_value is 0.001466078432752682\n",
      "\n",
      "Between 1973 and 1990 the p_value is 0.0024744551505992167\n",
      "\n",
      "Between 1973 and 1992 the p_value is 0.008013588030201309\n",
      "\n",
      "Between 1973 and 1993 the p_value is 0.0013285295196557817\n",
      "\n",
      "Between 1973 and 1994 the p_value is 0.023470426541253805\n",
      "\n",
      "Between 1973 and 1995 the p_value is 0.003966598864635981\n",
      "\n",
      "Between 1973 and 1996 the p_value is 0.01568400037327688\n",
      "\n",
      "Between 1973 and 1997 the p_value is 0.0006466098724685374\n",
      "\n",
      "Between 1973 and 1998 the p_value is 0.00023261451310175067\n",
      "\n",
      "Between 1973 and 1999 the p_value is 0.0008386402009968464\n",
      "\n",
      "Between 1973 and 2000 the p_value is 0.00010451387868616538\n",
      "\n",
      "Between 1973 and 2001 the p_value is 5.65741856272097e-05\n",
      "\n",
      "Between 1973 and 2002 the p_value is 1.6630179618129715e-06\n",
      "\n",
      "Between 1973 and 2003 the p_value is 0.0013866192600644515\n",
      "\n",
      "Between 1973 and 2004 the p_value is 0.0010110034770405717\n",
      "\n",
      "Between 1973 and 2005 the p_value is 0.000703159831892461\n",
      "\n",
      "Between 1973 and 2006 the p_value is 2.7764832344608463e-05\n",
      "\n",
      "Between 1973 and 2007 the p_value is 4.582361149690492e-06\n",
      "\n",
      "Between 1973 and 2008 the p_value is 2.229575513872857e-06\n",
      "\n",
      "Between 1973 and 2009 the p_value is 0.0011882707602386495\n",
      "\n",
      "Between 1974 and 1982 the p_value is 0.025377892764973535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Between 1974 and 1989 the p_value is 0.013147376653868292\n",
      "\n",
      "Between 1974 and 2001 the p_value is 0.012467669013488525\n",
      "\n",
      "Between 1974 and 2002 the p_value is 0.005819788999319483\n",
      "\n",
      "Between 1974 and 2006 the p_value is 0.010693402835575029\n",
      "\n",
      "Between 1974 and 2007 the p_value is 0.003587216585301447\n",
      "\n",
      "Between 1974 and 2008 the p_value is 0.00012642722848391796\n",
      "\n",
      "Between 1974 and 2009 the p_value is 0.028511207213204433\n",
      "\n",
      "Between 1975 and 1981 the p_value is 0.038592319340582396\n",
      "\n",
      "Between 1975 and 1988 the p_value is 0.04121703859797539\n",
      "\n",
      "Between 1975 and 1989 the p_value is 0.009678642378506073\n",
      "\n",
      "Between 1975 and 1990 the p_value is 0.03274219084376401\n",
      "\n",
      "Between 1975 and 1993 the p_value is 0.02624779567443235\n",
      "\n",
      "Between 1975 and 1995 the p_value is 0.032599029735894555\n",
      "\n",
      "Between 1975 and 1997 the p_value is 0.01596079459348118\n",
      "\n",
      "Between 1975 and 1998 the p_value is 0.01033017985201405\n",
      "\n",
      "Between 1975 and 1999 the p_value is 0.026823950579631703\n",
      "\n",
      "Between 1975 and 2000 the p_value is 0.007854489049867899\n",
      "\n",
      "Between 1975 and 2001 the p_value is 0.0025195679976594827\n",
      "\n",
      "Between 1975 and 2002 the p_value is 0.00030266878604093413\n",
      "\n",
      "Between 1975 and 2003 the p_value is 0.02904332261353567\n",
      "\n",
      "Between 1975 and 2004 the p_value is 0.03608870228679835\n",
      "\n",
      "Between 1975 and 2005 the p_value is 0.020565079871202846\n",
      "\n",
      "Between 1975 and 2006 the p_value is 0.002347450719212813\n",
      "\n",
      "Between 1975 and 2007 the p_value is 0.0006570748996362639\n",
      "\n",
      "Between 1975 and 2008 the p_value is 0.0001096221917704149\n",
      "\n",
      "Between 1975 and 2009 the p_value is 0.014277325930167775\n",
      "\n",
      "Between 1976 and 1981 the p_value is 0.0013616996836267577\n",
      "\n",
      "Between 1976 and 1986 the p_value is 0.037163881594493804\n",
      "\n",
      "Between 1976 and 1988 the p_value is 0.009828281549621738\n",
      "\n",
      "Between 1976 and 1989 the p_value is 0.00020236538073052112\n",
      "\n",
      "Between 1976 and 1990 the p_value is 0.001491461869593393\n",
      "\n",
      "Between 1976 and 1992 the p_value is 0.004393103384407076\n",
      "\n",
      "Between 1976 and 1993 the p_value is 0.0007545949561636846\n",
      "\n",
      "Between 1976 and 1994 the p_value is 0.003040119241888941\n",
      "\n",
      "Between 1976 and 1995 the p_value is 0.000128567850585121\n",
      "\n",
      "Between 1976 and 1996 the p_value is 0.001211208585539952\n",
      "\n",
      "Between 1976 and 1997 the p_value is 0.0008906801319853133\n",
      "\n",
      "Between 1976 and 1998 the p_value is 7.594254307971767e-05\n",
      "\n",
      "Between 1976 and 1999 the p_value is 0.0007570822656541791\n",
      "\n",
      "Between 1976 and 2000 the p_value is 0.00018193719877300973\n",
      "\n",
      "Between 1976 and 2001 the p_value is 2.1279863656052747e-06\n",
      "\n",
      "Between 1976 and 2002 the p_value is 7.107588042813224e-07\n",
      "\n",
      "Between 1976 and 2003 the p_value is 0.00014260298774461375\n",
      "\n",
      "Between 1976 and 2004 the p_value is 0.00045330887455624327\n",
      "\n",
      "Between 1976 and 2005 the p_value is 4.1415306551671264e-05\n",
      "\n",
      "Between 1976 and 2006 the p_value is 7.442759909850004e-08\n",
      "\n",
      "Between 1976 and 2007 the p_value is 6.4917414995833655e-09\n",
      "\n",
      "Between 1976 and 2008 the p_value is 2.2301414987140155e-11\n",
      "\n",
      "Between 1976 and 2009 the p_value is 1.5640120612562417e-05\n",
      "\n",
      "Between 1977 and 1981 the p_value is 0.0005563581151410068\n",
      "\n",
      "Between 1977 and 1982 the p_value is 0.031029365577856727\n",
      "\n",
      "Between 1977 and 1986 the p_value is 0.022035827629956206\n",
      "\n",
      "Between 1977 and 1988 the p_value is 0.0052634144464227\n",
      "\n",
      "Between 1977 and 1989 the p_value is 2.712026849186912e-05\n",
      "\n",
      "Between 1977 and 1990 the p_value is 0.0008693776345630813\n",
      "\n",
      "Between 1977 and 1992 the p_value is 0.0036578749986355156\n",
      "\n",
      "Between 1977 and 1993 the p_value is 0.0005219250279297921\n",
      "\n",
      "Between 1977 and 1994 the p_value is 0.0037965775544857544\n",
      "\n",
      "Between 1977 and 1995 the p_value is 0.00014729596019016244\n",
      "\n",
      "Between 1977 and 1996 the p_value is 0.0014169976843079661\n",
      "\n",
      "Between 1977 and 1997 the p_value is 0.00039989909458099603\n",
      "\n",
      "Between 1977 and 1998 the p_value is 5.6032560846393e-05\n",
      "\n",
      "Between 1977 and 1999 the p_value is 0.0006683333534723723\n",
      "\n",
      "Between 1977 and 2000 the p_value is 9.872220064117367e-05\n",
      "\n",
      "Between 1977 and 2001 the p_value is 1.0460858743436002e-06\n",
      "\n",
      "Between 1977 and 2002 the p_value is 1.2543541560752997e-07\n",
      "\n",
      "Between 1977 and 2003 the p_value is 0.0002187653953852111\n",
      "\n",
      "Between 1977 and 2004 the p_value is 0.0007247114323632674\n",
      "\n",
      "Between 1977 and 2005 the p_value is 7.532417701301536e-05\n",
      "\n",
      "Between 1977 and 2006 the p_value is 1.4672202611003355e-07\n",
      "\n",
      "Between 1977 and 2007 the p_value is 8.898380538301518e-09\n",
      "\n",
      "Between 1977 and 2008 the p_value is 2.3882498739718868e-11\n",
      "\n",
      "Between 1977 and 2009 the p_value is 1.6212230236328537e-05\n",
      "\n",
      "Between 1978 and 1981 the p_value is 0.002332235966569676\n",
      "\n",
      "Between 1978 and 1986 the p_value is 0.017461453599096063\n",
      "\n",
      "Between 1978 and 1988 the p_value is 0.0032024810169637738\n",
      "\n",
      "Between 1978 and 1989 the p_value is 7.657369864783785e-05\n",
      "\n",
      "Between 1978 and 1990 the p_value is 0.0025287167233737848\n",
      "\n",
      "Between 1978 and 1992 the p_value is 0.011053135822653543\n",
      "\n",
      "Between 1978 and 1993 the p_value is 0.0019426826242170015\n",
      "\n",
      "Between 1978 and 1994 the p_value is 0.024412732286553815\n",
      "\n",
      "Between 1978 and 1995 the p_value is 0.0030233693936960555\n",
      "\n",
      "Between 1978 and 1996 the p_value is 0.013466349459997357\n",
      "\n",
      "Between 1978 and 1997 the p_value is 0.0006636299574318453\n",
      "\n",
      "Between 1978 and 1998 the p_value is 0.0004141382765016256\n",
      "\n",
      "Between 1978 and 1999 the p_value is 0.0022481674174724933\n",
      "\n",
      "Between 1978 and 2000 the p_value is 0.00023478915529626346\n",
      "\n",
      "Between 1978 and 2001 the p_value is 3.349810816502297e-05\n",
      "\n",
      "Between 1978 and 2002 the p_value is 7.208522087914981e-07\n",
      "\n",
      "Between 1978 and 2003 the p_value is 0.002754518811631236\n",
      "\n",
      "Between 1978 and 2004 the p_value is 0.004205389737220802\n",
      "\n",
      "Between 1978 and 2005 the p_value is 0.0015043940968247457\n",
      "\n",
      "Between 1978 and 2006 the p_value is 3.240875544633497e-05\n",
      "\n",
      "Between 1978 and 2007 the p_value is 3.7706743472323847e-06\n",
      "\n",
      "Between 1978 and 2008 the p_value is 2.089341395875323e-07\n",
      "\n",
      "Between 1978 and 2009 the p_value is 0.0007083483859629089\n",
      "\n",
      "Between 1979 and 1981 the p_value is 0.022476539864401526\n",
      "\n",
      "Between 1979 and 1988 the p_value is 0.020331850004895725\n",
      "\n",
      "Between 1979 and 1989 the p_value is 0.00433204250296878\n",
      "\n",
      "Between 1979 and 1990 the p_value is 0.01757912464522853\n",
      "\n",
      "Between 1979 and 1992 the p_value is 0.04412321824381032\n",
      "\n",
      "Between 1979 and 1993 the p_value is 0.013460064565874333\n",
      "\n",
      "Between 1979 and 1995 the p_value is 0.02013306455868308\n",
      "\n",
      "Between 1979 and 1997 the p_value is 0.007105224590982556\n",
      "\n",
      "Between 1979 and 1998 the p_value is 0.004515934929152868\n",
      "\n",
      "Between 1979 and 1999 the p_value is 0.013238270018743191\n",
      "\n",
      "Between 1979 and 2000 the p_value is 0.0029554747812238764\n",
      "\n",
      "Between 1979 and 2001 the p_value is 0.0009803334894444423\n",
      "\n",
      "Between 1979 and 2002 the p_value is 7.182466660426852e-05\n",
      "\n",
      "Between 1979 and 2003 the p_value is 0.016066189937285945\n",
      "\n",
      "Between 1979 and 2004 the p_value is 0.019044673609038235\n",
      "\n",
      "Between 1979 and 2005 the p_value is 0.010745391079713059\n",
      "\n",
      "Between 1979 and 2006 the p_value is 0.0009203319287369977\n",
      "\n",
      "Between 1979 and 2007 the p_value is 0.00021640508012704944\n",
      "\n",
      "Between 1979 and 2008 the p_value is 3.7642957999495484e-05\n",
      "\n",
      "Between 1979 and 2009 the p_value is 0.007900047790365825\n",
      "\n",
      "Between 1980 and 1981 the p_value is 0.02197015933246991\n",
      "\n",
      "Between 1980 and 1988 the p_value is 0.03127514039820173\n",
      "\n",
      "Between 1980 and 1989 the p_value is 0.0026978409934305192\n",
      "\n",
      "Between 1980 and 1990 the p_value is 0.023747220698582608\n",
      "\n",
      "Between 1980 and 1993 the p_value is 0.02025484366182241\n",
      "\n",
      "Between 1980 and 1995 the p_value is 0.02257784262603171\n",
      "\n",
      "Between 1980 and 1997 the p_value is 0.010733600661252363\n",
      "\n",
      "Between 1980 and 1998 the p_value is 0.0076045943514038775\n",
      "\n",
      "Between 1980 and 1999 the p_value is 0.023474467665486515\n",
      "\n",
      "Between 1980 and 2000 the p_value is 0.005884597804129128\n",
      "\n",
      "Between 1980 and 2001 the p_value is 0.0012879393474553517\n",
      "\n",
      "Between 1980 and 2002 the p_value is 0.0001142148133994719\n",
      "\n",
      "Between 1980 and 2003 the p_value is 0.02399657903615963\n",
      "\n",
      "Between 1980 and 2004 the p_value is 0.03489022164269656\n",
      "\n",
      "Between 1980 and 2005 the p_value is 0.01657196198001808\n",
      "\n",
      "Between 1980 and 2006 the p_value is 0.001348358309134344\n",
      "\n",
      "Between 1980 and 2007 the p_value is 0.00031337238910273356\n",
      "\n",
      "Between 1980 and 2008 the p_value is 2.8908199435816347e-05\n",
      "\n",
      "Between 1980 and 2009 the p_value is 0.008679044402720458\n",
      "\n",
      "Between 1981 and 1982 the p_value is 0.008146091601753995\n",
      "\n",
      "Between 1981 and 1984 the p_value is 0.01791193273241567\n",
      "\n",
      "Between 1982 and 1984 the p_value is 0.033819296669790014\n",
      "\n",
      "Between 1982 and 1985 the p_value is 0.03637259093911111\n",
      "\n",
      "Between 1982 and 1986 the p_value is 0.004597322333099851\n",
      "\n",
      "Between 1982 and 1987 the p_value is 0.002059845244516437\n",
      "\n",
      "Between 1982 and 1988 the p_value is 0.0001822330720709433\n",
      "\n",
      "Between 1982 and 1989 the p_value is 0.0001766017401621276\n",
      "\n",
      "Between 1982 and 1990 the p_value is 0.0028805017091883367\n",
      "\n",
      "Between 1982 and 1992 the p_value is 0.014144719016742982\n",
      "\n",
      "Between 1982 and 1993 the p_value is 0.0021564473269512965\n",
      "\n",
      "Between 1982 and 1995 the p_value is 0.01756290739596871\n",
      "\n",
      "Between 1982 and 1996 the p_value is 0.049024453295710056\n",
      "\n",
      "Between 1982 and 1997 the p_value is 0.00024476013955518914\n",
      "\n",
      "Between 1982 and 1998 the p_value is 0.0006091679060083547\n",
      "\n",
      "Between 1982 and 1999 the p_value is 0.0016152347489669337\n",
      "\n",
      "Between 1982 and 2000 the p_value is 7.148685098293523e-05\n",
      "\n",
      "Between 1982 and 2001 the p_value is 0.0002259623269899934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Between 1982 and 2002 the p_value is 4.0226663715740794e-07\n",
      "\n",
      "Between 1982 and 2003 the p_value is 0.007234929804930258\n",
      "\n",
      "Between 1982 and 2004 the p_value is 0.004396503860796291\n",
      "\n",
      "Between 1982 and 2005 the p_value is 0.004973886317524403\n",
      "\n",
      "Between 1982 and 2006 the p_value is 0.0005738888115005835\n",
      "\n",
      "Between 1982 and 2007 the p_value is 0.00011514680636103648\n",
      "\n",
      "Between 1982 and 2008 the p_value is 9.749136459893467e-05\n",
      "\n",
      "Between 1982 and 2009 the p_value is 0.007000565253724768\n",
      "\n",
      "Between 1983 and 2008 the p_value is 0.010594269299209591\n",
      "\n",
      "Between 1984 and 1988 the p_value is 0.015802353783224077\n",
      "\n",
      "Between 1984 and 1989 the p_value is 0.0009264969008947872\n",
      "\n",
      "Between 1984 and 1990 the p_value is 0.019841597671625286\n",
      "\n",
      "Between 1984 and 1993 the p_value is 0.018235595558826947\n",
      "\n",
      "Between 1984 and 1995 the p_value is 0.026918935447011703\n",
      "\n",
      "Between 1984 and 1997 the p_value is 0.006748644682753415\n",
      "\n",
      "Between 1984 and 1998 the p_value is 0.007248876172379491\n",
      "\n",
      "Between 1984 and 1999 the p_value is 0.02204384055298347\n",
      "\n",
      "Between 1984 and 2000 the p_value is 0.004188320639926108\n",
      "\n",
      "Between 1984 and 2001 the p_value is 0.0012644432309060682\n",
      "\n",
      "Between 1984 and 2002 the p_value is 4.95539025809164e-05\n",
      "\n",
      "Between 1984 and 2003 the p_value is 0.02809080740788407\n",
      "\n",
      "Between 1984 and 2004 the p_value is 0.03842811287191039\n",
      "\n",
      "Between 1984 and 2005 the p_value is 0.020236130468482307\n",
      "\n",
      "Between 1984 and 2006 the p_value is 0.0019346959904595182\n",
      "\n",
      "Between 1984 and 2007 the p_value is 0.0004504442821036163\n",
      "\n",
      "Between 1984 and 2008 the p_value is 5.0642813131580197e-05\n",
      "\n",
      "Between 1984 and 2009 the p_value is 0.010671767815350001\n",
      "\n",
      "Between 1985 and 2008 the p_value is 0.0065111559409705535\n",
      "\n",
      "Between 1986 and 2008 the p_value is 0.01572418294356892\n",
      "\n",
      "Between 1987 and 2008 the p_value is 0.035228778173430876\n",
      "\n",
      "Between 1988 and 1989 the p_value is 0.011900586245968356\n",
      "\n",
      "Between 1988 and 2007 the p_value is 0.02665215919785917\n",
      "\n",
      "Between 1988 and 2008 the p_value is 0.00029814466633827435\n",
      "\n",
      "Between 1989 and 1997 the p_value is 0.039453416098142\n",
      "\n",
      "Between 1989 and 1999 the p_value is 0.04772659072590347\n",
      "\n",
      "Between 1989 and 2000 the p_value is 0.012687156066053525\n",
      "\n",
      "Between 1989 and 2002 the p_value is 0.026575921425663035\n",
      "\n",
      "Between 1990 and 2008 the p_value is 0.0010322021062708457\n",
      "\n",
      "Between 1991 and 2007 the p_value is 0.043589371547594284\n",
      "\n",
      "Between 1991 and 2008 the p_value is 0.005340647408998465\n",
      "\n",
      "Between 1992 and 2008 the p_value is 0.0008306240369115186\n",
      "\n",
      "Between 1993 and 2008 the p_value is 0.0001161797475971771\n",
      "\n",
      "Between 1994 and 2008 the p_value is 0.015331087366720215\n",
      "\n",
      "Between 1995 and 2004 the p_value is 0.02289007832713218\n",
      "\n",
      "Between 1995 and 2008 the p_value is 0.02465474464645265\n",
      "\n",
      "Between 1996 and 2008 the p_value is 0.04084369946352538\n",
      "\n",
      "Between 1997 and 2007 the p_value is 0.04992218201602702\n",
      "\n",
      "Between 1997 and 2008 the p_value is 0.00018657628647892138\n",
      "\n",
      "Between 1998 and 2007 the p_value is 0.036598576895414556\n",
      "\n",
      "Between 1998 and 2008 the p_value is 8.499780660907426e-06\n",
      "\n",
      "Between 1999 and 2006 the p_value is 0.02052331853763051\n",
      "\n",
      "Between 1999 and 2007 the p_value is 0.007715100659623139\n",
      "\n",
      "Between 1999 and 2008 the p_value is 2.1687214332359904e-06\n",
      "\n",
      "Between 1999 and 2009 the p_value is 0.027029854394393767\n",
      "\n",
      "Between 2000 and 2001 the p_value is 0.04552340507171338\n",
      "\n",
      "Between 2000 and 2006 the p_value is 0.019370121067102657\n",
      "\n",
      "Between 2000 and 2007 the p_value is 0.006110546103999566\n",
      "\n",
      "Between 2000 and 2008 the p_value is 1.4173142410246976e-06\n",
      "\n",
      "Between 2000 and 2009 the p_value is 0.02196212731795492\n",
      "\n",
      "Between 2001 and 2008 the p_value is 0.0006201341600973806\n",
      "\n",
      "Between 2002 and 2008 the p_value is 1.3681200461946967e-05\n",
      "\n",
      "Between 2003 and 2008 the p_value is 4.063853403369127e-06\n",
      "\n",
      "Between 2004 and 2006 the p_value is 0.0025905572238169323\n",
      "\n",
      "Between 2004 and 2007 the p_value is 0.0010690317495824792\n",
      "\n",
      "Between 2004 and 2008 the p_value is 2.3977754711835965e-09\n",
      "\n",
      "Between 2004 and 2009 the p_value is 0.007103766997009271\n",
      "\n",
      "Between 2005 and 2008 the p_value is 6.623862603903843e-07\n",
      "\n",
      "Between 2006 and 2008 the p_value is 2.4974496669128607e-05\n",
      "\n",
      "Between 2007 and 2008 the p_value is 6.962758745875408e-05\n",
      "\n",
      "Between 2008 and 2009 the p_value is 0.030891624803256475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check whether there is significant difference between the price of building year\n",
    "# build year do give difference\n",
    "for year in range(1950,2011):\n",
    "    for compare in range(year+1,2011):\n",
    "        p_year_build_test(house_residential, year, compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 4 and 9 the p_value is 0.040058419468244975\n",
      "\n",
      "Between 4 and 11 the p_value is 0.043115189130592556\n",
      "\n",
      "Between 5 and 9 the p_value is 0.018639526613212224\n",
      "\n",
      "Between 5 and 11 the p_value is 0.020106790809764075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# month infor also useful, keep the month column for later\n",
    "\n",
    "for m1 in range(1,13):\n",
    "    for m2 in range(m1+1,13):\n",
    "        p_month_test(house_residential, m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test splite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by year\n",
    "train_data = house_residential[house_residential.YrSold< 2010 ].copy()\n",
    "test_data = house_residential[house_residential.YrSold == 2010 ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length\n",
    "(len(train_data)+len(test_data)) == len(house_residential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1278, 172)\n"
     ]
    }
   ],
   "source": [
    "# 1278 training data, 172 testing data\n",
    "print((len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training set, get predictor and target\n",
    "\n",
    "fixed_predictor = train_data[fixed_ch]\n",
    "y_train = train_data.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1), dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all categorical data and MSsubClass as dummies\n",
    "object_col = [col for col in fixed_ch if fixed_predictor[col].dtype==object]\n",
    "dummy_num_col = ['MSSubClass']\n",
    "dummy_col = dummy_num_col + object_col\n",
    "X_train = pd.get_dummies(fixed_predictor, columns=dummy_col, drop_first=True)\n",
    "# no object datatype\n",
    "np.argwhere(X_train.dtypes == object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1), dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing set do the same\n",
    "\n",
    "fixed_predictor = test_data[fixed_ch]\n",
    "y_test = test_data.SalePrice\n",
    "X_test = pd.get_dummies(fixed_predictor, columns=dummy_col, drop_first=True)\n",
    "# no object\n",
    "np.argwhere(X_test.dtypes == object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1278, 136), (1278,), (172, 116), (172,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of training and testing does not match\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSSubClass_45',\n",
       " 'MSSubClass_75',\n",
       " 'MSZoning_RH',\n",
       " 'LotShape_IR3',\n",
       " 'Utilities_NoSeWa',\n",
       " 'LotConfig_FR3',\n",
       " 'Neighborhood_Blueste',\n",
       " 'Neighborhood_Veenker',\n",
       " 'Condition2_Feedr',\n",
       " 'Condition2_PosA',\n",
       " 'Condition2_PosN',\n",
       " 'Condition2_RRAe',\n",
       " 'Condition2_RRAn',\n",
       " 'Condition2_RRNn',\n",
       " 'HouseStyle_1.5Unf',\n",
       " 'HouseStyle_2.5Fin',\n",
       " 'HouseStyle_2.5Unf',\n",
       " 'GarageType_Attchd',\n",
       " 'MiscFeature_NA',\n",
       " 'MiscFeature_TenC']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are categoies do not exist in testing data\n",
    "missing_col = [col for col in X_train.columns if col not in X_test.columns]\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing data do not have additonal category than training data\n",
    "[col for col in X_test.columns if col not in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refill the missing columns\n",
    "missing_col_loc = [np.argwhere(X_train.columns==col)[0,0] for col in missing_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 35,\n",
       " 43,\n",
       " 50,\n",
       " 55,\n",
       " 58,\n",
       " 62,\n",
       " 85,\n",
       " 94,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 105,\n",
       " 107,\n",
       " 108,\n",
       " 124,\n",
       " 132,\n",
       " 135]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_col_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'LotFrontage'),\n",
       " (1, 'LotArea'),\n",
       " (2, 'sold_built_age'),\n",
       " (3, 'sold_Remod_age'),\n",
       " (4, 'MasVnrArea'),\n",
       " (5, 'TotalBsmtSF'),\n",
       " (6, '1stFlrSF'),\n",
       " (7, '2ndFlrSF'),\n",
       " (8, 'GrLivArea'),\n",
       " (9, 'BsmtFullBath'),\n",
       " (10, 'BsmtHalfBath'),\n",
       " (11, 'FullBath'),\n",
       " (12, 'HalfBath'),\n",
       " (13, 'BedroomAbvGr'),\n",
       " (14, 'KitchenAbvGr'),\n",
       " (15, 'TotRmsAbvGrd'),\n",
       " (16, 'Fireplaces'),\n",
       " (17, 'have_garage'),\n",
       " (18, 'sold_Garage_age'),\n",
       " (19, 'GarageCars'),\n",
       " (20, 'GarageArea'),\n",
       " (21, 'WoodDeckSF'),\n",
       " (22, 'OpenPorchSF'),\n",
       " (23, 'EnclosedPorch'),\n",
       " (24, '3SsnPorch'),\n",
       " (25, 'ScreenPorch'),\n",
       " (26, 'PorchSF'),\n",
       " (27, 'PoolArea'),\n",
       " (28, 'MiscVal'),\n",
       " (29, 'MSSubClass_30'),\n",
       " (30, 'MSSubClass_40'),\n",
       " (31, 'MSSubClass_45'),\n",
       " (32, 'MSSubClass_50'),\n",
       " (33, 'MSSubClass_60'),\n",
       " (34, 'MSSubClass_70'),\n",
       " (35, 'MSSubClass_75'),\n",
       " (36, 'MSSubClass_80'),\n",
       " (37, 'MSSubClass_85'),\n",
       " (38, 'MSSubClass_90'),\n",
       " (39, 'MSSubClass_120'),\n",
       " (40, 'MSSubClass_160'),\n",
       " (41, 'MSSubClass_180'),\n",
       " (42, 'MSSubClass_190'),\n",
       " (43, 'MSZoning_RH'),\n",
       " (44, 'MSZoning_RL'),\n",
       " (45, 'MSZoning_RM'),\n",
       " (46, 'Street_Pave'),\n",
       " (47, 'Alley_NA'),\n",
       " (48, 'Alley_Pave'),\n",
       " (49, 'LotShape_IR2'),\n",
       " (50, 'LotShape_IR3'),\n",
       " (51, 'LotShape_Reg'),\n",
       " (52, 'LandContour_HLS'),\n",
       " (53, 'LandContour_Low'),\n",
       " (54, 'LandContour_Lvl'),\n",
       " (55, 'Utilities_NoSeWa'),\n",
       " (56, 'LotConfig_CulDSac'),\n",
       " (57, 'LotConfig_FR2'),\n",
       " (58, 'LotConfig_FR3'),\n",
       " (59, 'LotConfig_Inside'),\n",
       " (60, 'LandSlope_Mod'),\n",
       " (61, 'LandSlope_Sev'),\n",
       " (62, 'Neighborhood_Blueste'),\n",
       " (63, 'Neighborhood_BrDale'),\n",
       " (64, 'Neighborhood_BrkSide'),\n",
       " (65, 'Neighborhood_ClearCr'),\n",
       " (66, 'Neighborhood_CollgCr'),\n",
       " (67, 'Neighborhood_Crawfor'),\n",
       " (68, 'Neighborhood_Edwards'),\n",
       " (69, 'Neighborhood_Gilbert'),\n",
       " (70, 'Neighborhood_IDOTRR'),\n",
       " (71, 'Neighborhood_MeadowV'),\n",
       " (72, 'Neighborhood_Mitchel'),\n",
       " (73, 'Neighborhood_NAmes'),\n",
       " (74, 'Neighborhood_NPkVill'),\n",
       " (75, 'Neighborhood_NWAmes'),\n",
       " (76, 'Neighborhood_NoRidge'),\n",
       " (77, 'Neighborhood_NridgHt'),\n",
       " (78, 'Neighborhood_OldTown'),\n",
       " (79, 'Neighborhood_SWISU'),\n",
       " (80, 'Neighborhood_Sawyer'),\n",
       " (81, 'Neighborhood_SawyerW'),\n",
       " (82, 'Neighborhood_Somerst'),\n",
       " (83, 'Neighborhood_StoneBr'),\n",
       " (84, 'Neighborhood_Timber'),\n",
       " (85, 'Neighborhood_Veenker'),\n",
       " (86, 'Condition1_Feedr'),\n",
       " (87, 'Condition1_Norm'),\n",
       " (88, 'Condition1_PosA'),\n",
       " (89, 'Condition1_PosN'),\n",
       " (90, 'Condition1_RRAe'),\n",
       " (91, 'Condition1_RRAn'),\n",
       " (92, 'Condition1_RRNe'),\n",
       " (93, 'Condition1_RRNn'),\n",
       " (94, 'Condition2_Feedr'),\n",
       " (95, 'Condition2_Norm'),\n",
       " (96, 'Condition2_PosA'),\n",
       " (97, 'Condition2_PosN'),\n",
       " (98, 'Condition2_RRAe'),\n",
       " (99, 'Condition2_RRAn'),\n",
       " (100, 'Condition2_RRNn'),\n",
       " (101, 'BldgType_2fmCon'),\n",
       " (102, 'BldgType_Duplex'),\n",
       " (103, 'BldgType_Twnhs'),\n",
       " (104, 'BldgType_TwnhsE'),\n",
       " (105, 'HouseStyle_1.5Unf'),\n",
       " (106, 'HouseStyle_1Story'),\n",
       " (107, 'HouseStyle_2.5Fin'),\n",
       " (108, 'HouseStyle_2.5Unf'),\n",
       " (109, 'HouseStyle_2Story'),\n",
       " (110, 'HouseStyle_SFoyer'),\n",
       " (111, 'HouseStyle_SLvl'),\n",
       " (112, 'MasVnrType_BrkFace'),\n",
       " (113, 'MasVnrType_NA'),\n",
       " (114, 'MasVnrType_Stone'),\n",
       " (115, 'Foundation_CBlock'),\n",
       " (116, 'Foundation_PConc'),\n",
       " (117, 'Foundation_Slab'),\n",
       " (118, 'Foundation_Stone'),\n",
       " (119, 'Foundation_Wood'),\n",
       " (120, 'BsmtExposure_Gd'),\n",
       " (121, 'BsmtExposure_Mn'),\n",
       " (122, 'BsmtExposure_NA'),\n",
       " (123, 'BsmtExposure_No'),\n",
       " (124, 'GarageType_Attchd'),\n",
       " (125, 'GarageType_Basment'),\n",
       " (126, 'GarageType_BuiltIn'),\n",
       " (127, 'GarageType_CarPort'),\n",
       " (128, 'GarageType_Detchd'),\n",
       " (129, 'GarageType_NA'),\n",
       " (130, 'PavedDrive_P'),\n",
       " (131, 'PavedDrive_Y'),\n",
       " (132, 'MiscFeature_NA'),\n",
       " (133, 'MiscFeature_Othr'),\n",
       " (134, 'MiscFeature_Shed'),\n",
       " (135, 'MiscFeature_TenC')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillin zeros for the missing columns\n",
    "for i,locnumber in enumerate(missing_col_loc):\n",
    "    X_test.insert(loc=int(locnumber), column=missing_col[i], value=[0 for ele in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass_45</th>\n",
       "      <th>MSSubClass_75</th>\n",
       "      <th>MSZoning_RH</th>\n",
       "      <th>LotShape_IR3</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>LotConfig_FR3</th>\n",
       "      <th>Neighborhood_Blueste</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>Condition2_Feedr</th>\n",
       "      <th>Condition2_PosA</th>\n",
       "      <th>Condition2_PosN</th>\n",
       "      <th>Condition2_RRAe</th>\n",
       "      <th>Condition2_RRAn</th>\n",
       "      <th>Condition2_RRNn</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>GarageType_Attchd</th>\n",
       "      <th>MiscFeature_NA</th>\n",
       "      <th>MiscFeature_TenC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass_45  MSSubClass_75  MSZoning_RH  LotShape_IR3  Utilities_NoSeWa  \\\n",
       "16              0              0            0             0                 0   \n",
       "24              0              0            0             0                 0   \n",
       "26              0              0            0             0                 0   \n",
       "27              0              0            0             0                 0   \n",
       "33              0              0            0             0                 0   \n",
       "\n",
       "    LotConfig_FR3  Neighborhood_Blueste  Neighborhood_Veenker  \\\n",
       "16              0                     0                     0   \n",
       "24              0                     0                     0   \n",
       "26              0                     0                     0   \n",
       "27              0                     0                     0   \n",
       "33              0                     0                     0   \n",
       "\n",
       "    Condition2_Feedr  Condition2_PosA  Condition2_PosN  Condition2_RRAe  \\\n",
       "16                 0                0                0                0   \n",
       "24                 0                0                0                0   \n",
       "26                 0                0                0                0   \n",
       "27                 0                0                0                0   \n",
       "33                 0                0                0                0   \n",
       "\n",
       "    Condition2_RRAn  Condition2_RRNn  HouseStyle_1.5Unf  HouseStyle_2.5Fin  \\\n",
       "16                0                0                  0                  0   \n",
       "24                0                0                  0                  0   \n",
       "26                0                0                  0                  0   \n",
       "27                0                0                  0                  0   \n",
       "33                0                0                  0                  0   \n",
       "\n",
       "    HouseStyle_2.5Unf  GarageType_Attchd  MiscFeature_NA  MiscFeature_TenC  \n",
       "16                  0                  0               0                 0  \n",
       "24                  0                  0               0                 0  \n",
       "26                  0                  0               0                 0  \n",
       "27                  0                  0               0                 0  \n",
       "33                  0                  0               0                 0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[:, missing_col_loc].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass_45</th>\n",
       "      <th>MSSubClass_75</th>\n",
       "      <th>MSZoning_RH</th>\n",
       "      <th>LotShape_IR3</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>LotConfig_FR3</th>\n",
       "      <th>Neighborhood_Blueste</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>Condition2_Feedr</th>\n",
       "      <th>Condition2_PosA</th>\n",
       "      <th>Condition2_PosN</th>\n",
       "      <th>Condition2_RRAe</th>\n",
       "      <th>Condition2_RRAn</th>\n",
       "      <th>Condition2_RRNn</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>GarageType_Attchd</th>\n",
       "      <th>MiscFeature_NA</th>\n",
       "      <th>MiscFeature_TenC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass_45  MSSubClass_75  MSZoning_RH  LotShape_IR3  Utilities_NoSeWa  \\\n",
       "0              0              0            0             0                 0   \n",
       "1              0              0            0             0                 0   \n",
       "2              0              0            0             0                 0   \n",
       "3              0              0            0             0                 0   \n",
       "4              0              0            0             0                 0   \n",
       "\n",
       "   LotConfig_FR3  Neighborhood_Blueste  Neighborhood_Veenker  \\\n",
       "0              0                     0                     0   \n",
       "1              0                     0                     1   \n",
       "2              0                     0                     0   \n",
       "3              0                     0                     0   \n",
       "4              0                     0                     0   \n",
       "\n",
       "   Condition2_Feedr  Condition2_PosA  Condition2_PosN  Condition2_RRAe  \\\n",
       "0                 0                0                0                0   \n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "3                 0                0                0                0   \n",
       "4                 0                0                0                0   \n",
       "\n",
       "   Condition2_RRAn  Condition2_RRNn  HouseStyle_1.5Unf  HouseStyle_2.5Fin  \\\n",
       "0                0                0                  0                  0   \n",
       "1                0                0                  0                  0   \n",
       "2                0                0                  0                  0   \n",
       "3                0                0                  0                  0   \n",
       "4                0                0                  0                  0   \n",
       "\n",
       "   HouseStyle_2.5Unf  GarageType_Attchd  MiscFeature_NA  MiscFeature_TenC  \n",
       "0                  0                  1               1                 0  \n",
       "1                  0                  1               1                 0  \n",
       "2                  0                  1               1                 0  \n",
       "3                  0                  0               1                 0  \n",
       "4                  0                  1               1                 0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:, missing_col_loc].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1278, 136), (1278,), (172, 136), (172,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns matched\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-3.0546012262963243e+21, 8.2167088226185521e+21)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 1 LinearRegression\n",
    "# very bad, will not take into consideration\n",
    "scaler1 = StandardScaler()\n",
    "lr = LinearRegression()\n",
    "X_train_std = scaler1.fit_transform(X_train)\n",
    "score1 = cross_val_score(lr, X_train_std, y_train, scoring='r2', cv=10)\n",
    "np.mean(score1), np.std(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# transfer the test data\n",
    "X_test_std = scaler1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is 1049.122807017544\n",
      "The CV score mean is 0.793720141911953, the CV score std is 0.10466657233137884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85993251505694102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # model 2 Lasso\n",
    "# # using Lasso model to remove features not important\n",
    "# lassoCV = LassoCV(alphas=np.linspace(1000, 1050, 400), random_state=42, cv=10, max_iter=5000)\n",
    "# lassoCV.fit(X_train_std, y_train)\n",
    "# print(f'alpha is {lassoCV.alpha_}')\n",
    "# lasso = Lasso(alpha=lassoCV.alpha_, random_state=42, max_iter=5000)\n",
    "# score2 = cross_val_score(lasso, X_train_std, y_train, scoring='r2', cv=10)\n",
    "# print(f\"The CV score mean is {np.mean(score2)}, the CV score std is {np.std(score2)}\")\n",
    "# lasso.fit(X_train_std,y_train)\n",
    "# lasso.score(X_test_std,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 61 non zero coef\n",
    "Feature_coef = pd.DataFrame(data=lasso.coef_, index=X_train.columns, columns=['Lasso'])\n",
    "Feature_coef.loc['Intercept', 'Lasso'] = lasso.intercept_\n",
    "Feature_coef.iloc[np.argsort(Feature_coef.Lasso.abs())[::-1]].head(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is 419.7543859649123\n",
      "The CV score mean 0.8002753855451216, the CV score std 0.09194925239891304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85833780033472973"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # model 3 ridge \n",
    "# ridgeCV = RidgeCV(alphas=np.linspace(418, 425, 400), cv=10,scoring='r2')\n",
    "# ridgeCV.fit(X_train_std, y_train)\n",
    "# print(f'alpha is {ridgeCV.alpha_}')\n",
    "# ridge = Ridge(alpha=ridgeCV.alpha_,random_state=42, max_iter=5000)\n",
    "# score3 = cross_val_score(ridge,X_train_std, y_train, scoring='r2', cv=10)\n",
    "# print(f\"The CV score mean {np.mean(score3)}, the CV score std {np.std(score3)}\")\n",
    "# ridge.fit(X_train_std,y_train)\n",
    "# ridge.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Ridge coef\n",
    "Feature_coef.loc[:-1,'Ridge'] = ridge.coef_\n",
    "Feature_coef.loc['Intercept','Ridge'] = ridge.intercept_\n",
    "Feature_coef.iloc[np.argsort(Feature_coef.Ridge.abs())[::-1]].head(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The l1 ration is 0.989909090909091, the alpha is 56.56489665034848\n",
      "The CV score mean 0.798557630089781, the CV score std 0.08867224983754936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8471841575049972"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # model 4 elasticNet\n",
    "# l1_ratios=np.linspace(0.001, 1, 100)\n",
    "# n_alpha = 1000\n",
    "\n",
    "# elasticNetCV = ElasticNetCV(l1_ratio=l1_ratios, n_alphas=n_alpha, cv=10, max_iter=5000, random_state=42)\n",
    "# elasticNetCV.fit(X_train_std, y_train)\n",
    "\n",
    "# print(f\"The l1 ration is {elasticNetCV.l1_ratio_}, the alpha is {elasticNetCV.alpha_}\")\n",
    "# elasticNet = ElasticNet(alpha=elasticNetCV.alpha_, l1_ratio=elasticNetCV.l1_ratio_, max_iter=5000)\n",
    "# score4 = cross_val_score(elasticNet,X_train_std, y_train, scoring='r2', cv=10)\n",
    "# print(f\"The CV score mean {np.mean(score4)}, the CV score std {np.std(score4)}\")\n",
    "\n",
    "# elasticNet.fit(X_train_std, y_train)\n",
    "\n",
    "# elasticNet.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add elastic net feature\n",
    "Feature_coef.loc[:-1,'ElasticNet'] = elasticNet.coef_\n",
    "Feature_coef.loc['Intercept','ElasticNet'] = elasticNet.intercept_\n",
    "Feature_coef.iloc[np.argsort(Feature_coef.ElasticNet.abs())[::-1]].head(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: dict_keys(['memory', 'steps', 'KB', 'LR', 'KB__k', 'KB__score_func', 'LR__copy_X', 'LR__fit_intercept', 'LR__n_jobs', 'LR__normalize'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'KB__k': 91, 'KB__score_func': <function mutual_info_regression at 0x000002918D6E2510>}\n"
     ]
    }
   ],
   "source": [
    "# # model 5 SelectKBest + LinearRegression\n",
    "\n",
    "# pipeline_KB_LR = Pipeline([(\"KB\", SelectKBest()), (\"LR\", LinearRegression())])\n",
    "\n",
    "# print(f\"Hyperparameter: {pipeline_KB_LR.get_params().keys()}\")\n",
    "\n",
    "# KB_LR_parameter = {\"KB__k\": list(range(1,100)),\n",
    "#                   \"KB__score_func\": [f_regression, mutual_info_regression]}\n",
    "\n",
    "# KB_LR = GridSearchCV(pipeline_KB_LR, KB_LR_parameter, cv=10, scoring=\"r2\", n_jobs=3 )\n",
    "# KB_LR.fit(X_train_std, y_train)\n",
    "\n",
    "# print(f\"The best parameters are {KB_LR.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV score is 0.8080204316758324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84257821505187014"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"The CV score is {KB_LR.best_score_}\")\n",
    "# KB_lR_predict = KB_LR.predict(X_test_std)\n",
    "# r2_score(y_test, KB_lR_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std_df = pd.DataFrame(X_train_std, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='C:\\\\Users\\\\HUWENM~1\\\\AppData\\\\Local\\\\Temp\\\\tmprmzud52l'\", use \"location='C:\\\\Users\\\\HUWENM~1\\\\AppData\\\\Local\\\\Temp\\\\tmprmzud52l'\" instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: dict_keys(['memory', 'steps', 'KB', 'Lasso', 'KB__k', 'KB__score_func', 'Lasso__alpha', 'Lasso__copy_X', 'Lasso__fit_intercept', 'Lasso__max_iter', 'Lasso__normalize', 'Lasso__positive', 'Lasso__precompute', 'Lasso__random_state', 'Lasso__selection', 'Lasso__tol', 'Lasso__warm_start'])\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.074257, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "156     109500\n",
      "159     320000\n",
      "160     162500\n",
      "161     412500\n",
      "163     103200\n",
      "164     152000\n",
      "165     127500\n",
      "166     190000\n",
      "167     325624\n",
      "168     183500\n",
      "169     228000\n",
      "170     128500\n",
      "172     239000\n",
      "173     163000\n",
      "174     184000\n",
      "175     243000\n",
      "176     211000\n",
      "177     172500\n",
      "178     501837\n",
      "179     100000\n",
      "180     177000\n",
      "181     200100\n",
      "182     120000\n",
      "183     200000\n",
      "184     127000\n",
      "185     475000\n",
      "186     173000\n",
      "187     135000\n",
      "188     153337\n",
      "189     286000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1150, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.50628 , ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1424    144000\n",
      "1425    142000\n",
      "1426    271000\n",
      "1427    140000\n",
      "1429    182900\n",
      "1430    192140\n",
      "1431    143750\n",
      "1432     64500\n",
      "1433    186500\n",
      "1434    160000\n",
      "1435    174000\n",
      "1436    120500\n",
      "1437    394617\n",
      "1439    197000\n",
      "1440    191000\n",
      "1441    149300\n",
      "1442    310000\n",
      "1443    121000\n",
      "1444    179600\n",
      "1445    129000\n",
      "1447    240000\n",
      "1448    112000\n",
      "1449     92000\n",
      "1450    136000\n",
      "1451    287090\n",
      "1452    145000\n",
      "1453     84500\n",
      "1454    185000\n",
      "1455    175000\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1151, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(SelectKBest(k=1, score_func=<function f_regression at 0x000001D5F64BB840>), array([[ 0.218265, ..., -0.027984],\n",
      "       ..., \n",
      "       [ 0.074257, ..., -0.027984]]), \n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "5       143000\n",
      "6       307000\n",
      "7       200000\n",
      "8       129900\n",
      "9       118000\n",
      "10      129500\n",
      "11      345000\n",
      "12      144000\n",
      "13      279500\n",
      "14      157000\n",
      "15      132000\n",
      "17       90000\n",
      "18      159000\n",
      "19      139000\n",
      "20      325300\n",
      "21      139400\n",
      "22      230000\n",
      "23      129900\n",
      "25      256300\n",
      "28      207500\n",
      "29       68500\n",
      "31      149350\n",
      "32      179900\n",
      "34      277500\n",
      "35      309000\n",
      "         ...  \n",
      "1277    197900\n",
      "1278    237000\n",
      "1280    227000\n",
      "1281    180000\n",
      "1282    150500\n",
      "1285    132500\n",
      "1287    190000\n",
      "1288    278000\n",
      "1289    281000\n",
      "1291    119500\n",
      "1292    107500\n",
      "1293    162900\n",
      "1294    115000\n",
      "1295    138500\n",
      "1296    155000\n",
      "1297    140000\n",
      "1298    160000\n",
      "1300    225000\n",
      "1301    177500\n",
      "1302    290000\n",
      "1303    232000\n",
      "1304    130000\n",
      "1305    325000\n",
      "1306    202500\n",
      "1307    138000\n",
      "1308    147000\n",
      "1309    179200\n",
      "1311    203000\n",
      "1312    302000\n",
      "1314    119000\n",
      "Name: SalePrice, Length: 1151, dtype: int64, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\eb1d62b82415fa7d334faa3bf19d3201\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7de768d0e37627be357e98fc22869cd1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\6ed920083c3bbbedaef7356e8806596b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\1a9725bdf19e06b4a46b8ceaa833a0cb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\db93c62adf5f3c8097b2d8e9aaf65c45\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\bb55728762cf1a46c8e4bea6532454d2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\d59ef0db968fc721624391e3e01e220b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2aed8bc023e12e65287438fde280d796\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\1feea41e27b1e5ab1e892fd1a61f5226\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\8bcf1a4322642ab15940647cf10626ea\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\eb1d62b82415fa7d334faa3bf19d3201\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7de768d0e37627be357e98fc22869cd1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\6ed920083c3bbbedaef7356e8806596b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\1a9725bdf19e06b4a46b8ceaa833a0cb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\db93c62adf5f3c8097b2d8e9aaf65c45\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\bb55728762cf1a46c8e4bea6532454d2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\d59ef0db968fc721624391e3e01e220b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2aed8bc023e12e65287438fde280d796\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\1feea41e27b1e5ab1e892fd1a61f5226\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\8bcf1a4322642ab15940647cf10626ea\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\eb1d62b82415fa7d334faa3bf19d3201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\9f58044be2f598dab577a2378af3f17d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\02f66e822d1b2314d9c642aad4e76644\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\83783fb6c0fe31c8110e352dad6800a1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7f70769e01d3cafde4796e80a7b5fbd8\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\32d16e476cedf8e47300df274af1c245\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2bfed59600927fc21eb5ac6dd81ab6bb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\daedf8d71e41b6196f2e4619e0446b02\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\0c710e4668456b7585c4dc1b1e8b7afc\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\fcbbccff0a44a3a014c63c561cd02141\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\69fd30b9aad3964583d9bafed07f14c1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\9f58044be2f598dab577a2378af3f17d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\02f66e822d1b2314d9c642aad4e76644\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\83783fb6c0fe31c8110e352dad6800a1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7f70769e01d3cafde4796e80a7b5fbd8\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\32d16e476cedf8e47300df274af1c245\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2bfed59600927fc21eb5ac6dd81ab6bb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\daedf8d71e41b6196f2e4619e0446b02\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\0c710e4668456b7585c4dc1b1e8b7afc\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\fcbbccff0a44a3a014c63c561cd02141\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\69fd30b9aad3964583d9bafed07f14c1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\9f58044be2f598dab577a2378af3f17d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\02f66e822d1b2314d9c642aad4e76644\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\83783fb6c0fe31c8110e352dad6800a1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7f70769e01d3cafde4796e80a7b5fbd8\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\32d16e476cedf8e47300df274af1c245\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2bfed59600927fc21eb5ac6dd81ab6bb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\daedf8d71e41b6196f2e4619e0446b02\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\0c710e4668456b7585c4dc1b1e8b7afc\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\fcbbccff0a44a3a014c63c561cd02141\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\69fd30b9aad3964583d9bafed07f14c1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\9f58044be2f598dab577a2378af3f17d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\02f66e822d1b2314d9c642aad4e76644\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\83783fb6c0fe31c8110e352dad6800a1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7f70769e01d3cafde4796e80a7b5fbd8\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\32d16e476cedf8e47300df274af1c245\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2bfed59600927fc21eb5ac6dd81ab6bb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\daedf8d71e41b6196f2e4619e0446b02\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\0c710e4668456b7585c4dc1b1e8b7afc\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\fcbbccff0a44a3a014c63c561cd02141\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\69fd30b9aad3964583d9bafed07f14c1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\9f58044be2f598dab577a2378af3f17d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\02f66e822d1b2314d9c642aad4e76644\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\83783fb6c0fe31c8110e352dad6800a1\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7f70769e01d3cafde4796e80a7b5fbd8\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\32d16e476cedf8e47300df274af1c245\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\HUWENM~1\\AppData\\Local\\Temp\\tmprmzud52l\\joblib\\sklearn\\pipeline\\_fit_transform_one\\2bfed59600927fc21eb5ac6dd81ab6bb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'KB__k': 86, 'KB__score_func': <function mutual_info_regression at 0x000002B51B032510>, 'Ridge__alphas': array([   1.        ,    1.7997998 ,    2.5995996 ,    3.3993994 ,\n",
      "          4.1991992 ,    4.998999  ,    5.7987988 ,    6.5985986 ,\n",
      "          7.3983984 ,    8.1981982 ,    8.997998  ,    9.7977978 ,\n",
      "         10.5975976 ,   11.3973974 ,   12.1971972 ,   12.996997  ,\n",
      "         13.7967968 ,   14.5965966 ,   15.3963964 ,   16.1961962 ,\n",
      "         16.995996  ,   17.7957958 ,   18.5955956 ,   19.3953954 ,\n",
      "         20.1951952 ,   20.99499499,   21.79479479,   22.59459459,\n",
      "         23.39439439,   24.19419419,   24.99399399,   25.79379379,\n",
      "         26.59359359,   27.39339339,   28.19319319,   28.99299299,\n",
      "         29.79279279,   30.59259259,   31.39239239,   32.19219219,\n",
      "         32.99199199,   33.79179179,   34.59159159,   35.39139139,\n",
      "         36.19119119,   36.99099099,   37.79079079,   38.59059059,\n",
      "         39.39039039,   40.19019019,   40.98998999,   41.78978979,\n",
      "         42.58958959,   43.38938939,   44.18918919,   44.98898899,\n",
      "         45.78878879,   46.58858859,   47.38838839,   48.18818819,\n",
      "         48.98798799,   49.78778779,   50.58758759,   51.38738739,\n",
      "         52.18718719,   52.98698699,   53.78678679,   54.58658659,\n",
      "         55.38638639,   56.18618619,   56.98598599,   57.78578579,\n",
      "         58.58558559,   59.38538539,   60.18518519,   60.98498498,\n",
      "         61.78478478,   62.58458458,   63.38438438,   64.18418418,\n",
      "         64.98398398,   65.78378378,   66.58358358,   67.38338338,\n",
      "         68.18318318,   68.98298298,   69.78278278,   70.58258258,\n",
      "         71.38238238,   72.18218218,   72.98198198,   73.78178178,\n",
      "         74.58158158,   75.38138138,   76.18118118,   76.98098098,\n",
      "         77.78078078,   78.58058058,   79.38038038,   80.18018018,\n",
      "         80.97997998,   81.77977978,   82.57957958,   83.37937938,\n",
      "         84.17917918,   84.97897898,   85.77877878,   86.57857858,\n",
      "         87.37837838,   88.17817818,   88.97797798,   89.77777778,\n",
      "         90.57757758,   91.37737738,   92.17717718,   92.97697698,\n",
      "         93.77677678,   94.57657658,   95.37637638,   96.17617618,\n",
      "         96.97597598,   97.77577578,   98.57557558,   99.37537538,\n",
      "        100.17517518,  100.97497497,  101.77477477,  102.57457457,\n",
      "        103.37437437,  104.17417417,  104.97397397,  105.77377377,\n",
      "        106.57357357,  107.37337337,  108.17317317,  108.97297297,\n",
      "        109.77277277,  110.57257257,  111.37237237,  112.17217217,\n",
      "        112.97197197,  113.77177177,  114.57157157,  115.37137137,\n",
      "        116.17117117,  116.97097097,  117.77077077,  118.57057057,\n",
      "        119.37037037,  120.17017017,  120.96996997,  121.76976977,\n",
      "        122.56956957,  123.36936937,  124.16916917,  124.96896897,\n",
      "        125.76876877,  126.56856857,  127.36836837,  128.16816817,\n",
      "        128.96796797,  129.76776777,  130.56756757,  131.36736737,\n",
      "        132.16716717,  132.96696697,  133.76676677,  134.56656657,\n",
      "        135.36636637,  136.16616617,  136.96596597,  137.76576577,\n",
      "        138.56556557,  139.36536537,  140.16516517,  140.96496496,\n",
      "        141.76476476,  142.56456456,  143.36436436,  144.16416416,\n",
      "        144.96396396,  145.76376376,  146.56356356,  147.36336336,\n",
      "        148.16316316,  148.96296296,  149.76276276,  150.56256256,\n",
      "        151.36236236,  152.16216216,  152.96196196,  153.76176176,\n",
      "        154.56156156,  155.36136136,  156.16116116,  156.96096096,\n",
      "        157.76076076,  158.56056056,  159.36036036,  160.16016016,\n",
      "        160.95995996,  161.75975976,  162.55955956,  163.35935936,\n",
      "        164.15915916,  164.95895896,  165.75875876,  166.55855856,\n",
      "        167.35835836,  168.15815816,  168.95795796,  169.75775776,\n",
      "        170.55755756,  171.35735736,  172.15715716,  172.95695696,\n",
      "        173.75675676,  174.55655656,  175.35635636,  176.15615616,\n",
      "        176.95595596,  177.75575576,  178.55555556,  179.35535536,\n",
      "        180.15515516,  180.95495495,  181.75475475,  182.55455455,\n",
      "        183.35435435,  184.15415415,  184.95395395,  185.75375375,\n",
      "        186.55355355,  187.35335335,  188.15315315,  188.95295295,\n",
      "        189.75275275,  190.55255255,  191.35235235,  192.15215215,\n",
      "        192.95195195,  193.75175175,  194.55155155,  195.35135135,\n",
      "        196.15115115,  196.95095095,  197.75075075,  198.55055055,\n",
      "        199.35035035,  200.15015015,  200.94994995,  201.74974975,\n",
      "        202.54954955,  203.34934935,  204.14914915,  204.94894895,\n",
      "        205.74874875,  206.54854855,  207.34834835,  208.14814815,\n",
      "        208.94794795,  209.74774775,  210.54754755,  211.34734735,\n",
      "        212.14714715,  212.94694695,  213.74674675,  214.54654655,\n",
      "        215.34634635,  216.14614615,  216.94594595,  217.74574575,\n",
      "        218.54554555,  219.34534535,  220.14514515,  220.94494494,\n",
      "        221.74474474,  222.54454454,  223.34434434,  224.14414414,\n",
      "        224.94394394,  225.74374374,  226.54354354,  227.34334334,\n",
      "        228.14314314,  228.94294294,  229.74274274,  230.54254254,\n",
      "        231.34234234,  232.14214214,  232.94194194,  233.74174174,\n",
      "        234.54154154,  235.34134134,  236.14114114,  236.94094094,\n",
      "        237.74074074,  238.54054054,  239.34034034,  240.14014014,\n",
      "        240.93993994,  241.73973974,  242.53953954,  243.33933934,\n",
      "        244.13913914,  244.93893894,  245.73873874,  246.53853854,\n",
      "        247.33833834,  248.13813814,  248.93793794,  249.73773774,\n",
      "        250.53753754,  251.33733734,  252.13713714,  252.93693694,\n",
      "        253.73673674,  254.53653654,  255.33633634,  256.13613614,\n",
      "        256.93593594,  257.73573574,  258.53553554,  259.33533534,\n",
      "        260.13513514,  260.93493493,  261.73473473,  262.53453453,\n",
      "        263.33433433,  264.13413413,  264.93393393,  265.73373373,\n",
      "        266.53353353,  267.33333333,  268.13313313,  268.93293293,\n",
      "        269.73273273,  270.53253253,  271.33233233,  272.13213213,\n",
      "        272.93193193,  273.73173173,  274.53153153,  275.33133133,\n",
      "        276.13113113,  276.93093093,  277.73073073,  278.53053053,\n",
      "        279.33033033,  280.13013013,  280.92992993,  281.72972973,\n",
      "        282.52952953,  283.32932933,  284.12912913,  284.92892893,\n",
      "        285.72872873,  286.52852853,  287.32832833,  288.12812813,\n",
      "        288.92792793,  289.72772773,  290.52752753,  291.32732733,\n",
      "        292.12712713,  292.92692693,  293.72672673,  294.52652653,\n",
      "        295.32632633,  296.12612613,  296.92592593,  297.72572573,\n",
      "        298.52552553,  299.32532533,  300.12512513,  300.92492492,\n",
      "        301.72472472,  302.52452452,  303.32432432,  304.12412412,\n",
      "        304.92392392,  305.72372372,  306.52352352,  307.32332332,\n",
      "        308.12312312,  308.92292292,  309.72272272,  310.52252252,\n",
      "        311.32232232,  312.12212212,  312.92192192,  313.72172172,\n",
      "        314.52152152,  315.32132132,  316.12112112,  316.92092092,\n",
      "        317.72072072,  318.52052052,  319.32032032,  320.12012012,\n",
      "        320.91991992,  321.71971972,  322.51951952,  323.31931932,\n",
      "        324.11911912,  324.91891892,  325.71871872,  326.51851852,\n",
      "        327.31831832,  328.11811812,  328.91791792,  329.71771772,\n",
      "        330.51751752,  331.31731732,  332.11711712,  332.91691692,\n",
      "        333.71671672,  334.51651652,  335.31631632,  336.11611612,\n",
      "        336.91591592,  337.71571572,  338.51551552,  339.31531532,\n",
      "        340.11511512,  340.91491491,  341.71471471,  342.51451451,\n",
      "        343.31431431,  344.11411411,  344.91391391,  345.71371371,\n",
      "        346.51351351,  347.31331331,  348.11311311,  348.91291291,\n",
      "        349.71271271,  350.51251251,  351.31231231,  352.11211211,\n",
      "        352.91191191,  353.71171171,  354.51151151,  355.31131131,\n",
      "        356.11111111,  356.91091091,  357.71071071,  358.51051051,\n",
      "        359.31031031,  360.11011011,  360.90990991,  361.70970971,\n",
      "        362.50950951,  363.30930931,  364.10910911,  364.90890891,\n",
      "        365.70870871,  366.50850851,  367.30830831,  368.10810811,\n",
      "        368.90790791,  369.70770771,  370.50750751,  371.30730731,\n",
      "        372.10710711,  372.90690691,  373.70670671,  374.50650651,\n",
      "        375.30630631,  376.10610611,  376.90590591,  377.70570571,\n",
      "        378.50550551,  379.30530531,  380.10510511,  380.9049049 ,\n",
      "        381.7047047 ,  382.5045045 ,  383.3043043 ,  384.1041041 ,\n",
      "        384.9039039 ,  385.7037037 ,  386.5035035 ,  387.3033033 ,\n",
      "        388.1031031 ,  388.9029029 ,  389.7027027 ,  390.5025025 ,\n",
      "        391.3023023 ,  392.1021021 ,  392.9019019 ,  393.7017017 ,\n",
      "        394.5015015 ,  395.3013013 ,  396.1011011 ,  396.9009009 ,\n",
      "        397.7007007 ,  398.5005005 ,  399.3003003 ,  400.1001001 ,\n",
      "        400.8998999 ,  401.6996997 ,  402.4994995 ,  403.2992993 ,\n",
      "        404.0990991 ,  404.8988989 ,  405.6986987 ,  406.4984985 ,\n",
      "        407.2982983 ,  408.0980981 ,  408.8978979 ,  409.6976977 ,\n",
      "        410.4974975 ,  411.2972973 ,  412.0970971 ,  412.8968969 ,\n",
      "        413.6966967 ,  414.4964965 ,  415.2962963 ,  416.0960961 ,\n",
      "        416.8958959 ,  417.6956957 ,  418.4954955 ,  419.2952953 ,\n",
      "        420.0950951 ,  420.89489489,  421.69469469,  422.49449449,\n",
      "        423.29429429,  424.09409409,  424.89389389,  425.69369369,\n",
      "        426.49349349,  427.29329329,  428.09309309,  428.89289289,\n",
      "        429.69269269,  430.49249249,  431.29229229,  432.09209209,\n",
      "        432.89189189,  433.69169169,  434.49149149,  435.29129129,\n",
      "        436.09109109,  436.89089089,  437.69069069,  438.49049049,\n",
      "        439.29029029,  440.09009009,  440.88988989,  441.68968969,\n",
      "        442.48948949,  443.28928929,  444.08908909,  444.88888889,\n",
      "        445.68868869,  446.48848849,  447.28828829,  448.08808809,\n",
      "        448.88788789,  449.68768769,  450.48748749,  451.28728729,\n",
      "        452.08708709,  452.88688689,  453.68668669,  454.48648649,\n",
      "        455.28628629,  456.08608609,  456.88588589,  457.68568569,\n",
      "        458.48548549,  459.28528529,  460.08508509,  460.88488488,\n",
      "        461.68468468,  462.48448448,  463.28428428,  464.08408408,\n",
      "        464.88388388,  465.68368368,  466.48348348,  467.28328328,\n",
      "        468.08308308,  468.88288288,  469.68268268,  470.48248248,\n",
      "        471.28228228,  472.08208208,  472.88188188,  473.68168168,\n",
      "        474.48148148,  475.28128128,  476.08108108,  476.88088088,\n",
      "        477.68068068,  478.48048048,  479.28028028,  480.08008008,\n",
      "        480.87987988,  481.67967968,  482.47947948,  483.27927928,\n",
      "        484.07907908,  484.87887888,  485.67867868,  486.47847848,\n",
      "        487.27827828,  488.07807808,  488.87787788,  489.67767768,\n",
      "        490.47747748,  491.27727728,  492.07707708,  492.87687688,\n",
      "        493.67667668,  494.47647648,  495.27627628,  496.07607608,\n",
      "        496.87587588,  497.67567568,  498.47547548,  499.27527528,\n",
      "        500.07507508,  500.87487487,  501.67467467,  502.47447447,\n",
      "        503.27427427,  504.07407407,  504.87387387,  505.67367367,\n",
      "        506.47347347,  507.27327327,  508.07307307,  508.87287287,\n",
      "        509.67267267,  510.47247247,  511.27227227,  512.07207207,\n",
      "        512.87187187,  513.67167167,  514.47147147,  515.27127127,\n",
      "        516.07107107,  516.87087087,  517.67067067,  518.47047047,\n",
      "        519.27027027,  520.07007007,  520.86986987,  521.66966967,\n",
      "        522.46946947,  523.26926927,  524.06906907,  524.86886887,\n",
      "        525.66866867,  526.46846847,  527.26826827,  528.06806807,\n",
      "        528.86786787,  529.66766767,  530.46746747,  531.26726727,\n",
      "        532.06706707,  532.86686687,  533.66666667,  534.46646647,\n",
      "        535.26626627,  536.06606607,  536.86586587,  537.66566567,\n",
      "        538.46546547,  539.26526527,  540.06506507,  540.86486486,\n",
      "        541.66466466,  542.46446446,  543.26426426,  544.06406406,\n",
      "        544.86386386,  545.66366366,  546.46346346,  547.26326326,\n",
      "        548.06306306,  548.86286286,  549.66266266,  550.46246246,\n",
      "        551.26226226,  552.06206206,  552.86186186,  553.66166166,\n",
      "        554.46146146,  555.26126126,  556.06106106,  556.86086086,\n",
      "        557.66066066,  558.46046046,  559.26026026,  560.06006006,\n",
      "        560.85985986,  561.65965966,  562.45945946,  563.25925926,\n",
      "        564.05905906,  564.85885886,  565.65865866,  566.45845846,\n",
      "        567.25825826,  568.05805806,  568.85785786,  569.65765766,\n",
      "        570.45745746,  571.25725726,  572.05705706,  572.85685686,\n",
      "        573.65665666,  574.45645646,  575.25625626,  576.05605606,\n",
      "        576.85585586,  577.65565566,  578.45545546,  579.25525526,\n",
      "        580.05505506,  580.85485485,  581.65465465,  582.45445445,\n",
      "        583.25425425,  584.05405405,  584.85385385,  585.65365365,\n",
      "        586.45345345,  587.25325325,  588.05305305,  588.85285285,\n",
      "        589.65265265,  590.45245245,  591.25225225,  592.05205205,\n",
      "        592.85185185,  593.65165165,  594.45145145,  595.25125125,\n",
      "        596.05105105,  596.85085085,  597.65065065,  598.45045045,\n",
      "        599.25025025,  600.05005005,  600.84984985,  601.64964965,\n",
      "        602.44944945,  603.24924925,  604.04904905,  604.84884885,\n",
      "        605.64864865,  606.44844845,  607.24824825,  608.04804805,\n",
      "        608.84784785,  609.64764765,  610.44744745,  611.24724725,\n",
      "        612.04704705,  612.84684685,  613.64664665,  614.44644645,\n",
      "        615.24624625,  616.04604605,  616.84584585,  617.64564565,\n",
      "        618.44544545,  619.24524525,  620.04504505,  620.84484484,\n",
      "        621.64464464,  622.44444444,  623.24424424,  624.04404404,\n",
      "        624.84384384,  625.64364364,  626.44344344,  627.24324324,\n",
      "        628.04304304,  628.84284284,  629.64264264,  630.44244244,\n",
      "        631.24224224,  632.04204204,  632.84184184,  633.64164164,\n",
      "        634.44144144,  635.24124124,  636.04104104,  636.84084084,\n",
      "        637.64064064,  638.44044044,  639.24024024,  640.04004004,\n",
      "        640.83983984,  641.63963964,  642.43943944,  643.23923924,\n",
      "        644.03903904,  644.83883884,  645.63863864,  646.43843844,\n",
      "        647.23823824,  648.03803804,  648.83783784,  649.63763764,\n",
      "        650.43743744,  651.23723724,  652.03703704,  652.83683684,\n",
      "        653.63663664,  654.43643644,  655.23623624,  656.03603604,\n",
      "        656.83583584,  657.63563564,  658.43543544,  659.23523524,\n",
      "        660.03503504,  660.83483483,  661.63463463,  662.43443443,\n",
      "        663.23423423,  664.03403403,  664.83383383,  665.63363363,\n",
      "        666.43343343,  667.23323323,  668.03303303,  668.83283283,\n",
      "        669.63263263,  670.43243243,  671.23223223,  672.03203203,\n",
      "        672.83183183,  673.63163163,  674.43143143,  675.23123123,\n",
      "        676.03103103,  676.83083083,  677.63063063,  678.43043043,\n",
      "        679.23023023,  680.03003003,  680.82982983,  681.62962963,\n",
      "        682.42942943,  683.22922923,  684.02902903,  684.82882883,\n",
      "        685.62862863,  686.42842843,  687.22822823,  688.02802803,\n",
      "        688.82782783,  689.62762763,  690.42742743,  691.22722723,\n",
      "        692.02702703,  692.82682683,  693.62662663,  694.42642643,\n",
      "        695.22622623,  696.02602603,  696.82582583,  697.62562563,\n",
      "        698.42542543,  699.22522523,  700.02502503,  700.82482482,\n",
      "        701.62462462,  702.42442442,  703.22422422,  704.02402402,\n",
      "        704.82382382,  705.62362362,  706.42342342,  707.22322322,\n",
      "        708.02302302,  708.82282282,  709.62262262,  710.42242242,\n",
      "        711.22222222,  712.02202202,  712.82182182,  713.62162162,\n",
      "        714.42142142,  715.22122122,  716.02102102,  716.82082082,\n",
      "        717.62062062,  718.42042042,  719.22022022,  720.02002002,\n",
      "        720.81981982,  721.61961962,  722.41941942,  723.21921922,\n",
      "        724.01901902,  724.81881882,  725.61861862,  726.41841842,\n",
      "        727.21821822,  728.01801802,  728.81781782,  729.61761762,\n",
      "        730.41741742,  731.21721722,  732.01701702,  732.81681682,\n",
      "        733.61661662,  734.41641642,  735.21621622,  736.01601602,\n",
      "        736.81581582,  737.61561562,  738.41541542,  739.21521522,\n",
      "        740.01501502,  740.81481481,  741.61461461,  742.41441441,\n",
      "        743.21421421,  744.01401401,  744.81381381,  745.61361361,\n",
      "        746.41341341,  747.21321321,  748.01301301,  748.81281281,\n",
      "        749.61261261,  750.41241241,  751.21221221,  752.01201201,\n",
      "        752.81181181,  753.61161161,  754.41141141,  755.21121121,\n",
      "        756.01101101,  756.81081081,  757.61061061,  758.41041041,\n",
      "        759.21021021,  760.01001001,  760.80980981,  761.60960961,\n",
      "        762.40940941,  763.20920921,  764.00900901,  764.80880881,\n",
      "        765.60860861,  766.40840841,  767.20820821,  768.00800801,\n",
      "        768.80780781,  769.60760761,  770.40740741,  771.20720721,\n",
      "        772.00700701,  772.80680681,  773.60660661,  774.40640641,\n",
      "        775.20620621,  776.00600601,  776.80580581,  777.60560561,\n",
      "        778.40540541,  779.20520521,  780.00500501,  780.8048048 ,\n",
      "        781.6046046 ,  782.4044044 ,  783.2042042 ,  784.004004  ,\n",
      "        784.8038038 ,  785.6036036 ,  786.4034034 ,  787.2032032 ,\n",
      "        788.003003  ,  788.8028028 ,  789.6026026 ,  790.4024024 ,\n",
      "        791.2022022 ,  792.002002  ,  792.8018018 ,  793.6016016 ,\n",
      "        794.4014014 ,  795.2012012 ,  796.001001  ,  796.8008008 ,\n",
      "        797.6006006 ,  798.4004004 ,  799.2002002 ,  800.        ])}\n",
      "The CV score is 0.8115095689672831\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-a7f8fbf841b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The CV score is {KB_Ridge.best_score_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mKB_Ridge_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKB_Ridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKB_Ridge_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test_std' is not defined"
     ]
    }
   ],
   "source": [
    "# # model 7 SelectKBest + Ridge (cv)\n",
    "\n",
    "# pipeline_KB_Ridge = Pipeline([(\"KB\", SelectKBest()), (\"Ridge\", RidgeCV(cv=5))])\n",
    "\n",
    "# print(f\"Hyperparameter: {pipeline_KB_Ridge.get_params().keys()}\")\n",
    "\n",
    "# KB_Ridge_parameter = {\"KB__k\": list(range(1,100)),\n",
    "#                       \"KB__score_func\": [f_regression, mutual_info_regression],\n",
    "#                       \"Ridge__alphas\":[np.linspace(1, 800, 1000)]}\n",
    "\n",
    "# KB_Ridge = GridSearchCV(pipeline_KB_Ridge, KB_Ridge_parameter, cv=10, scoring=\"r2\" )\n",
    "# KB_Ridge.fit(X_train_std, y_train)\n",
    "\n",
    "# print(f\"The best parameters are {KB_Ridge.best_params_}\")\n",
    "\n",
    "# print(f\"The CV score is {KB_Ridge.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844892682319625"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KB_Ridge_predict = KB_Ridge.predict(X_test_std)\n",
    "# r2_score(y_test, KB_Ridge_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_KB_Ridge_best = Pipeline([(\"KB\", SelectKBest(k=86)), (\"Ridge\", RidgeCV(cv=5, alphas=np.linspace(1, 800, 1000)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('KB', SelectKBest(k=86, score_func=<function f_classif at 0x00000251418A4620>)), ('Ridge', RidgeCV(alphas=array([   1.    ,    1.7998, ...,  799.2002,  800.    ]), cv=5,\n",
       "    fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "    store_cv_values=False))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_KB_Ridge_best.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: dict_keys(['memory', 'steps', 'RFECV', 'Ridge', 'RFECV__cv', 'RFECV__estimator__copy_X', 'RFECV__estimator__fit_intercept', 'RFECV__estimator__n_jobs', 'RFECV__estimator__normalize', 'RFECV__estimator', 'RFECV__min_features_to_select', 'RFECV__n_jobs', 'RFECV__scoring', 'RFECV__step', 'RFECV__verbose', 'Ridge__alphas', 'Ridge__cv', 'Ridge__fit_intercept', 'Ridge__gcv_mode', 'Ridge__normalize', 'Ridge__scoring', 'Ridge__store_cv_values'])\n",
      "The best parameters are {'RFECV__step': 4, 'Ridge__alphas': array([   1.        ,    1.7997998 ,    2.5995996 ,    3.3993994 ,\n",
      "          4.1991992 ,    4.998999  ,    5.7987988 ,    6.5985986 ,\n",
      "          7.3983984 ,    8.1981982 ,    8.997998  ,    9.7977978 ,\n",
      "         10.5975976 ,   11.3973974 ,   12.1971972 ,   12.996997  ,\n",
      "         13.7967968 ,   14.5965966 ,   15.3963964 ,   16.1961962 ,\n",
      "         16.995996  ,   17.7957958 ,   18.5955956 ,   19.3953954 ,\n",
      "         20.1951952 ,   20.99499499,   21.79479479,   22.59459459,\n",
      "         23.39439439,   24.19419419,   24.99399399,   25.79379379,\n",
      "         26.59359359,   27.39339339,   28.19319319,   28.99299299,\n",
      "         29.79279279,   30.59259259,   31.39239239,   32.19219219,\n",
      "         32.99199199,   33.79179179,   34.59159159,   35.39139139,\n",
      "         36.19119119,   36.99099099,   37.79079079,   38.59059059,\n",
      "         39.39039039,   40.19019019,   40.98998999,   41.78978979,\n",
      "         42.58958959,   43.38938939,   44.18918919,   44.98898899,\n",
      "         45.78878879,   46.58858859,   47.38838839,   48.18818819,\n",
      "         48.98798799,   49.78778779,   50.58758759,   51.38738739,\n",
      "         52.18718719,   52.98698699,   53.78678679,   54.58658659,\n",
      "         55.38638639,   56.18618619,   56.98598599,   57.78578579,\n",
      "         58.58558559,   59.38538539,   60.18518519,   60.98498498,\n",
      "         61.78478478,   62.58458458,   63.38438438,   64.18418418,\n",
      "         64.98398398,   65.78378378,   66.58358358,   67.38338338,\n",
      "         68.18318318,   68.98298298,   69.78278278,   70.58258258,\n",
      "         71.38238238,   72.18218218,   72.98198198,   73.78178178,\n",
      "         74.58158158,   75.38138138,   76.18118118,   76.98098098,\n",
      "         77.78078078,   78.58058058,   79.38038038,   80.18018018,\n",
      "         80.97997998,   81.77977978,   82.57957958,   83.37937938,\n",
      "         84.17917918,   84.97897898,   85.77877878,   86.57857858,\n",
      "         87.37837838,   88.17817818,   88.97797798,   89.77777778,\n",
      "         90.57757758,   91.37737738,   92.17717718,   92.97697698,\n",
      "         93.77677678,   94.57657658,   95.37637638,   96.17617618,\n",
      "         96.97597598,   97.77577578,   98.57557558,   99.37537538,\n",
      "        100.17517518,  100.97497497,  101.77477477,  102.57457457,\n",
      "        103.37437437,  104.17417417,  104.97397397,  105.77377377,\n",
      "        106.57357357,  107.37337337,  108.17317317,  108.97297297,\n",
      "        109.77277277,  110.57257257,  111.37237237,  112.17217217,\n",
      "        112.97197197,  113.77177177,  114.57157157,  115.37137137,\n",
      "        116.17117117,  116.97097097,  117.77077077,  118.57057057,\n",
      "        119.37037037,  120.17017017,  120.96996997,  121.76976977,\n",
      "        122.56956957,  123.36936937,  124.16916917,  124.96896897,\n",
      "        125.76876877,  126.56856857,  127.36836837,  128.16816817,\n",
      "        128.96796797,  129.76776777,  130.56756757,  131.36736737,\n",
      "        132.16716717,  132.96696697,  133.76676677,  134.56656657,\n",
      "        135.36636637,  136.16616617,  136.96596597,  137.76576577,\n",
      "        138.56556557,  139.36536537,  140.16516517,  140.96496496,\n",
      "        141.76476476,  142.56456456,  143.36436436,  144.16416416,\n",
      "        144.96396396,  145.76376376,  146.56356356,  147.36336336,\n",
      "        148.16316316,  148.96296296,  149.76276276,  150.56256256,\n",
      "        151.36236236,  152.16216216,  152.96196196,  153.76176176,\n",
      "        154.56156156,  155.36136136,  156.16116116,  156.96096096,\n",
      "        157.76076076,  158.56056056,  159.36036036,  160.16016016,\n",
      "        160.95995996,  161.75975976,  162.55955956,  163.35935936,\n",
      "        164.15915916,  164.95895896,  165.75875876,  166.55855856,\n",
      "        167.35835836,  168.15815816,  168.95795796,  169.75775776,\n",
      "        170.55755756,  171.35735736,  172.15715716,  172.95695696,\n",
      "        173.75675676,  174.55655656,  175.35635636,  176.15615616,\n",
      "        176.95595596,  177.75575576,  178.55555556,  179.35535536,\n",
      "        180.15515516,  180.95495495,  181.75475475,  182.55455455,\n",
      "        183.35435435,  184.15415415,  184.95395395,  185.75375375,\n",
      "        186.55355355,  187.35335335,  188.15315315,  188.95295295,\n",
      "        189.75275275,  190.55255255,  191.35235235,  192.15215215,\n",
      "        192.95195195,  193.75175175,  194.55155155,  195.35135135,\n",
      "        196.15115115,  196.95095095,  197.75075075,  198.55055055,\n",
      "        199.35035035,  200.15015015,  200.94994995,  201.74974975,\n",
      "        202.54954955,  203.34934935,  204.14914915,  204.94894895,\n",
      "        205.74874875,  206.54854855,  207.34834835,  208.14814815,\n",
      "        208.94794795,  209.74774775,  210.54754755,  211.34734735,\n",
      "        212.14714715,  212.94694695,  213.74674675,  214.54654655,\n",
      "        215.34634635,  216.14614615,  216.94594595,  217.74574575,\n",
      "        218.54554555,  219.34534535,  220.14514515,  220.94494494,\n",
      "        221.74474474,  222.54454454,  223.34434434,  224.14414414,\n",
      "        224.94394394,  225.74374374,  226.54354354,  227.34334334,\n",
      "        228.14314314,  228.94294294,  229.74274274,  230.54254254,\n",
      "        231.34234234,  232.14214214,  232.94194194,  233.74174174,\n",
      "        234.54154154,  235.34134134,  236.14114114,  236.94094094,\n",
      "        237.74074074,  238.54054054,  239.34034034,  240.14014014,\n",
      "        240.93993994,  241.73973974,  242.53953954,  243.33933934,\n",
      "        244.13913914,  244.93893894,  245.73873874,  246.53853854,\n",
      "        247.33833834,  248.13813814,  248.93793794,  249.73773774,\n",
      "        250.53753754,  251.33733734,  252.13713714,  252.93693694,\n",
      "        253.73673674,  254.53653654,  255.33633634,  256.13613614,\n",
      "        256.93593594,  257.73573574,  258.53553554,  259.33533534,\n",
      "        260.13513514,  260.93493493,  261.73473473,  262.53453453,\n",
      "        263.33433433,  264.13413413,  264.93393393,  265.73373373,\n",
      "        266.53353353,  267.33333333,  268.13313313,  268.93293293,\n",
      "        269.73273273,  270.53253253,  271.33233233,  272.13213213,\n",
      "        272.93193193,  273.73173173,  274.53153153,  275.33133133,\n",
      "        276.13113113,  276.93093093,  277.73073073,  278.53053053,\n",
      "        279.33033033,  280.13013013,  280.92992993,  281.72972973,\n",
      "        282.52952953,  283.32932933,  284.12912913,  284.92892893,\n",
      "        285.72872873,  286.52852853,  287.32832833,  288.12812813,\n",
      "        288.92792793,  289.72772773,  290.52752753,  291.32732733,\n",
      "        292.12712713,  292.92692693,  293.72672673,  294.52652653,\n",
      "        295.32632633,  296.12612613,  296.92592593,  297.72572573,\n",
      "        298.52552553,  299.32532533,  300.12512513,  300.92492492,\n",
      "        301.72472472,  302.52452452,  303.32432432,  304.12412412,\n",
      "        304.92392392,  305.72372372,  306.52352352,  307.32332332,\n",
      "        308.12312312,  308.92292292,  309.72272272,  310.52252252,\n",
      "        311.32232232,  312.12212212,  312.92192192,  313.72172172,\n",
      "        314.52152152,  315.32132132,  316.12112112,  316.92092092,\n",
      "        317.72072072,  318.52052052,  319.32032032,  320.12012012,\n",
      "        320.91991992,  321.71971972,  322.51951952,  323.31931932,\n",
      "        324.11911912,  324.91891892,  325.71871872,  326.51851852,\n",
      "        327.31831832,  328.11811812,  328.91791792,  329.71771772,\n",
      "        330.51751752,  331.31731732,  332.11711712,  332.91691692,\n",
      "        333.71671672,  334.51651652,  335.31631632,  336.11611612,\n",
      "        336.91591592,  337.71571572,  338.51551552,  339.31531532,\n",
      "        340.11511512,  340.91491491,  341.71471471,  342.51451451,\n",
      "        343.31431431,  344.11411411,  344.91391391,  345.71371371,\n",
      "        346.51351351,  347.31331331,  348.11311311,  348.91291291,\n",
      "        349.71271271,  350.51251251,  351.31231231,  352.11211211,\n",
      "        352.91191191,  353.71171171,  354.51151151,  355.31131131,\n",
      "        356.11111111,  356.91091091,  357.71071071,  358.51051051,\n",
      "        359.31031031,  360.11011011,  360.90990991,  361.70970971,\n",
      "        362.50950951,  363.30930931,  364.10910911,  364.90890891,\n",
      "        365.70870871,  366.50850851,  367.30830831,  368.10810811,\n",
      "        368.90790791,  369.70770771,  370.50750751,  371.30730731,\n",
      "        372.10710711,  372.90690691,  373.70670671,  374.50650651,\n",
      "        375.30630631,  376.10610611,  376.90590591,  377.70570571,\n",
      "        378.50550551,  379.30530531,  380.10510511,  380.9049049 ,\n",
      "        381.7047047 ,  382.5045045 ,  383.3043043 ,  384.1041041 ,\n",
      "        384.9039039 ,  385.7037037 ,  386.5035035 ,  387.3033033 ,\n",
      "        388.1031031 ,  388.9029029 ,  389.7027027 ,  390.5025025 ,\n",
      "        391.3023023 ,  392.1021021 ,  392.9019019 ,  393.7017017 ,\n",
      "        394.5015015 ,  395.3013013 ,  396.1011011 ,  396.9009009 ,\n",
      "        397.7007007 ,  398.5005005 ,  399.3003003 ,  400.1001001 ,\n",
      "        400.8998999 ,  401.6996997 ,  402.4994995 ,  403.2992993 ,\n",
      "        404.0990991 ,  404.8988989 ,  405.6986987 ,  406.4984985 ,\n",
      "        407.2982983 ,  408.0980981 ,  408.8978979 ,  409.6976977 ,\n",
      "        410.4974975 ,  411.2972973 ,  412.0970971 ,  412.8968969 ,\n",
      "        413.6966967 ,  414.4964965 ,  415.2962963 ,  416.0960961 ,\n",
      "        416.8958959 ,  417.6956957 ,  418.4954955 ,  419.2952953 ,\n",
      "        420.0950951 ,  420.89489489,  421.69469469,  422.49449449,\n",
      "        423.29429429,  424.09409409,  424.89389389,  425.69369369,\n",
      "        426.49349349,  427.29329329,  428.09309309,  428.89289289,\n",
      "        429.69269269,  430.49249249,  431.29229229,  432.09209209,\n",
      "        432.89189189,  433.69169169,  434.49149149,  435.29129129,\n",
      "        436.09109109,  436.89089089,  437.69069069,  438.49049049,\n",
      "        439.29029029,  440.09009009,  440.88988989,  441.68968969,\n",
      "        442.48948949,  443.28928929,  444.08908909,  444.88888889,\n",
      "        445.68868869,  446.48848849,  447.28828829,  448.08808809,\n",
      "        448.88788789,  449.68768769,  450.48748749,  451.28728729,\n",
      "        452.08708709,  452.88688689,  453.68668669,  454.48648649,\n",
      "        455.28628629,  456.08608609,  456.88588589,  457.68568569,\n",
      "        458.48548549,  459.28528529,  460.08508509,  460.88488488,\n",
      "        461.68468468,  462.48448448,  463.28428428,  464.08408408,\n",
      "        464.88388388,  465.68368368,  466.48348348,  467.28328328,\n",
      "        468.08308308,  468.88288288,  469.68268268,  470.48248248,\n",
      "        471.28228228,  472.08208208,  472.88188188,  473.68168168,\n",
      "        474.48148148,  475.28128128,  476.08108108,  476.88088088,\n",
      "        477.68068068,  478.48048048,  479.28028028,  480.08008008,\n",
      "        480.87987988,  481.67967968,  482.47947948,  483.27927928,\n",
      "        484.07907908,  484.87887888,  485.67867868,  486.47847848,\n",
      "        487.27827828,  488.07807808,  488.87787788,  489.67767768,\n",
      "        490.47747748,  491.27727728,  492.07707708,  492.87687688,\n",
      "        493.67667668,  494.47647648,  495.27627628,  496.07607608,\n",
      "        496.87587588,  497.67567568,  498.47547548,  499.27527528,\n",
      "        500.07507508,  500.87487487,  501.67467467,  502.47447447,\n",
      "        503.27427427,  504.07407407,  504.87387387,  505.67367367,\n",
      "        506.47347347,  507.27327327,  508.07307307,  508.87287287,\n",
      "        509.67267267,  510.47247247,  511.27227227,  512.07207207,\n",
      "        512.87187187,  513.67167167,  514.47147147,  515.27127127,\n",
      "        516.07107107,  516.87087087,  517.67067067,  518.47047047,\n",
      "        519.27027027,  520.07007007,  520.86986987,  521.66966967,\n",
      "        522.46946947,  523.26926927,  524.06906907,  524.86886887,\n",
      "        525.66866867,  526.46846847,  527.26826827,  528.06806807,\n",
      "        528.86786787,  529.66766767,  530.46746747,  531.26726727,\n",
      "        532.06706707,  532.86686687,  533.66666667,  534.46646647,\n",
      "        535.26626627,  536.06606607,  536.86586587,  537.66566567,\n",
      "        538.46546547,  539.26526527,  540.06506507,  540.86486486,\n",
      "        541.66466466,  542.46446446,  543.26426426,  544.06406406,\n",
      "        544.86386386,  545.66366366,  546.46346346,  547.26326326,\n",
      "        548.06306306,  548.86286286,  549.66266266,  550.46246246,\n",
      "        551.26226226,  552.06206206,  552.86186186,  553.66166166,\n",
      "        554.46146146,  555.26126126,  556.06106106,  556.86086086,\n",
      "        557.66066066,  558.46046046,  559.26026026,  560.06006006,\n",
      "        560.85985986,  561.65965966,  562.45945946,  563.25925926,\n",
      "        564.05905906,  564.85885886,  565.65865866,  566.45845846,\n",
      "        567.25825826,  568.05805806,  568.85785786,  569.65765766,\n",
      "        570.45745746,  571.25725726,  572.05705706,  572.85685686,\n",
      "        573.65665666,  574.45645646,  575.25625626,  576.05605606,\n",
      "        576.85585586,  577.65565566,  578.45545546,  579.25525526,\n",
      "        580.05505506,  580.85485485,  581.65465465,  582.45445445,\n",
      "        583.25425425,  584.05405405,  584.85385385,  585.65365365,\n",
      "        586.45345345,  587.25325325,  588.05305305,  588.85285285,\n",
      "        589.65265265,  590.45245245,  591.25225225,  592.05205205,\n",
      "        592.85185185,  593.65165165,  594.45145145,  595.25125125,\n",
      "        596.05105105,  596.85085085,  597.65065065,  598.45045045,\n",
      "        599.25025025,  600.05005005,  600.84984985,  601.64964965,\n",
      "        602.44944945,  603.24924925,  604.04904905,  604.84884885,\n",
      "        605.64864865,  606.44844845,  607.24824825,  608.04804805,\n",
      "        608.84784785,  609.64764765,  610.44744745,  611.24724725,\n",
      "        612.04704705,  612.84684685,  613.64664665,  614.44644645,\n",
      "        615.24624625,  616.04604605,  616.84584585,  617.64564565,\n",
      "        618.44544545,  619.24524525,  620.04504505,  620.84484484,\n",
      "        621.64464464,  622.44444444,  623.24424424,  624.04404404,\n",
      "        624.84384384,  625.64364364,  626.44344344,  627.24324324,\n",
      "        628.04304304,  628.84284284,  629.64264264,  630.44244244,\n",
      "        631.24224224,  632.04204204,  632.84184184,  633.64164164,\n",
      "        634.44144144,  635.24124124,  636.04104104,  636.84084084,\n",
      "        637.64064064,  638.44044044,  639.24024024,  640.04004004,\n",
      "        640.83983984,  641.63963964,  642.43943944,  643.23923924,\n",
      "        644.03903904,  644.83883884,  645.63863864,  646.43843844,\n",
      "        647.23823824,  648.03803804,  648.83783784,  649.63763764,\n",
      "        650.43743744,  651.23723724,  652.03703704,  652.83683684,\n",
      "        653.63663664,  654.43643644,  655.23623624,  656.03603604,\n",
      "        656.83583584,  657.63563564,  658.43543544,  659.23523524,\n",
      "        660.03503504,  660.83483483,  661.63463463,  662.43443443,\n",
      "        663.23423423,  664.03403403,  664.83383383,  665.63363363,\n",
      "        666.43343343,  667.23323323,  668.03303303,  668.83283283,\n",
      "        669.63263263,  670.43243243,  671.23223223,  672.03203203,\n",
      "        672.83183183,  673.63163163,  674.43143143,  675.23123123,\n",
      "        676.03103103,  676.83083083,  677.63063063,  678.43043043,\n",
      "        679.23023023,  680.03003003,  680.82982983,  681.62962963,\n",
      "        682.42942943,  683.22922923,  684.02902903,  684.82882883,\n",
      "        685.62862863,  686.42842843,  687.22822823,  688.02802803,\n",
      "        688.82782783,  689.62762763,  690.42742743,  691.22722723,\n",
      "        692.02702703,  692.82682683,  693.62662663,  694.42642643,\n",
      "        695.22622623,  696.02602603,  696.82582583,  697.62562563,\n",
      "        698.42542543,  699.22522523,  700.02502503,  700.82482482,\n",
      "        701.62462462,  702.42442442,  703.22422422,  704.02402402,\n",
      "        704.82382382,  705.62362362,  706.42342342,  707.22322322,\n",
      "        708.02302302,  708.82282282,  709.62262262,  710.42242242,\n",
      "        711.22222222,  712.02202202,  712.82182182,  713.62162162,\n",
      "        714.42142142,  715.22122122,  716.02102102,  716.82082082,\n",
      "        717.62062062,  718.42042042,  719.22022022,  720.02002002,\n",
      "        720.81981982,  721.61961962,  722.41941942,  723.21921922,\n",
      "        724.01901902,  724.81881882,  725.61861862,  726.41841842,\n",
      "        727.21821822,  728.01801802,  728.81781782,  729.61761762,\n",
      "        730.41741742,  731.21721722,  732.01701702,  732.81681682,\n",
      "        733.61661662,  734.41641642,  735.21621622,  736.01601602,\n",
      "        736.81581582,  737.61561562,  738.41541542,  739.21521522,\n",
      "        740.01501502,  740.81481481,  741.61461461,  742.41441441,\n",
      "        743.21421421,  744.01401401,  744.81381381,  745.61361361,\n",
      "        746.41341341,  747.21321321,  748.01301301,  748.81281281,\n",
      "        749.61261261,  750.41241241,  751.21221221,  752.01201201,\n",
      "        752.81181181,  753.61161161,  754.41141141,  755.21121121,\n",
      "        756.01101101,  756.81081081,  757.61061061,  758.41041041,\n",
      "        759.21021021,  760.01001001,  760.80980981,  761.60960961,\n",
      "        762.40940941,  763.20920921,  764.00900901,  764.80880881,\n",
      "        765.60860861,  766.40840841,  767.20820821,  768.00800801,\n",
      "        768.80780781,  769.60760761,  770.40740741,  771.20720721,\n",
      "        772.00700701,  772.80680681,  773.60660661,  774.40640641,\n",
      "        775.20620621,  776.00600601,  776.80580581,  777.60560561,\n",
      "        778.40540541,  779.20520521,  780.00500501,  780.8048048 ,\n",
      "        781.6046046 ,  782.4044044 ,  783.2042042 ,  784.004004  ,\n",
      "        784.8038038 ,  785.6036036 ,  786.4034034 ,  787.2032032 ,\n",
      "        788.003003  ,  788.8028028 ,  789.6026026 ,  790.4024024 ,\n",
      "        791.2022022 ,  792.002002  ,  792.8018018 ,  793.6016016 ,\n",
      "        794.4014014 ,  795.2012012 ,  796.001001  ,  796.8008008 ,\n",
      "        797.6006006 ,  798.4004004 ,  799.2002002 ,  800.        ])}\n",
      "The CV score is 0.7831836418332688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8513434109742456"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # model 8 rfecv + Ridge (cv)\n",
    "\n",
    "\n",
    "# pipeline_RFECV_Ridge = Pipeline([(\"RFECV\", RFECV(LinearRegression(), cv=5, scoring='r2')), (\"Ridge\", RidgeCV(cv=5))])\n",
    "\n",
    "# print(f\"Hyperparameter: {pipeline_RFECV_Ridge.get_params().keys()}\")\n",
    "\n",
    "# RFECV_Ridge_parameter = {\"RFECV__step\": [1,2,3,4,5,6],\n",
    "#                       \"Ridge__alphas\":[np.linspace(1, 800, 1000)]}\n",
    "\n",
    "# RFECV_Ridge = GridSearchCV(pipeline_RFECV_Ridge, RFECV_Ridge_parameter, cv=10, scoring=\"r2\" )\n",
    "# RFECV_Ridge.fit(X_train_std, y_train)\n",
    "\n",
    "# print(f\"The best parameters are {RFECV_Ridge.best_params_}\")\n",
    "\n",
    "# print(f\"The CV score is {RFECV_Ridge.best_score_}\")\n",
    "# RFECV_Ridge_predict = RFECV_Ridge.predict(X_test_std)\n",
    "# r2_score(y_test, RFECV_Ridge_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: dict_keys(['memory', 'steps', 'RFECV', 'Lasso', 'RFECV__cv', 'RFECV__estimator__copy_X', 'RFECV__estimator__fit_intercept', 'RFECV__estimator__n_jobs', 'RFECV__estimator__normalize', 'RFECV__estimator', 'RFECV__min_features_to_select', 'RFECV__n_jobs', 'RFECV__scoring', 'RFECV__step', 'RFECV__verbose', 'Lasso__alphas', 'Lasso__copy_X', 'Lasso__cv', 'Lasso__eps', 'Lasso__fit_intercept', 'Lasso__max_iter', 'Lasso__n_alphas', 'Lasso__n_jobs', 'Lasso__normalize', 'Lasso__positive', 'Lasso__precompute', 'Lasso__random_state', 'Lasso__selection', 'Lasso__tol', 'Lasso__verbose'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\huwenmiao\\Anaconda3\\envs\\gadsi36\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'Lasso__alphas': array([   1.        ,    1.66638866,    2.33277731, ...,  798.66722269,\n",
      "        799.33361134,  800.        ]), 'RFECV__step': 5}\n",
      "The CV score is 0.7736017626531299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84249199220787951"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # model 9 rfecv + Lasso (cv)\n",
    "\n",
    "\n",
    "# pipeline_RFECV_Lasso = Pipeline([(\"RFECV\", RFECV(LinearRegression(), cv=5, scoring='r2')), (\"Lasso\", LassoCV(cv=5))])\n",
    "\n",
    "# print(f\"Hyperparameter: {pipeline_RFECV_Lasso.get_params().keys()}\")\n",
    "\n",
    "# RFECV_Lasso_parameter = {\"RFECV__step\": [1,2,3,4,5,6],\n",
    "#                       \"Lasso__alphas\":[np.linspace(1, 800, 1200)]}\n",
    "\n",
    "# RFECV_Lasso = GridSearchCV(pipeline_RFECV_Lasso, RFECV_Lasso_parameter, cv=10, scoring=\"r2\" )\n",
    "# RFECV_Lasso.fit(X_train_std, y_train)\n",
    "\n",
    "# print(f\"The best parameters are {RFECV_Lasso.best_params_}\")\n",
    "\n",
    "# print(f\"The CV score is {RFECV_Lasso.best_score_}\")\n",
    "# RFECV_Lasso_predict = RFECV_Lasso.predict(X_test_std)\n",
    "# r2_score(y_test, RFECV_Lasso_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pickle.dump( [KB_Ridge, KB_LR, elasticNet, ridge, lasso, RFECV_Ridge, RFECV_Lasso], open( \"save_result2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( [ KB_LR, elasticNet, ridge, lasso], open( \"save_result2part.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pickle.dump( [KB_Ridge], open( \"save_result2KB_ridge.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( [RFECV_Ridge, RFECV_Lasso], open( \"save_result2KB_RFECV.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_Ridge = pickle.load(open(\"save_result2KB_ridge.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFECV_Ridge, RFECV_Lasso = pickle.load(open(\"save_result2KB_RFECV.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_LR, elasticNet, ridge, lasso = pickle.load(open(\"save_result2part.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "        estimator=Pipeline(memory=None,\n",
       "      steps=[('KB', SelectKBest(k=10, score_func=<function f_classif at 0x00000251418A4620>)), ('Ridge', RidgeCV(alphas=array([  0.1,   1. ,  10. ]), cv=5, fit_intercept=True,\n",
       "     gcv_mode=None, normalize=False, scoring=None, store_cv_values=False))]),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'KB__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,...t 0x0000025141947620>], 'Ridge__alphas': [array([   1.    ,    1.7998, ...,  799.2002,  800.    ])]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring='r2', verbose=0)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KB_Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RFECV__step': 4,\n",
       " 'Ridge__alphas': array([   1.        ,    1.7997998 ,    2.5995996 ,    3.3993994 ,\n",
       "           4.1991992 ,    4.998999  ,    5.7987988 ,    6.5985986 ,\n",
       "           7.3983984 ,    8.1981982 ,    8.997998  ,    9.7977978 ,\n",
       "          10.5975976 ,   11.3973974 ,   12.1971972 ,   12.996997  ,\n",
       "          13.7967968 ,   14.5965966 ,   15.3963964 ,   16.1961962 ,\n",
       "          16.995996  ,   17.7957958 ,   18.5955956 ,   19.3953954 ,\n",
       "          20.1951952 ,   20.99499499,   21.79479479,   22.59459459,\n",
       "          23.39439439,   24.19419419,   24.99399399,   25.79379379,\n",
       "          26.59359359,   27.39339339,   28.19319319,   28.99299299,\n",
       "          29.79279279,   30.59259259,   31.39239239,   32.19219219,\n",
       "          32.99199199,   33.79179179,   34.59159159,   35.39139139,\n",
       "          36.19119119,   36.99099099,   37.79079079,   38.59059059,\n",
       "          39.39039039,   40.19019019,   40.98998999,   41.78978979,\n",
       "          42.58958959,   43.38938939,   44.18918919,   44.98898899,\n",
       "          45.78878879,   46.58858859,   47.38838839,   48.18818819,\n",
       "          48.98798799,   49.78778779,   50.58758759,   51.38738739,\n",
       "          52.18718719,   52.98698699,   53.78678679,   54.58658659,\n",
       "          55.38638639,   56.18618619,   56.98598599,   57.78578579,\n",
       "          58.58558559,   59.38538539,   60.18518519,   60.98498498,\n",
       "          61.78478478,   62.58458458,   63.38438438,   64.18418418,\n",
       "          64.98398398,   65.78378378,   66.58358358,   67.38338338,\n",
       "          68.18318318,   68.98298298,   69.78278278,   70.58258258,\n",
       "          71.38238238,   72.18218218,   72.98198198,   73.78178178,\n",
       "          74.58158158,   75.38138138,   76.18118118,   76.98098098,\n",
       "          77.78078078,   78.58058058,   79.38038038,   80.18018018,\n",
       "          80.97997998,   81.77977978,   82.57957958,   83.37937938,\n",
       "          84.17917918,   84.97897898,   85.77877878,   86.57857858,\n",
       "          87.37837838,   88.17817818,   88.97797798,   89.77777778,\n",
       "          90.57757758,   91.37737738,   92.17717718,   92.97697698,\n",
       "          93.77677678,   94.57657658,   95.37637638,   96.17617618,\n",
       "          96.97597598,   97.77577578,   98.57557558,   99.37537538,\n",
       "         100.17517518,  100.97497497,  101.77477477,  102.57457457,\n",
       "         103.37437437,  104.17417417,  104.97397397,  105.77377377,\n",
       "         106.57357357,  107.37337337,  108.17317317,  108.97297297,\n",
       "         109.77277277,  110.57257257,  111.37237237,  112.17217217,\n",
       "         112.97197197,  113.77177177,  114.57157157,  115.37137137,\n",
       "         116.17117117,  116.97097097,  117.77077077,  118.57057057,\n",
       "         119.37037037,  120.17017017,  120.96996997,  121.76976977,\n",
       "         122.56956957,  123.36936937,  124.16916917,  124.96896897,\n",
       "         125.76876877,  126.56856857,  127.36836837,  128.16816817,\n",
       "         128.96796797,  129.76776777,  130.56756757,  131.36736737,\n",
       "         132.16716717,  132.96696697,  133.76676677,  134.56656657,\n",
       "         135.36636637,  136.16616617,  136.96596597,  137.76576577,\n",
       "         138.56556557,  139.36536537,  140.16516517,  140.96496496,\n",
       "         141.76476476,  142.56456456,  143.36436436,  144.16416416,\n",
       "         144.96396396,  145.76376376,  146.56356356,  147.36336336,\n",
       "         148.16316316,  148.96296296,  149.76276276,  150.56256256,\n",
       "         151.36236236,  152.16216216,  152.96196196,  153.76176176,\n",
       "         154.56156156,  155.36136136,  156.16116116,  156.96096096,\n",
       "         157.76076076,  158.56056056,  159.36036036,  160.16016016,\n",
       "         160.95995996,  161.75975976,  162.55955956,  163.35935936,\n",
       "         164.15915916,  164.95895896,  165.75875876,  166.55855856,\n",
       "         167.35835836,  168.15815816,  168.95795796,  169.75775776,\n",
       "         170.55755756,  171.35735736,  172.15715716,  172.95695696,\n",
       "         173.75675676,  174.55655656,  175.35635636,  176.15615616,\n",
       "         176.95595596,  177.75575576,  178.55555556,  179.35535536,\n",
       "         180.15515516,  180.95495495,  181.75475475,  182.55455455,\n",
       "         183.35435435,  184.15415415,  184.95395395,  185.75375375,\n",
       "         186.55355355,  187.35335335,  188.15315315,  188.95295295,\n",
       "         189.75275275,  190.55255255,  191.35235235,  192.15215215,\n",
       "         192.95195195,  193.75175175,  194.55155155,  195.35135135,\n",
       "         196.15115115,  196.95095095,  197.75075075,  198.55055055,\n",
       "         199.35035035,  200.15015015,  200.94994995,  201.74974975,\n",
       "         202.54954955,  203.34934935,  204.14914915,  204.94894895,\n",
       "         205.74874875,  206.54854855,  207.34834835,  208.14814815,\n",
       "         208.94794795,  209.74774775,  210.54754755,  211.34734735,\n",
       "         212.14714715,  212.94694695,  213.74674675,  214.54654655,\n",
       "         215.34634635,  216.14614615,  216.94594595,  217.74574575,\n",
       "         218.54554555,  219.34534535,  220.14514515,  220.94494494,\n",
       "         221.74474474,  222.54454454,  223.34434434,  224.14414414,\n",
       "         224.94394394,  225.74374374,  226.54354354,  227.34334334,\n",
       "         228.14314314,  228.94294294,  229.74274274,  230.54254254,\n",
       "         231.34234234,  232.14214214,  232.94194194,  233.74174174,\n",
       "         234.54154154,  235.34134134,  236.14114114,  236.94094094,\n",
       "         237.74074074,  238.54054054,  239.34034034,  240.14014014,\n",
       "         240.93993994,  241.73973974,  242.53953954,  243.33933934,\n",
       "         244.13913914,  244.93893894,  245.73873874,  246.53853854,\n",
       "         247.33833834,  248.13813814,  248.93793794,  249.73773774,\n",
       "         250.53753754,  251.33733734,  252.13713714,  252.93693694,\n",
       "         253.73673674,  254.53653654,  255.33633634,  256.13613614,\n",
       "         256.93593594,  257.73573574,  258.53553554,  259.33533534,\n",
       "         260.13513514,  260.93493493,  261.73473473,  262.53453453,\n",
       "         263.33433433,  264.13413413,  264.93393393,  265.73373373,\n",
       "         266.53353353,  267.33333333,  268.13313313,  268.93293293,\n",
       "         269.73273273,  270.53253253,  271.33233233,  272.13213213,\n",
       "         272.93193193,  273.73173173,  274.53153153,  275.33133133,\n",
       "         276.13113113,  276.93093093,  277.73073073,  278.53053053,\n",
       "         279.33033033,  280.13013013,  280.92992993,  281.72972973,\n",
       "         282.52952953,  283.32932933,  284.12912913,  284.92892893,\n",
       "         285.72872873,  286.52852853,  287.32832833,  288.12812813,\n",
       "         288.92792793,  289.72772773,  290.52752753,  291.32732733,\n",
       "         292.12712713,  292.92692693,  293.72672673,  294.52652653,\n",
       "         295.32632633,  296.12612613,  296.92592593,  297.72572573,\n",
       "         298.52552553,  299.32532533,  300.12512513,  300.92492492,\n",
       "         301.72472472,  302.52452452,  303.32432432,  304.12412412,\n",
       "         304.92392392,  305.72372372,  306.52352352,  307.32332332,\n",
       "         308.12312312,  308.92292292,  309.72272272,  310.52252252,\n",
       "         311.32232232,  312.12212212,  312.92192192,  313.72172172,\n",
       "         314.52152152,  315.32132132,  316.12112112,  316.92092092,\n",
       "         317.72072072,  318.52052052,  319.32032032,  320.12012012,\n",
       "         320.91991992,  321.71971972,  322.51951952,  323.31931932,\n",
       "         324.11911912,  324.91891892,  325.71871872,  326.51851852,\n",
       "         327.31831832,  328.11811812,  328.91791792,  329.71771772,\n",
       "         330.51751752,  331.31731732,  332.11711712,  332.91691692,\n",
       "         333.71671672,  334.51651652,  335.31631632,  336.11611612,\n",
       "         336.91591592,  337.71571572,  338.51551552,  339.31531532,\n",
       "         340.11511512,  340.91491491,  341.71471471,  342.51451451,\n",
       "         343.31431431,  344.11411411,  344.91391391,  345.71371371,\n",
       "         346.51351351,  347.31331331,  348.11311311,  348.91291291,\n",
       "         349.71271271,  350.51251251,  351.31231231,  352.11211211,\n",
       "         352.91191191,  353.71171171,  354.51151151,  355.31131131,\n",
       "         356.11111111,  356.91091091,  357.71071071,  358.51051051,\n",
       "         359.31031031,  360.11011011,  360.90990991,  361.70970971,\n",
       "         362.50950951,  363.30930931,  364.10910911,  364.90890891,\n",
       "         365.70870871,  366.50850851,  367.30830831,  368.10810811,\n",
       "         368.90790791,  369.70770771,  370.50750751,  371.30730731,\n",
       "         372.10710711,  372.90690691,  373.70670671,  374.50650651,\n",
       "         375.30630631,  376.10610611,  376.90590591,  377.70570571,\n",
       "         378.50550551,  379.30530531,  380.10510511,  380.9049049 ,\n",
       "         381.7047047 ,  382.5045045 ,  383.3043043 ,  384.1041041 ,\n",
       "         384.9039039 ,  385.7037037 ,  386.5035035 ,  387.3033033 ,\n",
       "         388.1031031 ,  388.9029029 ,  389.7027027 ,  390.5025025 ,\n",
       "         391.3023023 ,  392.1021021 ,  392.9019019 ,  393.7017017 ,\n",
       "         394.5015015 ,  395.3013013 ,  396.1011011 ,  396.9009009 ,\n",
       "         397.7007007 ,  398.5005005 ,  399.3003003 ,  400.1001001 ,\n",
       "         400.8998999 ,  401.6996997 ,  402.4994995 ,  403.2992993 ,\n",
       "         404.0990991 ,  404.8988989 ,  405.6986987 ,  406.4984985 ,\n",
       "         407.2982983 ,  408.0980981 ,  408.8978979 ,  409.6976977 ,\n",
       "         410.4974975 ,  411.2972973 ,  412.0970971 ,  412.8968969 ,\n",
       "         413.6966967 ,  414.4964965 ,  415.2962963 ,  416.0960961 ,\n",
       "         416.8958959 ,  417.6956957 ,  418.4954955 ,  419.2952953 ,\n",
       "         420.0950951 ,  420.89489489,  421.69469469,  422.49449449,\n",
       "         423.29429429,  424.09409409,  424.89389389,  425.69369369,\n",
       "         426.49349349,  427.29329329,  428.09309309,  428.89289289,\n",
       "         429.69269269,  430.49249249,  431.29229229,  432.09209209,\n",
       "         432.89189189,  433.69169169,  434.49149149,  435.29129129,\n",
       "         436.09109109,  436.89089089,  437.69069069,  438.49049049,\n",
       "         439.29029029,  440.09009009,  440.88988989,  441.68968969,\n",
       "         442.48948949,  443.28928929,  444.08908909,  444.88888889,\n",
       "         445.68868869,  446.48848849,  447.28828829,  448.08808809,\n",
       "         448.88788789,  449.68768769,  450.48748749,  451.28728729,\n",
       "         452.08708709,  452.88688689,  453.68668669,  454.48648649,\n",
       "         455.28628629,  456.08608609,  456.88588589,  457.68568569,\n",
       "         458.48548549,  459.28528529,  460.08508509,  460.88488488,\n",
       "         461.68468468,  462.48448448,  463.28428428,  464.08408408,\n",
       "         464.88388388,  465.68368368,  466.48348348,  467.28328328,\n",
       "         468.08308308,  468.88288288,  469.68268268,  470.48248248,\n",
       "         471.28228228,  472.08208208,  472.88188188,  473.68168168,\n",
       "         474.48148148,  475.28128128,  476.08108108,  476.88088088,\n",
       "         477.68068068,  478.48048048,  479.28028028,  480.08008008,\n",
       "         480.87987988,  481.67967968,  482.47947948,  483.27927928,\n",
       "         484.07907908,  484.87887888,  485.67867868,  486.47847848,\n",
       "         487.27827828,  488.07807808,  488.87787788,  489.67767768,\n",
       "         490.47747748,  491.27727728,  492.07707708,  492.87687688,\n",
       "         493.67667668,  494.47647648,  495.27627628,  496.07607608,\n",
       "         496.87587588,  497.67567568,  498.47547548,  499.27527528,\n",
       "         500.07507508,  500.87487487,  501.67467467,  502.47447447,\n",
       "         503.27427427,  504.07407407,  504.87387387,  505.67367367,\n",
       "         506.47347347,  507.27327327,  508.07307307,  508.87287287,\n",
       "         509.67267267,  510.47247247,  511.27227227,  512.07207207,\n",
       "         512.87187187,  513.67167167,  514.47147147,  515.27127127,\n",
       "         516.07107107,  516.87087087,  517.67067067,  518.47047047,\n",
       "         519.27027027,  520.07007007,  520.86986987,  521.66966967,\n",
       "         522.46946947,  523.26926927,  524.06906907,  524.86886887,\n",
       "         525.66866867,  526.46846847,  527.26826827,  528.06806807,\n",
       "         528.86786787,  529.66766767,  530.46746747,  531.26726727,\n",
       "         532.06706707,  532.86686687,  533.66666667,  534.46646647,\n",
       "         535.26626627,  536.06606607,  536.86586587,  537.66566567,\n",
       "         538.46546547,  539.26526527,  540.06506507,  540.86486486,\n",
       "         541.66466466,  542.46446446,  543.26426426,  544.06406406,\n",
       "         544.86386386,  545.66366366,  546.46346346,  547.26326326,\n",
       "         548.06306306,  548.86286286,  549.66266266,  550.46246246,\n",
       "         551.26226226,  552.06206206,  552.86186186,  553.66166166,\n",
       "         554.46146146,  555.26126126,  556.06106106,  556.86086086,\n",
       "         557.66066066,  558.46046046,  559.26026026,  560.06006006,\n",
       "         560.85985986,  561.65965966,  562.45945946,  563.25925926,\n",
       "         564.05905906,  564.85885886,  565.65865866,  566.45845846,\n",
       "         567.25825826,  568.05805806,  568.85785786,  569.65765766,\n",
       "         570.45745746,  571.25725726,  572.05705706,  572.85685686,\n",
       "         573.65665666,  574.45645646,  575.25625626,  576.05605606,\n",
       "         576.85585586,  577.65565566,  578.45545546,  579.25525526,\n",
       "         580.05505506,  580.85485485,  581.65465465,  582.45445445,\n",
       "         583.25425425,  584.05405405,  584.85385385,  585.65365365,\n",
       "         586.45345345,  587.25325325,  588.05305305,  588.85285285,\n",
       "         589.65265265,  590.45245245,  591.25225225,  592.05205205,\n",
       "         592.85185185,  593.65165165,  594.45145145,  595.25125125,\n",
       "         596.05105105,  596.85085085,  597.65065065,  598.45045045,\n",
       "         599.25025025,  600.05005005,  600.84984985,  601.64964965,\n",
       "         602.44944945,  603.24924925,  604.04904905,  604.84884885,\n",
       "         605.64864865,  606.44844845,  607.24824825,  608.04804805,\n",
       "         608.84784785,  609.64764765,  610.44744745,  611.24724725,\n",
       "         612.04704705,  612.84684685,  613.64664665,  614.44644645,\n",
       "         615.24624625,  616.04604605,  616.84584585,  617.64564565,\n",
       "         618.44544545,  619.24524525,  620.04504505,  620.84484484,\n",
       "         621.64464464,  622.44444444,  623.24424424,  624.04404404,\n",
       "         624.84384384,  625.64364364,  626.44344344,  627.24324324,\n",
       "         628.04304304,  628.84284284,  629.64264264,  630.44244244,\n",
       "         631.24224224,  632.04204204,  632.84184184,  633.64164164,\n",
       "         634.44144144,  635.24124124,  636.04104104,  636.84084084,\n",
       "         637.64064064,  638.44044044,  639.24024024,  640.04004004,\n",
       "         640.83983984,  641.63963964,  642.43943944,  643.23923924,\n",
       "         644.03903904,  644.83883884,  645.63863864,  646.43843844,\n",
       "         647.23823824,  648.03803804,  648.83783784,  649.63763764,\n",
       "         650.43743744,  651.23723724,  652.03703704,  652.83683684,\n",
       "         653.63663664,  654.43643644,  655.23623624,  656.03603604,\n",
       "         656.83583584,  657.63563564,  658.43543544,  659.23523524,\n",
       "         660.03503504,  660.83483483,  661.63463463,  662.43443443,\n",
       "         663.23423423,  664.03403403,  664.83383383,  665.63363363,\n",
       "         666.43343343,  667.23323323,  668.03303303,  668.83283283,\n",
       "         669.63263263,  670.43243243,  671.23223223,  672.03203203,\n",
       "         672.83183183,  673.63163163,  674.43143143,  675.23123123,\n",
       "         676.03103103,  676.83083083,  677.63063063,  678.43043043,\n",
       "         679.23023023,  680.03003003,  680.82982983,  681.62962963,\n",
       "         682.42942943,  683.22922923,  684.02902903,  684.82882883,\n",
       "         685.62862863,  686.42842843,  687.22822823,  688.02802803,\n",
       "         688.82782783,  689.62762763,  690.42742743,  691.22722723,\n",
       "         692.02702703,  692.82682683,  693.62662663,  694.42642643,\n",
       "         695.22622623,  696.02602603,  696.82582583,  697.62562563,\n",
       "         698.42542543,  699.22522523,  700.02502503,  700.82482482,\n",
       "         701.62462462,  702.42442442,  703.22422422,  704.02402402,\n",
       "         704.82382382,  705.62362362,  706.42342342,  707.22322322,\n",
       "         708.02302302,  708.82282282,  709.62262262,  710.42242242,\n",
       "         711.22222222,  712.02202202,  712.82182182,  713.62162162,\n",
       "         714.42142142,  715.22122122,  716.02102102,  716.82082082,\n",
       "         717.62062062,  718.42042042,  719.22022022,  720.02002002,\n",
       "         720.81981982,  721.61961962,  722.41941942,  723.21921922,\n",
       "         724.01901902,  724.81881882,  725.61861862,  726.41841842,\n",
       "         727.21821822,  728.01801802,  728.81781782,  729.61761762,\n",
       "         730.41741742,  731.21721722,  732.01701702,  732.81681682,\n",
       "         733.61661662,  734.41641642,  735.21621622,  736.01601602,\n",
       "         736.81581582,  737.61561562,  738.41541542,  739.21521522,\n",
       "         740.01501502,  740.81481481,  741.61461461,  742.41441441,\n",
       "         743.21421421,  744.01401401,  744.81381381,  745.61361361,\n",
       "         746.41341341,  747.21321321,  748.01301301,  748.81281281,\n",
       "         749.61261261,  750.41241241,  751.21221221,  752.01201201,\n",
       "         752.81181181,  753.61161161,  754.41141141,  755.21121121,\n",
       "         756.01101101,  756.81081081,  757.61061061,  758.41041041,\n",
       "         759.21021021,  760.01001001,  760.80980981,  761.60960961,\n",
       "         762.40940941,  763.20920921,  764.00900901,  764.80880881,\n",
       "         765.60860861,  766.40840841,  767.20820821,  768.00800801,\n",
       "         768.80780781,  769.60760761,  770.40740741,  771.20720721,\n",
       "         772.00700701,  772.80680681,  773.60660661,  774.40640641,\n",
       "         775.20620621,  776.00600601,  776.80580581,  777.60560561,\n",
       "         778.40540541,  779.20520521,  780.00500501,  780.8048048 ,\n",
       "         781.6046046 ,  782.4044044 ,  783.2042042 ,  784.004004  ,\n",
       "         784.8038038 ,  785.6036036 ,  786.4034034 ,  787.2032032 ,\n",
       "         788.003003  ,  788.8028028 ,  789.6026026 ,  790.4024024 ,\n",
       "         791.2022022 ,  792.002002  ,  792.8018018 ,  793.6016016 ,\n",
       "         794.4014014 ,  795.2012012 ,  796.001001  ,  796.8008008 ,\n",
       "         797.6006006 ,  798.4004004 ,  799.2002002 ,  800.        ])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Determine any value of *changeable* property characteristics unexplained by the *fixed* ones.\n",
    "\n",
    "---\n",
    "\n",
    "Now that you have a model that estimates the price of a house based on its static characteristics, we can move forward with part 2 and 3 of the plan: what are the costs/benefits of quality, condition, and renovations?\n",
    "\n",
    "There are two specific requirements for these estimates:\n",
    "1. The estimates of effects must be in terms of dollars added or subtracted from the house value. \n",
    "2. The effects must be on the variance in price remaining from the first model.\n",
    "\n",
    "The residuals from the first model (training and testing) represent the variance in price unexplained by the fixed characteristics. Of that variance in price remaining, how much of it can be explained by the easy-to-change aspects of the property?\n",
    "\n",
    "---\n",
    "\n",
    "**Your goals:**\n",
    "1. Evaluate the effect in dollars of the renovate-able features. \n",
    "- How would your company use this second model and its coefficients to determine whether they should buy a property or not? Explain how the company can use the two models you have built to determine if they can make money. \n",
    "- Investigate how much of the variance in price remaining is explained by these features.\n",
    "- Do you trust your model? Should it be used to evaluate which properties to buy and fix up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. What property characteristics predict an \"abnormal\" sale?\n",
    "\n",
    "---\n",
    "\n",
    "The `SaleCondition` feature indicates the circumstances of the house sale. From the data file, we can see that the possibilities are:\n",
    "\n",
    "       Normal\tNormal Sale\n",
    "       Abnorml\tAbnormal Sale -  trade, foreclosure, short sale\n",
    "       AdjLand\tAdjoining Land Purchase\n",
    "       Alloca\tAllocation - two linked properties with separate deeds, typically condo with a garage unit\t\n",
    "       Family\tSale between family members\n",
    "       Partial\tHome was not completed when last assessed (associated with New Homes)\n",
    "       \n",
    "One of the executives at your company has an \"in\" with higher-ups at the major regional bank. His friends at the bank have made him a proposal: if he can reliably indicate what features, if any, predict \"abnormal\" sales (foreclosures, short sales, etc.), then in return the bank will give him first dibs on the pre-auction purchase of those properties (at a dirt-cheap price).\n",
    "\n",
    "He has tasked you with determining (and adequately validating) which features of a property predict this type of sale. \n",
    "\n",
    "---\n",
    "\n",
    "**Your task:**\n",
    "1. Determine which features predict the `Abnorml` category in the `SaleCondition` feature.\n",
    "- Justify your results.\n",
    "\n",
    "This is a challenging task that tests your ability to perform classification analysis in the face of severe class imbalance. You may find that simply running a classifier on the full dataset to predict the category ends up useless: when there is bad class imbalance classifiers often tend to simply guess the majority class.\n",
    "\n",
    "It is up to you to determine how you will tackle this problem. I recommend doing some research to find out how others have dealt with the problem in the past. Make sure to justify your solution. Don't worry about it being \"the best\" solution, but be rigorous.\n",
    "\n",
    "Be sure to indicate which features are predictive (if any) and whether they are positive or negative predictors of abnormal sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
